{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Library Import\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# import the train test split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List to store results\n",
    "# Stores the performance on test set.\n",
    "model_results = []\n",
    "#  Stores the name of model\n",
    "model_name = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jegan/workspace/Google3.0\n"
     ]
    }
   ],
   "source": [
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>TMP_HIGH</th>\n",
       "      <th>TMP_AVG</th>\n",
       "      <th>TMP_LOW</th>\n",
       "      <th>DP_HIGH</th>\n",
       "      <th>DP_AVG</th>\n",
       "      <th>DP_LOW</th>\n",
       "      <th>HUM_HIGH</th>\n",
       "      <th>...</th>\n",
       "      <th>VIS_HIGH</th>\n",
       "      <th>VIS_AVG</th>\n",
       "      <th>VIS_LOW</th>\n",
       "      <th>WIND_HIGH</th>\n",
       "      <th>WIND_AVG</th>\n",
       "      <th>PRECIP</th>\n",
       "      <th>NUMDAY_RAIN</th>\n",
       "      <th>NUM_DAYS_SNOW</th>\n",
       "      <th>NUM_DAYS_FOG</th>\n",
       "      <th>NUM_DAYS_THNDRSTRM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>168</td>\n",
       "      <td>2000</td>\n",
       "      <td>JAN</td>\n",
       "      <td>60.098523</td>\n",
       "      <td>24.919939</td>\n",
       "      <td>-5.750129</td>\n",
       "      <td>50.876803</td>\n",
       "      <td>18.079444</td>\n",
       "      <td>-19.152746</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.091680</td>\n",
       "      <td>8.231682</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>28.491682</td>\n",
       "      <td>9.674976</td>\n",
       "      <td>0.769395</td>\n",
       "      <td>4.527847</td>\n",
       "      <td>7.865567</td>\n",
       "      <td>3.129778</td>\n",
       "      <td>0.388589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>169</td>\n",
       "      <td>2000</td>\n",
       "      <td>FEB</td>\n",
       "      <td>53.769218</td>\n",
       "      <td>31.870521</td>\n",
       "      <td>5.378496</td>\n",
       "      <td>44.864486</td>\n",
       "      <td>25.930720</td>\n",
       "      <td>-0.570920</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.091680</td>\n",
       "      <td>7.687676</td>\n",
       "      <td>0.044134</td>\n",
       "      <td>28.925880</td>\n",
       "      <td>9.571381</td>\n",
       "      <td>287.022956</td>\n",
       "      <td>3.572751</td>\n",
       "      <td>4.803434</td>\n",
       "      <td>2.284139</td>\n",
       "      <td>0.385381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>170</td>\n",
       "      <td>2000</td>\n",
       "      <td>MAY</td>\n",
       "      <td>84.325735</td>\n",
       "      <td>62.360525</td>\n",
       "      <td>37.698853</td>\n",
       "      <td>68.870647</td>\n",
       "      <td>51.653895</td>\n",
       "      <td>25.975416</td>\n",
       "      <td>98.988685</td>\n",
       "      <td>...</td>\n",
       "      <td>11.091680</td>\n",
       "      <td>8.828890</td>\n",
       "      <td>0.570830</td>\n",
       "      <td>30.201699</td>\n",
       "      <td>9.129810</td>\n",
       "      <td>4.098408</td>\n",
       "      <td>10.428558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.610892</td>\n",
       "      <td>4.896486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>171</td>\n",
       "      <td>2000</td>\n",
       "      <td>JUN</td>\n",
       "      <td>88.622913</td>\n",
       "      <td>69.307943</td>\n",
       "      <td>52.228400</td>\n",
       "      <td>71.995094</td>\n",
       "      <td>60.166479</td>\n",
       "      <td>45.635832</td>\n",
       "      <td>99.068140</td>\n",
       "      <td>...</td>\n",
       "      <td>11.229015</td>\n",
       "      <td>9.513896</td>\n",
       "      <td>0.408795</td>\n",
       "      <td>27.953090</td>\n",
       "      <td>8.176592</td>\n",
       "      <td>3.466575</td>\n",
       "      <td>9.051719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.014093</td>\n",
       "      <td>4.340215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  YEAR MONTH   TMP_HIGH    TMP_AVG    TMP_LOW    DP_HIGH  \\\n",
       "168         168  2000   JAN  60.098523  24.919939  -5.750129  50.876803   \n",
       "169         169  2000   FEB  53.769218  31.870521   5.378496  44.864486   \n",
       "170         170  2000   MAY  84.325735  62.360525  37.698853  68.870647   \n",
       "171         171  2000   JUN  88.622913  69.307943  52.228400  71.995094   \n",
       "\n",
       "        DP_AVG     DP_LOW    HUM_HIGH         ...           VIS_HIGH  \\\n",
       "168  18.079444 -19.152746  100.000000         ...          11.091680   \n",
       "169  25.930720  -0.570920  100.000000         ...          11.091680   \n",
       "170  51.653895  25.975416   98.988685         ...          11.091680   \n",
       "171  60.166479  45.635832   99.068140         ...          11.229015   \n",
       "\n",
       "      VIS_AVG   VIS_LOW  WIND_HIGH  WIND_AVG      PRECIP  NUMDAY_RAIN  \\\n",
       "168  8.231682  0.061922  28.491682  9.674976    0.769395     4.527847   \n",
       "169  7.687676  0.044134  28.925880  9.571381  287.022956     3.572751   \n",
       "170  8.828890  0.570830  30.201699  9.129810    4.098408    10.428558   \n",
       "171  9.513896  0.408795  27.953090  8.176592    3.466575     9.051719   \n",
       "\n",
       "     NUM_DAYS_SNOW  NUM_DAYS_FOG  NUM_DAYS_THNDRSTRM  \n",
       "168       7.865567      3.129778            0.388589  \n",
       "169       4.803434      2.284139            0.385381  \n",
       "170       0.000000      1.610892            4.896486  \n",
       "171       0.000000      1.014093            4.340215  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('input_model_weighted_average.csv')\n",
    "df.head()\n",
    "df.iloc[168:172]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Period</th>\n",
       "      <th>Value</th>\n",
       "      <th>Discounted_Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2000</td>\n",
       "      <td>JAN</td>\n",
       "      <td>1.97</td>\n",
       "      <td>1.969512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>2000</td>\n",
       "      <td>FEB</td>\n",
       "      <td>2.03</td>\n",
       "      <td>2.029497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2000</td>\n",
       "      <td>MAY</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.199455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>2000</td>\n",
       "      <td>JUN</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.889531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year Period  Value  Discounted_Value\n",
       "168  2000    JAN   1.97          1.969512\n",
       "169  2000    FEB   2.03          2.029497\n",
       "170  2000    MAY   2.20          2.199455\n",
       "171  2000    JUN   1.89          1.889531"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_update = pd.read_csv('IL_Corn_Price_Updated_1986_2016.csv')\n",
    "p_update['Period'] = pd.Categorical(p_update['Period'], ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC'])\n",
    "\n",
    "p_update['Discounted_Value'] = p_update['Value'] /1.000248 \n",
    "#ps\n",
    "#Risk-free rate for 1986 - 2016 : 9.34% ( 30-year Treasury Constant Maturity Rate at 1986 )\n",
    "# Monthly discount rate = 1/1.000248 \n",
    "\n",
    "p_update = p_update[['Year', 'Period','Value', 'Discounted_Value']]\n",
    "price_updated = p_update.dropna(axis=0, how='any')\n",
    "#price_updated = p_update[p_update.Period != 'MARKETING YEAR']\n",
    "\n",
    "price_updated = price_updated.sort_values(['Year', 'Period'])\n",
    "price_updated = price_updated.reset_index(drop=True)\n",
    "#price_updated.iloc[168:174]\n",
    "\n",
    "price_updated = price_updated[~((price_updated.Year == 2000) & ((price_updated.Period == 'MAR') | (price_updated.Period == 'APR')))]\n",
    "price_updated = price_updated.reset_index(drop=True)\n",
    "price_updated.iloc[168:172]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>TMP_HIGH</th>\n",
       "      <th>TMP_AVG</th>\n",
       "      <th>TMP_LOW</th>\n",
       "      <th>DP_HIGH</th>\n",
       "      <th>DP_AVG</th>\n",
       "      <th>DP_LOW</th>\n",
       "      <th>HUM_HIGH</th>\n",
       "      <th>...</th>\n",
       "      <th>VIS_LOW</th>\n",
       "      <th>WIND_HIGH</th>\n",
       "      <th>WIND_AVG</th>\n",
       "      <th>PRECIP</th>\n",
       "      <th>NUMDAY_RAIN</th>\n",
       "      <th>NUM_DAYS_SNOW</th>\n",
       "      <th>NUM_DAYS_FOG</th>\n",
       "      <th>NUM_DAYS_THNDRSTRM</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discounted_Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>JAN</td>\n",
       "      <td>55.953658</td>\n",
       "      <td>27.617217</td>\n",
       "      <td>-2.980925</td>\n",
       "      <td>42.880246</td>\n",
       "      <td>19.992262</td>\n",
       "      <td>-8.511984</td>\n",
       "      <td>99.026800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186718</td>\n",
       "      <td>29.600197</td>\n",
       "      <td>11.077783</td>\n",
       "      <td>0.479003</td>\n",
       "      <td>2.544772</td>\n",
       "      <td>7.584738</td>\n",
       "      <td>9.707042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.349417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>FEB</td>\n",
       "      <td>53.838886</td>\n",
       "      <td>28.731112</td>\n",
       "      <td>0.353638</td>\n",
       "      <td>46.537659</td>\n",
       "      <td>23.958440</td>\n",
       "      <td>-4.555208</td>\n",
       "      <td>99.510367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>24.679482</td>\n",
       "      <td>9.622850</td>\n",
       "      <td>1.722515</td>\n",
       "      <td>5.510271</td>\n",
       "      <td>9.231277</td>\n",
       "      <td>15.203328</td>\n",
       "      <td>0.658457</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.379410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>MAR</td>\n",
       "      <td>77.668458</td>\n",
       "      <td>41.937001</td>\n",
       "      <td>13.926317</td>\n",
       "      <td>51.205991</td>\n",
       "      <td>29.862688</td>\n",
       "      <td>2.748564</td>\n",
       "      <td>99.138166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634101</td>\n",
       "      <td>36.374250</td>\n",
       "      <td>12.349509</td>\n",
       "      <td>15.116988</td>\n",
       "      <td>5.934169</td>\n",
       "      <td>3.868506</td>\n",
       "      <td>9.890731</td>\n",
       "      <td>1.055313</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.349417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>ARP</td>\n",
       "      <td>85.651529</td>\n",
       "      <td>52.949283</td>\n",
       "      <td>25.351909</td>\n",
       "      <td>60.274139</td>\n",
       "      <td>37.049373</td>\n",
       "      <td>11.479874</td>\n",
       "      <td>99.558871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474198</td>\n",
       "      <td>28.918771</td>\n",
       "      <td>10.770362</td>\n",
       "      <td>17.947352</td>\n",
       "      <td>9.195066</td>\n",
       "      <td>1.960945</td>\n",
       "      <td>7.611098</td>\n",
       "      <td>2.058521</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.339420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>MAY</td>\n",
       "      <td>85.415668</td>\n",
       "      <td>62.338688</td>\n",
       "      <td>36.602102</td>\n",
       "      <td>61.467698</td>\n",
       "      <td>47.555928</td>\n",
       "      <td>27.795232</td>\n",
       "      <td>95.246992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280723</td>\n",
       "      <td>33.072546</td>\n",
       "      <td>8.983943</td>\n",
       "      <td>15.212715</td>\n",
       "      <td>9.610446</td>\n",
       "      <td>0.327268</td>\n",
       "      <td>14.161837</td>\n",
       "      <td>4.663441</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.449393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>JUN</td>\n",
       "      <td>93.180721</td>\n",
       "      <td>71.263159</td>\n",
       "      <td>47.746898</td>\n",
       "      <td>67.021368</td>\n",
       "      <td>55.502022</td>\n",
       "      <td>33.450048</td>\n",
       "      <td>92.041052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950015</td>\n",
       "      <td>25.666101</td>\n",
       "      <td>8.317023</td>\n",
       "      <td>10.598875</td>\n",
       "      <td>8.135348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.649299</td>\n",
       "      <td>4.995009</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.449393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>JUL</td>\n",
       "      <td>93.651100</td>\n",
       "      <td>75.325134</td>\n",
       "      <td>57.224505</td>\n",
       "      <td>71.740380</td>\n",
       "      <td>60.440695</td>\n",
       "      <td>41.882198</td>\n",
       "      <td>91.024753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360831</td>\n",
       "      <td>30.924584</td>\n",
       "      <td>7.668664</td>\n",
       "      <td>11.201697</td>\n",
       "      <td>9.899297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.561325</td>\n",
       "      <td>6.286502</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.039494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>AUG</td>\n",
       "      <td>88.032940</td>\n",
       "      <td>68.355756</td>\n",
       "      <td>43.240501</td>\n",
       "      <td>70.470204</td>\n",
       "      <td>53.535626</td>\n",
       "      <td>34.387899</td>\n",
       "      <td>97.288101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123844</td>\n",
       "      <td>23.558382</td>\n",
       "      <td>6.839216</td>\n",
       "      <td>1.162475</td>\n",
       "      <td>6.987723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.848591</td>\n",
       "      <td>1.872076</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.769561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>SEP</td>\n",
       "      <td>86.350515</td>\n",
       "      <td>66.418060</td>\n",
       "      <td>39.848014</td>\n",
       "      <td>66.092621</td>\n",
       "      <td>53.110278</td>\n",
       "      <td>30.557896</td>\n",
       "      <td>92.975847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>31.473925</td>\n",
       "      <td>7.953490</td>\n",
       "      <td>6.651649</td>\n",
       "      <td>12.019534</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>13.926898</td>\n",
       "      <td>5.663013</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.519623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>OCT</td>\n",
       "      <td>74.910045</td>\n",
       "      <td>52.914618</td>\n",
       "      <td>29.721084</td>\n",
       "      <td>61.366736</td>\n",
       "      <td>42.196248</td>\n",
       "      <td>24.559025</td>\n",
       "      <td>98.802348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103207</td>\n",
       "      <td>25.729013</td>\n",
       "      <td>7.975400</td>\n",
       "      <td>10.803140</td>\n",
       "      <td>9.754228</td>\n",
       "      <td>0.327268</td>\n",
       "      <td>16.250392</td>\n",
       "      <td>1.152677</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.389655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>NOV</td>\n",
       "      <td>63.782516</td>\n",
       "      <td>37.252792</td>\n",
       "      <td>9.713161</td>\n",
       "      <td>52.539090</td>\n",
       "      <td>29.384204</td>\n",
       "      <td>0.391326</td>\n",
       "      <td>99.354278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.369823</td>\n",
       "      <td>9.360489</td>\n",
       "      <td>0.710853</td>\n",
       "      <td>8.425101</td>\n",
       "      <td>3.263099</td>\n",
       "      <td>17.081087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.469636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>DEC</td>\n",
       "      <td>49.151039</td>\n",
       "      <td>32.779945</td>\n",
       "      <td>8.565107</td>\n",
       "      <td>43.501253</td>\n",
       "      <td>26.403070</td>\n",
       "      <td>2.568691</td>\n",
       "      <td>99.867597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171012</td>\n",
       "      <td>30.035203</td>\n",
       "      <td>9.005973</td>\n",
       "      <td>13.063238</td>\n",
       "      <td>4.954492</td>\n",
       "      <td>6.141817</td>\n",
       "      <td>16.710648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.539618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>JAN</td>\n",
       "      <td>51.603404</td>\n",
       "      <td>27.391903</td>\n",
       "      <td>-0.148039</td>\n",
       "      <td>38.817646</td>\n",
       "      <td>20.733436</td>\n",
       "      <td>-8.513812</td>\n",
       "      <td>98.946455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353906</td>\n",
       "      <td>27.208144</td>\n",
       "      <td>9.937532</td>\n",
       "      <td>0.697591</td>\n",
       "      <td>2.158710</td>\n",
       "      <td>9.017004</td>\n",
       "      <td>14.353479</td>\n",
       "      <td>0.204561</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.529621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>FEB</td>\n",
       "      <td>55.807460</td>\n",
       "      <td>34.986696</td>\n",
       "      <td>13.475683</td>\n",
       "      <td>43.526687</td>\n",
       "      <td>24.955311</td>\n",
       "      <td>5.251212</td>\n",
       "      <td>98.128625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230043</td>\n",
       "      <td>32.970059</td>\n",
       "      <td>9.232392</td>\n",
       "      <td>25.414489</td>\n",
       "      <td>3.158557</td>\n",
       "      <td>2.563075</td>\n",
       "      <td>11.369789</td>\n",
       "      <td>0.077182</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.449640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>MAR</td>\n",
       "      <td>72.278760</td>\n",
       "      <td>42.527518</td>\n",
       "      <td>16.163708</td>\n",
       "      <td>47.173214</td>\n",
       "      <td>29.577827</td>\n",
       "      <td>6.417299</td>\n",
       "      <td>99.149593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171012</td>\n",
       "      <td>29.907703</td>\n",
       "      <td>11.045504</td>\n",
       "      <td>6.407882</td>\n",
       "      <td>6.858583</td>\n",
       "      <td>2.852181</td>\n",
       "      <td>10.275397</td>\n",
       "      <td>0.716611</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.509626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>ARP</td>\n",
       "      <td>85.305443</td>\n",
       "      <td>51.656418</td>\n",
       "      <td>21.970248</td>\n",
       "      <td>53.181639</td>\n",
       "      <td>35.382062</td>\n",
       "      <td>12.721515</td>\n",
       "      <td>98.923711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185989</td>\n",
       "      <td>27.064239</td>\n",
       "      <td>9.509505</td>\n",
       "      <td>12.758238</td>\n",
       "      <td>8.253307</td>\n",
       "      <td>0.808174</td>\n",
       "      <td>9.891667</td>\n",
       "      <td>1.736127</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.559613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>MAY</td>\n",
       "      <td>88.717714</td>\n",
       "      <td>65.765110</td>\n",
       "      <td>38.133418</td>\n",
       "      <td>64.652678</td>\n",
       "      <td>48.141753</td>\n",
       "      <td>26.138192</td>\n",
       "      <td>95.677253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593883</td>\n",
       "      <td>31.632215</td>\n",
       "      <td>8.975590</td>\n",
       "      <td>24.486700</td>\n",
       "      <td>9.510037</td>\n",
       "      <td>0.218179</td>\n",
       "      <td>9.562606</td>\n",
       "      <td>6.851201</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.719574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>JUN</td>\n",
       "      <td>94.935745</td>\n",
       "      <td>72.250194</td>\n",
       "      <td>49.378090</td>\n",
       "      <td>66.975597</td>\n",
       "      <td>55.163926</td>\n",
       "      <td>34.929410</td>\n",
       "      <td>92.224308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884931</td>\n",
       "      <td>26.598362</td>\n",
       "      <td>7.892497</td>\n",
       "      <td>9.973803</td>\n",
       "      <td>7.983505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.068762</td>\n",
       "      <td>6.553994</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.749566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>JUL</td>\n",
       "      <td>94.486551</td>\n",
       "      <td>74.924089</td>\n",
       "      <td>52.515406</td>\n",
       "      <td>69.564460</td>\n",
       "      <td>60.019149</td>\n",
       "      <td>44.308975</td>\n",
       "      <td>91.503566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084196</td>\n",
       "      <td>29.864555</td>\n",
       "      <td>7.596663</td>\n",
       "      <td>39.075101</td>\n",
       "      <td>11.250805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.522498</td>\n",
       "      <td>7.200001</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.659588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>AUG</td>\n",
       "      <td>95.769169</td>\n",
       "      <td>72.236452</td>\n",
       "      <td>49.913575</td>\n",
       "      <td>70.645707</td>\n",
       "      <td>57.160585</td>\n",
       "      <td>37.261179</td>\n",
       "      <td>92.336720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204506</td>\n",
       "      <td>33.754199</td>\n",
       "      <td>7.664999</td>\n",
       "      <td>3.963129</td>\n",
       "      <td>10.481103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.100641</td>\n",
       "      <td>6.232162</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.529621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>SEP</td>\n",
       "      <td>86.330704</td>\n",
       "      <td>64.757891</td>\n",
       "      <td>42.685359</td>\n",
       "      <td>61.761861</td>\n",
       "      <td>49.221634</td>\n",
       "      <td>31.004195</td>\n",
       "      <td>93.210550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204506</td>\n",
       "      <td>25.124887</td>\n",
       "      <td>7.115135</td>\n",
       "      <td>5.524179</td>\n",
       "      <td>7.851274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.212706</td>\n",
       "      <td>2.418011</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.529621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>OCT</td>\n",
       "      <td>75.338957</td>\n",
       "      <td>48.717642</td>\n",
       "      <td>26.178812</td>\n",
       "      <td>53.494604</td>\n",
       "      <td>35.590995</td>\n",
       "      <td>20.118526</td>\n",
       "      <td>98.458155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343522</td>\n",
       "      <td>27.792394</td>\n",
       "      <td>9.120230</td>\n",
       "      <td>8.947375</td>\n",
       "      <td>8.870945</td>\n",
       "      <td>0.458632</td>\n",
       "      <td>10.156463</td>\n",
       "      <td>0.873945</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.599603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>NOV</td>\n",
       "      <td>73.387568</td>\n",
       "      <td>42.445148</td>\n",
       "      <td>11.759290</td>\n",
       "      <td>53.810402</td>\n",
       "      <td>31.334574</td>\n",
       "      <td>6.142447</td>\n",
       "      <td>99.130187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087948</td>\n",
       "      <td>27.737506</td>\n",
       "      <td>10.504951</td>\n",
       "      <td>6.922821</td>\n",
       "      <td>11.552323</td>\n",
       "      <td>3.370795</td>\n",
       "      <td>11.110684</td>\n",
       "      <td>0.573445</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.679583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>DEC</td>\n",
       "      <td>54.851605</td>\n",
       "      <td>31.435940</td>\n",
       "      <td>7.705909</td>\n",
       "      <td>48.230410</td>\n",
       "      <td>24.561279</td>\n",
       "      <td>2.072395</td>\n",
       "      <td>99.310970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.844028</td>\n",
       "      <td>11.646561</td>\n",
       "      <td>6.928783</td>\n",
       "      <td>9.977321</td>\n",
       "      <td>8.323399</td>\n",
       "      <td>14.877116</td>\n",
       "      <td>1.181361</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.799554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>IL</td>\n",
       "      <td>1988</td>\n",
       "      <td>JAN</td>\n",
       "      <td>56.407002</td>\n",
       "      <td>25.686117</td>\n",
       "      <td>-0.995032</td>\n",
       "      <td>46.273711</td>\n",
       "      <td>17.866325</td>\n",
       "      <td>-7.806793</td>\n",
       "      <td>99.867597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>31.689132</td>\n",
       "      <td>11.978539</td>\n",
       "      <td>1.444931</td>\n",
       "      <td>3.289410</td>\n",
       "      <td>8.299551</td>\n",
       "      <td>7.312518</td>\n",
       "      <td>0.576510</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.809551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>IL</td>\n",
       "      <td>1988</td>\n",
       "      <td>FEB</td>\n",
       "      <td>59.645858</td>\n",
       "      <td>26.753927</td>\n",
       "      <td>-0.651426</td>\n",
       "      <td>40.515945</td>\n",
       "      <td>18.611068</td>\n",
       "      <td>-7.612461</td>\n",
       "      <td>98.683446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171012</td>\n",
       "      <td>30.835942</td>\n",
       "      <td>11.366350</td>\n",
       "      <td>6.048729</td>\n",
       "      <td>2.780704</td>\n",
       "      <td>9.300564</td>\n",
       "      <td>9.198702</td>\n",
       "      <td>0.044134</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.899529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>IL</td>\n",
       "      <td>1988</td>\n",
       "      <td>MAR</td>\n",
       "      <td>69.757621</td>\n",
       "      <td>38.434919</td>\n",
       "      <td>15.024229</td>\n",
       "      <td>51.018937</td>\n",
       "      <td>27.693562</td>\n",
       "      <td>8.844748</td>\n",
       "      <td>99.867597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>34.058311</td>\n",
       "      <td>11.332478</td>\n",
       "      <td>372.538106</td>\n",
       "      <td>6.725774</td>\n",
       "      <td>6.585668</td>\n",
       "      <td>10.346304</td>\n",
       "      <td>2.881596</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.909526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>IL</td>\n",
       "      <td>1988</td>\n",
       "      <td>ARP</td>\n",
       "      <td>80.916532</td>\n",
       "      <td>49.536814</td>\n",
       "      <td>24.445530</td>\n",
       "      <td>55.833496</td>\n",
       "      <td>33.898013</td>\n",
       "      <td>11.252577</td>\n",
       "      <td>98.650757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275534</td>\n",
       "      <td>41.454133</td>\n",
       "      <td>10.788335</td>\n",
       "      <td>3.927050</td>\n",
       "      <td>8.654531</td>\n",
       "      <td>0.460086</td>\n",
       "      <td>6.650997</td>\n",
       "      <td>3.396558</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.939519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>IL</td>\n",
       "      <td>1988</td>\n",
       "      <td>MAY</td>\n",
       "      <td>88.648736</td>\n",
       "      <td>63.285999</td>\n",
       "      <td>38.007417</td>\n",
       "      <td>58.065659</td>\n",
       "      <td>44.089405</td>\n",
       "      <td>26.759350</td>\n",
       "      <td>95.544515</td>\n",
       "      <td>...</td>\n",
       "      <td>1.358716</td>\n",
       "      <td>37.070596</td>\n",
       "      <td>8.760104</td>\n",
       "      <td>16.876689</td>\n",
       "      <td>6.618215</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>3.626469</td>\n",
       "      <td>3.238601</td>\n",
       "      <td>2.03</td>\n",
       "      <td>2.029497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>IL</td>\n",
       "      <td>1988</td>\n",
       "      <td>JUN</td>\n",
       "      <td>97.441783</td>\n",
       "      <td>71.362958</td>\n",
       "      <td>44.108245</td>\n",
       "      <td>65.334190</td>\n",
       "      <td>49.943229</td>\n",
       "      <td>30.379235</td>\n",
       "      <td>92.462399</td>\n",
       "      <td>...</td>\n",
       "      <td>2.432953</td>\n",
       "      <td>22.162214</td>\n",
       "      <td>7.857640</td>\n",
       "      <td>4.046168</td>\n",
       "      <td>2.947843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.199814</td>\n",
       "      <td>1.358670</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.509378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>IL</td>\n",
       "      <td>2014</td>\n",
       "      <td>JUL</td>\n",
       "      <td>93.846979</td>\n",
       "      <td>69.533818</td>\n",
       "      <td>50.719371</td>\n",
       "      <td>74.213351</td>\n",
       "      <td>58.528124</td>\n",
       "      <td>44.680399</td>\n",
       "      <td>98.567577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280616</td>\n",
       "      <td>31.631130</td>\n",
       "      <td>6.640193</td>\n",
       "      <td>2.428684</td>\n",
       "      <td>9.495698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.217399</td>\n",
       "      <td>5.176212</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.118978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>IL</td>\n",
       "      <td>2014</td>\n",
       "      <td>AUG</td>\n",
       "      <td>90.890957</td>\n",
       "      <td>72.263597</td>\n",
       "      <td>50.677898</td>\n",
       "      <td>76.059315</td>\n",
       "      <td>63.058648</td>\n",
       "      <td>43.453073</td>\n",
       "      <td>98.017750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247688</td>\n",
       "      <td>29.820666</td>\n",
       "      <td>5.866341</td>\n",
       "      <td>4.025913</td>\n",
       "      <td>11.777075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.004267</td>\n",
       "      <td>7.259140</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.739073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>IL</td>\n",
       "      <td>2014</td>\n",
       "      <td>SEP</td>\n",
       "      <td>91.921477</td>\n",
       "      <td>63.870515</td>\n",
       "      <td>39.322797</td>\n",
       "      <td>71.925580</td>\n",
       "      <td>53.068572</td>\n",
       "      <td>34.902195</td>\n",
       "      <td>98.619932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280616</td>\n",
       "      <td>29.115301</td>\n",
       "      <td>6.217876</td>\n",
       "      <td>3.105514</td>\n",
       "      <td>7.529903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.055058</td>\n",
       "      <td>4.412042</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.509130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>IL</td>\n",
       "      <td>2014</td>\n",
       "      <td>OCT</td>\n",
       "      <td>79.939694</td>\n",
       "      <td>52.377502</td>\n",
       "      <td>30.009565</td>\n",
       "      <td>63.019905</td>\n",
       "      <td>42.385667</td>\n",
       "      <td>16.371301</td>\n",
       "      <td>99.549425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142584</td>\n",
       "      <td>30.957589</td>\n",
       "      <td>7.942366</td>\n",
       "      <td>2.901838</td>\n",
       "      <td>12.432674</td>\n",
       "      <td>0.437813</td>\n",
       "      <td>4.171134</td>\n",
       "      <td>2.420886</td>\n",
       "      <td>3.58</td>\n",
       "      <td>3.579112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>IL</td>\n",
       "      <td>2014</td>\n",
       "      <td>NOV</td>\n",
       "      <td>67.480255</td>\n",
       "      <td>35.825140</td>\n",
       "      <td>9.772755</td>\n",
       "      <td>54.625033</td>\n",
       "      <td>26.391946</td>\n",
       "      <td>0.093362</td>\n",
       "      <td>98.642157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179613</td>\n",
       "      <td>31.722576</td>\n",
       "      <td>10.206118</td>\n",
       "      <td>1.708385</td>\n",
       "      <td>7.384086</td>\n",
       "      <td>7.818873</td>\n",
       "      <td>2.738053</td>\n",
       "      <td>0.033495</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>IL</td>\n",
       "      <td>2014</td>\n",
       "      <td>DEC</td>\n",
       "      <td>54.855890</td>\n",
       "      <td>33.104515</td>\n",
       "      <td>6.871596</td>\n",
       "      <td>47.711540</td>\n",
       "      <td>27.232756</td>\n",
       "      <td>-3.721176</td>\n",
       "      <td>99.246957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>27.371385</td>\n",
       "      <td>8.089446</td>\n",
       "      <td>1.598096</td>\n",
       "      <td>7.857272</td>\n",
       "      <td>3.744419</td>\n",
       "      <td>4.276190</td>\n",
       "      <td>0.128912</td>\n",
       "      <td>3.81</td>\n",
       "      <td>3.809055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>JAN</td>\n",
       "      <td>51.130063</td>\n",
       "      <td>25.235828</td>\n",
       "      <td>-7.132326</td>\n",
       "      <td>38.658598</td>\n",
       "      <td>18.235895</td>\n",
       "      <td>-13.382659</td>\n",
       "      <td>98.593333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169950</td>\n",
       "      <td>31.706604</td>\n",
       "      <td>8.372164</td>\n",
       "      <td>1.221606</td>\n",
       "      <td>6.276689</td>\n",
       "      <td>9.317115</td>\n",
       "      <td>3.029987</td>\n",
       "      <td>0.048754</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.819053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>FEB</td>\n",
       "      <td>51.533679</td>\n",
       "      <td>20.571537</td>\n",
       "      <td>-7.815383</td>\n",
       "      <td>41.202170</td>\n",
       "      <td>11.970765</td>\n",
       "      <td>-19.757363</td>\n",
       "      <td>97.397698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.970867</td>\n",
       "      <td>9.334185</td>\n",
       "      <td>1.278564</td>\n",
       "      <td>1.240099</td>\n",
       "      <td>11.237240</td>\n",
       "      <td>3.037794</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.779063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>MAR</td>\n",
       "      <td>73.101701</td>\n",
       "      <td>38.194787</td>\n",
       "      <td>-1.751123</td>\n",
       "      <td>49.947801</td>\n",
       "      <td>26.901783</td>\n",
       "      <td>-8.428940</td>\n",
       "      <td>99.345740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123844</td>\n",
       "      <td>28.651405</td>\n",
       "      <td>8.266480</td>\n",
       "      <td>1.465324</td>\n",
       "      <td>7.336679</td>\n",
       "      <td>2.717470</td>\n",
       "      <td>3.919565</td>\n",
       "      <td>0.523391</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.789060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>ARP</td>\n",
       "      <td>78.882895</td>\n",
       "      <td>52.573288</td>\n",
       "      <td>25.908735</td>\n",
       "      <td>61.805920</td>\n",
       "      <td>37.288333</td>\n",
       "      <td>11.739397</td>\n",
       "      <td>98.647477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364541</td>\n",
       "      <td>34.562780</td>\n",
       "      <td>9.528799</td>\n",
       "      <td>2.739780</td>\n",
       "      <td>11.121298</td>\n",
       "      <td>0.274590</td>\n",
       "      <td>1.776208</td>\n",
       "      <td>4.189540</td>\n",
       "      <td>3.71</td>\n",
       "      <td>3.709080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>MAY</td>\n",
       "      <td>84.884745</td>\n",
       "      <td>63.428020</td>\n",
       "      <td>35.767946</td>\n",
       "      <td>67.599624</td>\n",
       "      <td>51.224953</td>\n",
       "      <td>29.976145</td>\n",
       "      <td>98.472728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203939</td>\n",
       "      <td>31.978787</td>\n",
       "      <td>9.271944</td>\n",
       "      <td>4.577083</td>\n",
       "      <td>13.856634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.490798</td>\n",
       "      <td>5.521460</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.619102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>JUN</td>\n",
       "      <td>91.473565</td>\n",
       "      <td>70.896868</td>\n",
       "      <td>47.813800</td>\n",
       "      <td>73.311502</td>\n",
       "      <td>60.763065</td>\n",
       "      <td>41.743208</td>\n",
       "      <td>98.374647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123844</td>\n",
       "      <td>40.923783</td>\n",
       "      <td>7.503916</td>\n",
       "      <td>6.465531</td>\n",
       "      <td>13.572339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.206530</td>\n",
       "      <td>10.167240</td>\n",
       "      <td>3.65</td>\n",
       "      <td>3.649095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>JUL</td>\n",
       "      <td>90.701935</td>\n",
       "      <td>72.878642</td>\n",
       "      <td>54.529864</td>\n",
       "      <td>76.958795</td>\n",
       "      <td>63.472975</td>\n",
       "      <td>49.014850</td>\n",
       "      <td>97.220065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269692</td>\n",
       "      <td>31.476291</td>\n",
       "      <td>6.044702</td>\n",
       "      <td>3.820778</td>\n",
       "      <td>9.391236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.508345</td>\n",
       "      <td>4.724758</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.849045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>AUG</td>\n",
       "      <td>90.465476</td>\n",
       "      <td>70.885366</td>\n",
       "      <td>50.222892</td>\n",
       "      <td>72.164270</td>\n",
       "      <td>60.343242</td>\n",
       "      <td>41.443154</td>\n",
       "      <td>98.073867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280616</td>\n",
       "      <td>25.342742</td>\n",
       "      <td>5.933962</td>\n",
       "      <td>2.931901</td>\n",
       "      <td>7.129545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.308489</td>\n",
       "      <td>4.609560</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>SEP</td>\n",
       "      <td>91.118224</td>\n",
       "      <td>68.628671</td>\n",
       "      <td>43.926815</td>\n",
       "      <td>71.261417</td>\n",
       "      <td>56.466760</td>\n",
       "      <td>36.758903</td>\n",
       "      <td>98.097529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455681</td>\n",
       "      <td>28.924293</td>\n",
       "      <td>6.483753</td>\n",
       "      <td>3.815673</td>\n",
       "      <td>7.487878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.742017</td>\n",
       "      <td>4.006889</td>\n",
       "      <td>3.69</td>\n",
       "      <td>3.689085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>OCT</td>\n",
       "      <td>80.709535</td>\n",
       "      <td>55.705794</td>\n",
       "      <td>28.846762</td>\n",
       "      <td>61.902473</td>\n",
       "      <td>42.871323</td>\n",
       "      <td>17.564898</td>\n",
       "      <td>98.096296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211792</td>\n",
       "      <td>29.154354</td>\n",
       "      <td>8.834588</td>\n",
       "      <td>1.599482</td>\n",
       "      <td>7.466220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.478168</td>\n",
       "      <td>0.909474</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.699083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>NOV</td>\n",
       "      <td>70.297323</td>\n",
       "      <td>44.810163</td>\n",
       "      <td>11.241853</td>\n",
       "      <td>59.617839</td>\n",
       "      <td>35.202024</td>\n",
       "      <td>4.497239</td>\n",
       "      <td>98.394890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309611</td>\n",
       "      <td>36.901813</td>\n",
       "      <td>10.017259</td>\n",
       "      <td>4.086457</td>\n",
       "      <td>9.970229</td>\n",
       "      <td>1.246099</td>\n",
       "      <td>5.044528</td>\n",
       "      <td>1.015249</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.699083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>DEC</td>\n",
       "      <td>65.188431</td>\n",
       "      <td>38.381905</td>\n",
       "      <td>14.931579</td>\n",
       "      <td>58.616438</td>\n",
       "      <td>31.494061</td>\n",
       "      <td>-1.608238</td>\n",
       "      <td>99.769506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.124233</td>\n",
       "      <td>10.323283</td>\n",
       "      <td>5.184984</td>\n",
       "      <td>10.398913</td>\n",
       "      <td>2.988891</td>\n",
       "      <td>6.702315</td>\n",
       "      <td>2.058452</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.789060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>JAN</td>\n",
       "      <td>56.208957</td>\n",
       "      <td>28.210967</td>\n",
       "      <td>1.826907</td>\n",
       "      <td>47.001875</td>\n",
       "      <td>20.913061</td>\n",
       "      <td>-8.831172</td>\n",
       "      <td>99.346200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059394</td>\n",
       "      <td>32.207612</td>\n",
       "      <td>9.778199</td>\n",
       "      <td>1.232944</td>\n",
       "      <td>4.799819</td>\n",
       "      <td>8.511348</td>\n",
       "      <td>2.459232</td>\n",
       "      <td>0.015259</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.729075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>FEB</td>\n",
       "      <td>66.900172</td>\n",
       "      <td>33.807153</td>\n",
       "      <td>6.331705</td>\n",
       "      <td>50.693125</td>\n",
       "      <td>23.887685</td>\n",
       "      <td>-6.109805</td>\n",
       "      <td>99.734310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>36.206540</td>\n",
       "      <td>9.824466</td>\n",
       "      <td>0.836330</td>\n",
       "      <td>4.224789</td>\n",
       "      <td>6.814530</td>\n",
       "      <td>3.299300</td>\n",
       "      <td>0.565800</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>MAR</td>\n",
       "      <td>71.538246</td>\n",
       "      <td>44.964954</td>\n",
       "      <td>14.037543</td>\n",
       "      <td>57.680772</td>\n",
       "      <td>35.297947</td>\n",
       "      <td>5.115327</td>\n",
       "      <td>99.868289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.789749</td>\n",
       "      <td>9.629134</td>\n",
       "      <td>3.098442</td>\n",
       "      <td>11.949852</td>\n",
       "      <td>2.288162</td>\n",
       "      <td>3.608914</td>\n",
       "      <td>2.883812</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.669090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>ARP</td>\n",
       "      <td>82.089725</td>\n",
       "      <td>52.325395</td>\n",
       "      <td>25.594821</td>\n",
       "      <td>59.313656</td>\n",
       "      <td>38.906985</td>\n",
       "      <td>12.135843</td>\n",
       "      <td>99.246957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.691810</td>\n",
       "      <td>39.165480</td>\n",
       "      <td>9.512475</td>\n",
       "      <td>2.662589</td>\n",
       "      <td>10.297377</td>\n",
       "      <td>1.718848</td>\n",
       "      <td>2.704257</td>\n",
       "      <td>3.283336</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.669090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>MAY</td>\n",
       "      <td>86.822760</td>\n",
       "      <td>61.317018</td>\n",
       "      <td>37.123673</td>\n",
       "      <td>67.550974</td>\n",
       "      <td>48.989342</td>\n",
       "      <td>24.687457</td>\n",
       "      <td>98.782513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572066</td>\n",
       "      <td>31.258585</td>\n",
       "      <td>8.051852</td>\n",
       "      <td>2.985320</td>\n",
       "      <td>13.488475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.162346</td>\n",
       "      <td>6.827281</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.819053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>JUN</td>\n",
       "      <td>93.436156</td>\n",
       "      <td>72.669555</td>\n",
       "      <td>49.718113</td>\n",
       "      <td>73.627985</td>\n",
       "      <td>58.861639</td>\n",
       "      <td>37.094091</td>\n",
       "      <td>98.280248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423862</td>\n",
       "      <td>26.449861</td>\n",
       "      <td>7.022661</td>\n",
       "      <td>2.645324</td>\n",
       "      <td>6.599050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.866101</td>\n",
       "      <td>4.338305</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.919028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>JUL</td>\n",
       "      <td>92.433461</td>\n",
       "      <td>74.195493</td>\n",
       "      <td>53.968246</td>\n",
       "      <td>77.715821</td>\n",
       "      <td>64.993758</td>\n",
       "      <td>46.259779</td>\n",
       "      <td>99.336052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247688</td>\n",
       "      <td>33.613471</td>\n",
       "      <td>6.212372</td>\n",
       "      <td>5.552655</td>\n",
       "      <td>11.304979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.765973</td>\n",
       "      <td>9.193705</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>AUG</td>\n",
       "      <td>89.771611</td>\n",
       "      <td>73.976134</td>\n",
       "      <td>53.152785</td>\n",
       "      <td>75.197298</td>\n",
       "      <td>65.722541</td>\n",
       "      <td>46.370412</td>\n",
       "      <td>98.526977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342538</td>\n",
       "      <td>29.407200</td>\n",
       "      <td>5.602346</td>\n",
       "      <td>4.854680</td>\n",
       "      <td>10.822233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.440266</td>\n",
       "      <td>7.746643</td>\n",
       "      <td>3.17</td>\n",
       "      <td>3.169214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>SEP</td>\n",
       "      <td>90.539780</td>\n",
       "      <td>68.526421</td>\n",
       "      <td>47.780800</td>\n",
       "      <td>72.822394</td>\n",
       "      <td>58.098483</td>\n",
       "      <td>31.084937</td>\n",
       "      <td>99.467762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185766</td>\n",
       "      <td>26.270849</td>\n",
       "      <td>6.906921</td>\n",
       "      <td>3.616776</td>\n",
       "      <td>9.658813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.307687</td>\n",
       "      <td>4.115569</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.269189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>OCT</td>\n",
       "      <td>83.401525</td>\n",
       "      <td>58.012848</td>\n",
       "      <td>33.431914</td>\n",
       "      <td>65.081202</td>\n",
       "      <td>47.633134</td>\n",
       "      <td>27.152489</td>\n",
       "      <td>99.670723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302619</td>\n",
       "      <td>31.092277</td>\n",
       "      <td>8.219588</td>\n",
       "      <td>1.907746</td>\n",
       "      <td>8.156643</td>\n",
       "      <td>0.022274</td>\n",
       "      <td>0.872271</td>\n",
       "      <td>3.452099</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.299182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>NOV</td>\n",
       "      <td>76.323585</td>\n",
       "      <td>44.534557</td>\n",
       "      <td>16.508950</td>\n",
       "      <td>60.101894</td>\n",
       "      <td>35.188458</td>\n",
       "      <td>10.089226</td>\n",
       "      <td>99.769506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189700</td>\n",
       "      <td>35.356631</td>\n",
       "      <td>7.994466</td>\n",
       "      <td>2.237944</td>\n",
       "      <td>6.781341</td>\n",
       "      <td>1.083395</td>\n",
       "      <td>3.198530</td>\n",
       "      <td>1.929992</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.319177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>DEC</td>\n",
       "      <td>56.499747</td>\n",
       "      <td>28.025835</td>\n",
       "      <td>-5.724015</td>\n",
       "      <td>52.141124</td>\n",
       "      <td>20.537073</td>\n",
       "      <td>-14.910411</td>\n",
       "      <td>99.769506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.388733</td>\n",
       "      <td>9.509581</td>\n",
       "      <td>1.391766</td>\n",
       "      <td>6.026444</td>\n",
       "      <td>7.998503</td>\n",
       "      <td>4.082039</td>\n",
       "      <td>0.110676</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.469140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    State  YEAR MONTH   TMP_HIGH    TMP_AVG    TMP_LOW    DP_HIGH     DP_AVG  \\\n",
       "0      IL  1986   JAN  55.953658  27.617217  -2.980925  42.880246  19.992262   \n",
       "1      IL  1986   FEB  53.838886  28.731112   0.353638  46.537659  23.958440   \n",
       "2      IL  1986   MAR  77.668458  41.937001  13.926317  51.205991  29.862688   \n",
       "3      IL  1986   ARP  85.651529  52.949283  25.351909  60.274139  37.049373   \n",
       "4      IL  1986   MAY  85.415668  62.338688  36.602102  61.467698  47.555928   \n",
       "5      IL  1986   JUN  93.180721  71.263159  47.746898  67.021368  55.502022   \n",
       "6      IL  1986   JUL  93.651100  75.325134  57.224505  71.740380  60.440695   \n",
       "7      IL  1986   AUG  88.032940  68.355756  43.240501  70.470204  53.535626   \n",
       "8      IL  1986   SEP  86.350515  66.418060  39.848014  66.092621  53.110278   \n",
       "9      IL  1986   OCT  74.910045  52.914618  29.721084  61.366736  42.196248   \n",
       "10     IL  1986   NOV  63.782516  37.252792   9.713161  52.539090  29.384204   \n",
       "11     IL  1986   DEC  49.151039  32.779945   8.565107  43.501253  26.403070   \n",
       "12     IL  1987   JAN  51.603404  27.391903  -0.148039  38.817646  20.733436   \n",
       "13     IL  1987   FEB  55.807460  34.986696  13.475683  43.526687  24.955311   \n",
       "14     IL  1987   MAR  72.278760  42.527518  16.163708  47.173214  29.577827   \n",
       "15     IL  1987   ARP  85.305443  51.656418  21.970248  53.181639  35.382062   \n",
       "16     IL  1987   MAY  88.717714  65.765110  38.133418  64.652678  48.141753   \n",
       "17     IL  1987   JUN  94.935745  72.250194  49.378090  66.975597  55.163926   \n",
       "18     IL  1987   JUL  94.486551  74.924089  52.515406  69.564460  60.019149   \n",
       "19     IL  1987   AUG  95.769169  72.236452  49.913575  70.645707  57.160585   \n",
       "20     IL  1987   SEP  86.330704  64.757891  42.685359  61.761861  49.221634   \n",
       "21     IL  1987   OCT  75.338957  48.717642  26.178812  53.494604  35.590995   \n",
       "22     IL  1987   NOV  73.387568  42.445148  11.759290  53.810402  31.334574   \n",
       "23     IL  1987   DEC  54.851605  31.435940   7.705909  48.230410  24.561279   \n",
       "24     IL  1988   JAN  56.407002  25.686117  -0.995032  46.273711  17.866325   \n",
       "25     IL  1988   FEB  59.645858  26.753927  -0.651426  40.515945  18.611068   \n",
       "26     IL  1988   MAR  69.757621  38.434919  15.024229  51.018937  27.693562   \n",
       "27     IL  1988   ARP  80.916532  49.536814  24.445530  55.833496  33.898013   \n",
       "28     IL  1988   MAY  88.648736  63.285999  38.007417  58.065659  44.089405   \n",
       "29     IL  1988   JUN  97.441783  71.362958  44.108245  65.334190  49.943229   \n",
       "..    ...   ...   ...        ...        ...        ...        ...        ...   \n",
       "340    IL  2014   JUL  93.846979  69.533818  50.719371  74.213351  58.528124   \n",
       "341    IL  2014   AUG  90.890957  72.263597  50.677898  76.059315  63.058648   \n",
       "342    IL  2014   SEP  91.921477  63.870515  39.322797  71.925580  53.068572   \n",
       "343    IL  2014   OCT  79.939694  52.377502  30.009565  63.019905  42.385667   \n",
       "344    IL  2014   NOV  67.480255  35.825140   9.772755  54.625033  26.391946   \n",
       "345    IL  2014   DEC  54.855890  33.104515   6.871596  47.711540  27.232756   \n",
       "346    IL  2015   JAN  51.130063  25.235828  -7.132326  38.658598  18.235895   \n",
       "347    IL  2015   FEB  51.533679  20.571537  -7.815383  41.202170  11.970765   \n",
       "348    IL  2015   MAR  73.101701  38.194787  -1.751123  49.947801  26.901783   \n",
       "349    IL  2015   ARP  78.882895  52.573288  25.908735  61.805920  37.288333   \n",
       "350    IL  2015   MAY  84.884745  63.428020  35.767946  67.599624  51.224953   \n",
       "351    IL  2015   JUN  91.473565  70.896868  47.813800  73.311502  60.763065   \n",
       "352    IL  2015   JUL  90.701935  72.878642  54.529864  76.958795  63.472975   \n",
       "353    IL  2015   AUG  90.465476  70.885366  50.222892  72.164270  60.343242   \n",
       "354    IL  2015   SEP  91.118224  68.628671  43.926815  71.261417  56.466760   \n",
       "355    IL  2015   OCT  80.709535  55.705794  28.846762  61.902473  42.871323   \n",
       "356    IL  2015   NOV  70.297323  44.810163  11.241853  59.617839  35.202024   \n",
       "357    IL  2015   DEC  65.188431  38.381905  14.931579  58.616438  31.494061   \n",
       "358    IL  2016   JAN  56.208957  28.210967   1.826907  47.001875  20.913061   \n",
       "359    IL  2016   FEB  66.900172  33.807153   6.331705  50.693125  23.887685   \n",
       "360    IL  2016   MAR  71.538246  44.964954  14.037543  57.680772  35.297947   \n",
       "361    IL  2016   ARP  82.089725  52.325395  25.594821  59.313656  38.906985   \n",
       "362    IL  2016   MAY  86.822760  61.317018  37.123673  67.550974  48.989342   \n",
       "363    IL  2016   JUN  93.436156  72.669555  49.718113  73.627985  58.861639   \n",
       "364    IL  2016   JUL  92.433461  74.195493  53.968246  77.715821  64.993758   \n",
       "365    IL  2016   AUG  89.771611  73.976134  53.152785  75.197298  65.722541   \n",
       "366    IL  2016   SEP  90.539780  68.526421  47.780800  72.822394  58.098483   \n",
       "367    IL  2016   OCT  83.401525  58.012848  33.431914  65.081202  47.633134   \n",
       "368    IL  2016   NOV  76.323585  44.534557  16.508950  60.101894  35.188458   \n",
       "369    IL  2016   DEC  56.499747  28.025835  -5.724015  52.141124  20.537073   \n",
       "\n",
       "        DP_LOW   HUM_HIGH        ...          VIS_LOW  WIND_HIGH   WIND_AVG  \\\n",
       "0    -8.511984  99.026800        ...         0.186718  29.600197  11.077783   \n",
       "1    -4.555208  99.510367        ...         0.109089  24.679482   9.622850   \n",
       "2     2.748564  99.138166        ...         0.634101  36.374250  12.349509   \n",
       "3    11.479874  99.558871        ...         0.474198  28.918771  10.770362   \n",
       "4    27.795232  95.246992        ...         0.280723  33.072546   8.983943   \n",
       "5    33.450048  92.041052        ...         0.950015  25.666101   8.317023   \n",
       "6    41.882198  91.024753        ...         0.360831  30.924584   7.668664   \n",
       "7    34.387899  97.288101        ...         0.123844  23.558382   6.839216   \n",
       "8    30.557896  92.975847        ...         0.061922  31.473925   7.953490   \n",
       "9    24.559025  98.802348        ...         0.103207  25.729013   7.975400   \n",
       "10    0.391326  99.354278        ...         0.000000  27.369823   9.360489   \n",
       "11    2.568691  99.867597        ...         0.171012  30.035203   9.005973   \n",
       "12   -8.513812  98.946455        ...         0.353906  27.208144   9.937532   \n",
       "13    5.251212  98.128625        ...         0.230043  32.970059   9.232392   \n",
       "14    6.417299  99.149593        ...         0.171012  29.907703  11.045504   \n",
       "15   12.721515  98.923711        ...         0.185989  27.064239   9.509505   \n",
       "16   26.138192  95.677253        ...         0.593883  31.632215   8.975590   \n",
       "17   34.929410  92.224308        ...         0.884931  26.598362   7.892497   \n",
       "18   44.308975  91.503566        ...         0.084196  29.864555   7.596663   \n",
       "19   37.261179  92.336720        ...         0.204506  33.754199   7.664999   \n",
       "20   31.004195  93.210550        ...         0.204506  25.124887   7.115135   \n",
       "21   20.118526  98.458155        ...         0.343522  27.792394   9.120230   \n",
       "22    6.142447  99.130187        ...         0.087948  27.737506  10.504951   \n",
       "23    2.072395  99.310970        ...         0.000000  38.844028  11.646561   \n",
       "24   -7.806793  99.867597        ...         0.061922  31.689132  11.978539   \n",
       "25   -7.612461  98.683446        ...         0.171012  30.835942  11.366350   \n",
       "26    8.844748  99.867597        ...         0.109089  34.058311  11.332478   \n",
       "27   11.252577  98.650757        ...         0.275534  41.454133  10.788335   \n",
       "28   26.759350  95.544515        ...         1.358716  37.070596   8.760104   \n",
       "29   30.379235  92.462399        ...         2.432953  22.162214   7.857640   \n",
       "..         ...        ...        ...              ...        ...        ...   \n",
       "340  44.680399  98.567577        ...         0.280616  31.631130   6.640193   \n",
       "341  43.453073  98.017750        ...         0.247688  29.820666   5.866341   \n",
       "342  34.902195  98.619932        ...         0.280616  29.115301   6.217876   \n",
       "343  16.371301  99.549425        ...         0.142584  30.957589   7.942366   \n",
       "344   0.093362  98.642157        ...         0.179613  31.722576  10.206118   \n",
       "345  -3.721176  99.246957        ...         0.061922  27.371385   8.089446   \n",
       "346 -13.382659  98.593333        ...         0.169950  31.706604   8.372164   \n",
       "347 -19.757363  97.397698        ...         0.000000  33.970867   9.334185   \n",
       "348  -8.428940  99.345740        ...         0.123844  28.651405   8.266480   \n",
       "349  11.739397  98.647477        ...         0.364541  34.562780   9.528799   \n",
       "350  29.976145  98.472728        ...         0.203939  31.978787   9.271944   \n",
       "351  41.743208  98.374647        ...         0.123844  40.923783   7.503916   \n",
       "352  49.014850  97.220065        ...         0.269692  31.476291   6.044702   \n",
       "353  41.443154  98.073867        ...         0.280616  25.342742   5.933962   \n",
       "354  36.758903  98.097529        ...         0.455681  28.924293   6.483753   \n",
       "355  17.564898  98.096296        ...         0.211792  29.154354   8.834588   \n",
       "356   4.497239  98.394890        ...         0.309611  36.901813  10.017259   \n",
       "357  -1.608238  99.769506        ...         0.000000  38.124233  10.323283   \n",
       "358  -8.831172  99.346200        ...         0.059394  32.207612   9.778199   \n",
       "359  -6.109805  99.734310        ...         0.061922  36.206540   9.824466   \n",
       "360   5.115327  99.868289        ...         0.000000  34.789749   9.629134   \n",
       "361  12.135843  99.246957        ...         0.691810  39.165480   9.512475   \n",
       "362  24.687457  98.782513        ...         0.572066  31.258585   8.051852   \n",
       "363  37.094091  98.280248        ...         0.423862  26.449861   7.022661   \n",
       "364  46.259779  99.336052        ...         0.247688  33.613471   6.212372   \n",
       "365  46.370412  98.526977        ...         0.342538  29.407200   5.602346   \n",
       "366  31.084937  99.467762        ...         0.185766  26.270849   6.906921   \n",
       "367  27.152489  99.670723        ...         0.302619  31.092277   8.219588   \n",
       "368  10.089226  99.769506        ...         0.189700  35.356631   7.994466   \n",
       "369 -14.910411  99.769506        ...         0.000000  29.388733   9.509581   \n",
       "\n",
       "         PRECIP  NUMDAY_RAIN  NUM_DAYS_SNOW  NUM_DAYS_FOG  NUM_DAYS_THNDRSTRM  \\\n",
       "0      0.479003     2.544772       7.584738      9.707042            0.000000   \n",
       "1      1.722515     5.510271       9.231277     15.203328            0.658457   \n",
       "2     15.116988     5.934169       3.868506      9.890731            1.055313   \n",
       "3     17.947352     9.195066       1.960945      7.611098            2.058521   \n",
       "4     15.212715     9.610446       0.327268     14.161837            4.663441   \n",
       "5     10.598875     8.135348       0.000000     12.649299            4.995009   \n",
       "6     11.201697     9.899297       0.000000     12.561325            6.286502   \n",
       "7      1.162475     6.987723       0.000000     11.848591            1.872076   \n",
       "8      6.651649    12.019534       0.109089     13.926898            5.663013   \n",
       "9     10.803140     9.754228       0.327268     16.250392            1.152677   \n",
       "10     0.710853     8.425101       3.263099     17.081087            0.000000   \n",
       "11    13.063238     4.954492       6.141817     16.710648            0.000000   \n",
       "12     0.697591     2.158710       9.017004     14.353479            0.204561   \n",
       "13    25.414489     3.158557       2.563075     11.369789            0.077182   \n",
       "14     6.407882     6.858583       2.852181     10.275397            0.716611   \n",
       "15    12.758238     8.253307       0.808174      9.891667            1.736127   \n",
       "16    24.486700     9.510037       0.218179      9.562606            6.851201   \n",
       "17     9.973803     7.983505       0.000000     11.068762            6.553994   \n",
       "18    39.075101    11.250805       0.000000     16.522498            7.200001   \n",
       "19     3.963129    10.481103       0.000000     14.100641            6.232162   \n",
       "20     5.524179     7.851274       0.000000     12.212706            2.418011   \n",
       "21     8.947375     8.870945       0.458632     10.156463            0.873945   \n",
       "22     6.922821    11.552323       3.370795     11.110684            0.573445   \n",
       "23     6.928783     9.977321       8.323399     14.877116            1.181361   \n",
       "24     1.444931     3.289410       8.299551      7.312518            0.576510   \n",
       "25     6.048729     2.780704       9.300564      9.198702            0.044134   \n",
       "26   372.538106     6.725774       6.585668     10.346304            2.881596   \n",
       "27     3.927050     8.654531       0.460086      6.650997            3.396558   \n",
       "28    16.876689     6.618215       0.109089      3.626469            3.238601   \n",
       "29     4.046168     2.947843       0.000000      3.199814            1.358670   \n",
       "..          ...          ...            ...           ...                 ...   \n",
       "340    2.428684     9.495698       0.000000      2.217399            5.176212   \n",
       "341    4.025913    11.777075       0.000000      5.004267            7.259140   \n",
       "342    3.105514     7.529903       0.000000      4.055058            4.412042   \n",
       "343    2.901838    12.432674       0.437813      4.171134            2.420886   \n",
       "344    1.708385     7.384086       7.818873      2.738053            0.033495   \n",
       "345    1.598096     7.857272       3.744419      4.276190            0.128912   \n",
       "346    1.221606     6.276689       9.317115      3.029987            0.048754   \n",
       "347    1.278564     1.240099      11.237240      3.037794            0.061922   \n",
       "348    1.465324     7.336679       2.717470      3.919565            0.523391   \n",
       "349    2.739780    11.121298       0.274590      1.776208            4.189540   \n",
       "350    4.577083    13.856634       0.000000      1.490798            5.521460   \n",
       "351    6.465531    13.572339       0.000000      4.206530           10.167240   \n",
       "352    3.820778     9.391236       0.000000      3.508345            4.724758   \n",
       "353    2.931901     7.129545       0.000000      2.308489            4.609560   \n",
       "354    3.815673     7.487878       0.000000      1.742017            4.006889   \n",
       "355    1.599482     7.466220       0.000000      2.478168            0.909474   \n",
       "356    4.086457     9.970229       1.246099      5.044528            1.015249   \n",
       "357    5.184984    10.398913       2.988891      6.702315            2.058452   \n",
       "358    1.232944     4.799819       8.511348      2.459232            0.015259   \n",
       "359    0.836330     4.224789       6.814530      3.299300            0.565800   \n",
       "360    3.098442    11.949852       2.288162      3.608914            2.883812   \n",
       "361    2.662589    10.297377       1.718848      2.704257            3.283336   \n",
       "362    2.985320    13.488475       0.000000      2.162346            6.827281   \n",
       "363    2.645324     6.599050       0.000000      1.866101            4.338305   \n",
       "364    5.552655    11.304979       0.000000      4.765973            9.193705   \n",
       "365    4.854680    10.822233       0.000000      3.440266            7.746643   \n",
       "366    3.616776     9.658813       0.000000      3.307687            4.115569   \n",
       "367    1.907746     8.156643       0.022274      0.872271            3.452099   \n",
       "368    2.237944     6.781341       1.083395      3.198530            1.929992   \n",
       "369    1.391766     6.026444       7.998503      4.082039            0.110676   \n",
       "\n",
       "     Price  Discounted_Price  \n",
       "0     2.35          2.349417  \n",
       "1     2.38          2.379410  \n",
       "2     2.35          2.349417  \n",
       "3     2.34          2.339420  \n",
       "4     2.45          2.449393  \n",
       "5     2.45          2.449393  \n",
       "6     2.04          2.039494  \n",
       "7     1.77          1.769561  \n",
       "8     1.52          1.519623  \n",
       "9     1.39          1.389655  \n",
       "10    1.47          1.469636  \n",
       "11    1.54          1.539618  \n",
       "12    1.53          1.529621  \n",
       "13    1.45          1.449640  \n",
       "14    1.51          1.509626  \n",
       "15    1.56          1.559613  \n",
       "16    1.72          1.719574  \n",
       "17    1.75          1.749566  \n",
       "18    1.66          1.659588  \n",
       "19    1.53          1.529621  \n",
       "20    1.53          1.529621  \n",
       "21    1.60          1.599603  \n",
       "22    1.68          1.679583  \n",
       "23    1.80          1.799554  \n",
       "24    1.81          1.809551  \n",
       "25    1.90          1.899529  \n",
       "26    1.91          1.909526  \n",
       "27    1.94          1.939519  \n",
       "28    2.03          2.029497  \n",
       "29    2.51          2.509378  \n",
       "..     ...               ...  \n",
       "340   4.12          4.118978  \n",
       "341   3.74          3.739073  \n",
       "342   3.51          3.509130  \n",
       "343   3.58          3.579112  \n",
       "344   3.63          3.629100  \n",
       "345   3.81          3.809055  \n",
       "346   3.82          3.819053  \n",
       "347   3.78          3.779063  \n",
       "348   3.79          3.789060  \n",
       "349   3.71          3.709080  \n",
       "350   3.62          3.619102  \n",
       "351   3.65          3.649095  \n",
       "352   3.85          3.849045  \n",
       "353   3.63          3.629100  \n",
       "354   3.69          3.689085  \n",
       "355   3.70          3.699083  \n",
       "356   3.70          3.699083  \n",
       "357   3.79          3.789060  \n",
       "358   3.73          3.729075  \n",
       "359   3.63          3.629100  \n",
       "360   3.67          3.669090  \n",
       "361   3.67          3.669090  \n",
       "362   3.82          3.819053  \n",
       "363   3.92          3.919028  \n",
       "364   3.63          3.629100  \n",
       "365   3.17          3.169214  \n",
       "366   3.27          3.269189  \n",
       "367   3.30          3.299182  \n",
       "368   3.32          3.319177  \n",
       "369   3.47          3.469140  \n",
       "\n",
       "[370 rows x 27 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print (ps)\n",
    "df['Price'] = price_updated['Value']\n",
    "df['Discounted_Price'] = price_updated['Discounted_Value']\n",
    "df['State'] = 'IL'\n",
    "\n",
    "# Reorder State Column\n",
    "cols = df.columns.tolist()\n",
    "#cols\n",
    "\n",
    "cols.insert(1, cols.pop(cols.index('State')))\n",
    "\n",
    "df = df.reindex(columns = cols)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "w_avg_model_scaled = df\n",
    "w_avg_model_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    State  YEAR MONTH   TMP_HIGH    TMP_AVG    TMP_LOW    DP_HIGH     DP_AVG  \\\n",
      "168    IL  2000   JAN  60.098523  24.919939  -5.750129  50.876803  18.079444   \n",
      "169    IL  2000   FEB  53.769218  31.870521   5.378496  44.864486  25.930720   \n",
      "170    IL  2000   MAY  84.325735  62.360525  37.698853  68.870647  51.653895   \n",
      "171    IL  2000   JUN  88.622913  69.307943  52.228400  71.995094  60.166479   \n",
      "172    IL  2000   JUL  87.461845  70.466481  51.066055  74.704992  62.448314   \n",
      "173    IL  2000   AUG  89.361775  71.781012  56.695409  74.533066  64.284915   \n",
      "174    IL  2000   SEP  90.167238  63.849101  36.705700  71.408262  52.829779   \n",
      "175    IL  2000   OCT  81.999760  54.605762  25.520160  62.377842  44.455570   \n",
      "176    IL  2000   NOV  74.379046  37.371940   9.233327  59.321373  29.790357   \n",
      "177    IL  2000   DEC  45.127840  19.968293  -7.283182  37.862855  14.533100   \n",
      "\n",
      "        DP_LOW    HUM_HIGH        ...          VIS_LOW  WIND_HIGH  WIND_AVG  \\\n",
      "168 -19.152746  100.000000        ...         0.061922  28.491682  9.674976   \n",
      "169  -0.570920  100.000000        ...         0.044134  28.925880  9.571381   \n",
      "170  25.975416   98.988685        ...         0.570830  30.201699  9.129810   \n",
      "171  45.635832   99.068140        ...         0.408795  27.953090  8.176592   \n",
      "172  46.322380   99.345463        ...         0.364541  34.450706  5.869163   \n",
      "173  49.179987   98.882374        ...         0.540686  22.741620  6.606653   \n",
      "174  30.919987  100.000000        ...         0.390551  23.160926  7.234548   \n",
      "175  19.058809  100.000000        ...         0.000000  25.779084  6.906690   \n",
      "176   2.151880  100.000000        ...         0.464038  26.935924  9.159310   \n",
      "177 -28.467988  100.000000        ...         0.061922  29.538696  9.032580   \n",
      "\n",
      "         PRECIP  NUMDAY_RAIN  NUM_DAYS_SNOW  NUM_DAYS_FOG  NUM_DAYS_THNDRSTRM  \\\n",
      "168    0.769395     4.527847       7.865567      3.129778            0.388589   \n",
      "169  287.022956     3.572751       4.803434      2.284139            0.385381   \n",
      "170    4.098408    10.428558       0.000000      1.610892            4.896486   \n",
      "171    3.466575     9.051719       0.000000      1.014093            4.340215   \n",
      "172    1.547238     7.626151       0.000000      2.554090            3.622915   \n",
      "173    0.749923     3.978840       0.000000      1.372018            2.996176   \n",
      "174    2.481654     7.601901       0.000000      1.483107            2.869120   \n",
      "175    1.183908     5.560382       0.171634      5.240789            1.311068   \n",
      "176    1.951554     8.234563       5.222024      1.607092            0.741233   \n",
      "177    0.743131     2.996964      15.732234      4.902815            0.125816   \n",
      "\n",
      "     Price  Discounted_Price  \n",
      "168   1.97          1.969512  \n",
      "169   2.03          2.029497  \n",
      "170   2.20          2.199455  \n",
      "171   1.89          1.889531  \n",
      "172   1.66          1.659588  \n",
      "173   1.54          1.539618  \n",
      "174   1.64          1.639593  \n",
      "175   1.80          1.799554  \n",
      "176   1.92          1.919524  \n",
      "177   2.03          2.029497  \n",
      "\n",
      "[10 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "w_group = w_avg_model_scaled.groupby([\"YEAR\"])\n",
    "year_list = w_avg_model_scaled[\"YEAR\"].drop_duplicates()\n",
    "\n",
    "for year in year_list:\n",
    "    temp_df = w_group.get_group(year)\n",
    "    #print(str(year) + \": \" +str(len(temp_df[\"PRICE\"].dropna())))\n",
    "\n",
    "print(w_group.get_group(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>TMP_HIGH</th>\n",
       "      <th>TMP_AVG</th>\n",
       "      <th>TMP_LOW</th>\n",
       "      <th>DP_HIGH</th>\n",
       "      <th>DP_AVG</th>\n",
       "      <th>DP_LOW</th>\n",
       "      <th>HUM_HIGH</th>\n",
       "      <th>...</th>\n",
       "      <th>VIS_LOW</th>\n",
       "      <th>WIND_HIGH</th>\n",
       "      <th>WIND_AVG</th>\n",
       "      <th>PRECIP</th>\n",
       "      <th>NUMDAY_RAIN</th>\n",
       "      <th>NUM_DAYS_SNOW</th>\n",
       "      <th>NUM_DAYS_FOG</th>\n",
       "      <th>NUM_DAYS_THNDRSTRM</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNTED_PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>JAN</td>\n",
       "      <td>55.953658</td>\n",
       "      <td>27.617217</td>\n",
       "      <td>-2.980925</td>\n",
       "      <td>42.880246</td>\n",
       "      <td>19.992262</td>\n",
       "      <td>-8.511984</td>\n",
       "      <td>99.026800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186718</td>\n",
       "      <td>29.600197</td>\n",
       "      <td>11.077783</td>\n",
       "      <td>0.479003</td>\n",
       "      <td>2.544772</td>\n",
       "      <td>7.584738</td>\n",
       "      <td>9.707042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.349417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>FEB</td>\n",
       "      <td>53.838886</td>\n",
       "      <td>28.731112</td>\n",
       "      <td>0.353638</td>\n",
       "      <td>46.537659</td>\n",
       "      <td>23.958440</td>\n",
       "      <td>-4.555208</td>\n",
       "      <td>99.510367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>24.679482</td>\n",
       "      <td>9.622850</td>\n",
       "      <td>1.722515</td>\n",
       "      <td>5.510271</td>\n",
       "      <td>9.231277</td>\n",
       "      <td>15.203328</td>\n",
       "      <td>0.658457</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.379410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>MAR</td>\n",
       "      <td>77.668458</td>\n",
       "      <td>41.937001</td>\n",
       "      <td>13.926317</td>\n",
       "      <td>51.205991</td>\n",
       "      <td>29.862688</td>\n",
       "      <td>2.748564</td>\n",
       "      <td>99.138166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634101</td>\n",
       "      <td>36.374250</td>\n",
       "      <td>12.349509</td>\n",
       "      <td>15.116988</td>\n",
       "      <td>5.934169</td>\n",
       "      <td>3.868506</td>\n",
       "      <td>9.890731</td>\n",
       "      <td>1.055313</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.349417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>ARP</td>\n",
       "      <td>85.651529</td>\n",
       "      <td>52.949283</td>\n",
       "      <td>25.351909</td>\n",
       "      <td>60.274139</td>\n",
       "      <td>37.049373</td>\n",
       "      <td>11.479874</td>\n",
       "      <td>99.558871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474198</td>\n",
       "      <td>28.918771</td>\n",
       "      <td>10.770362</td>\n",
       "      <td>17.947352</td>\n",
       "      <td>9.195066</td>\n",
       "      <td>1.960945</td>\n",
       "      <td>7.611098</td>\n",
       "      <td>2.058521</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.339420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>MAY</td>\n",
       "      <td>85.415668</td>\n",
       "      <td>62.338688</td>\n",
       "      <td>36.602102</td>\n",
       "      <td>61.467698</td>\n",
       "      <td>47.555928</td>\n",
       "      <td>27.795232</td>\n",
       "      <td>95.246992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280723</td>\n",
       "      <td>33.072546</td>\n",
       "      <td>8.983943</td>\n",
       "      <td>15.212715</td>\n",
       "      <td>9.610446</td>\n",
       "      <td>0.327268</td>\n",
       "      <td>14.161837</td>\n",
       "      <td>4.663441</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.449393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  STATE  YEAR MONTH   TMP_HIGH    TMP_AVG    TMP_LOW    DP_HIGH     DP_AVG  \\\n",
       "0    IL  1986   JAN  55.953658  27.617217  -2.980925  42.880246  19.992262   \n",
       "1    IL  1986   FEB  53.838886  28.731112   0.353638  46.537659  23.958440   \n",
       "2    IL  1986   MAR  77.668458  41.937001  13.926317  51.205991  29.862688   \n",
       "3    IL  1986   ARP  85.651529  52.949283  25.351909  60.274139  37.049373   \n",
       "4    IL  1986   MAY  85.415668  62.338688  36.602102  61.467698  47.555928   \n",
       "\n",
       "      DP_LOW   HUM_HIGH        ...          VIS_LOW  WIND_HIGH   WIND_AVG  \\\n",
       "0  -8.511984  99.026800        ...         0.186718  29.600197  11.077783   \n",
       "1  -4.555208  99.510367        ...         0.109089  24.679482   9.622850   \n",
       "2   2.748564  99.138166        ...         0.634101  36.374250  12.349509   \n",
       "3  11.479874  99.558871        ...         0.474198  28.918771  10.770362   \n",
       "4  27.795232  95.246992        ...         0.280723  33.072546   8.983943   \n",
       "\n",
       "      PRECIP  NUMDAY_RAIN  NUM_DAYS_SNOW  NUM_DAYS_FOG  NUM_DAYS_THNDRSTRM  \\\n",
       "0   0.479003     2.544772       7.584738      9.707042            0.000000   \n",
       "1   1.722515     5.510271       9.231277     15.203328            0.658457   \n",
       "2  15.116988     5.934169       3.868506      9.890731            1.055313   \n",
       "3  17.947352     9.195066       1.960945      7.611098            2.058521   \n",
       "4  15.212715     9.610446       0.327268     14.161837            4.663441   \n",
       "\n",
       "   PRICE  DISCOUNTED_PRICE  \n",
       "0   2.35          2.349417  \n",
       "1   2.38          2.379410  \n",
       "2   2.35          2.349417  \n",
       "3   2.34          2.339420  \n",
       "4   2.45          2.449393  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List to store results\n",
    "# Stores the performance on test set.\n",
    "model_results = []\n",
    "#  Stores the name of model\n",
    "model_name = []\n",
    "\n",
    "# Making all column names UPPER CASE --> inline with the weather data file\n",
    "w_avg_model_scaled.columns = map(str.upper, w_avg_model_scaled.columns)\n",
    "\n",
    "# rename UNNAMED:0 to INDEX\n",
    "#input_data.rename(columns={'UNNAMED: 0': 'INDEX'}, inplace=True)\n",
    "\n",
    "# drop UNNAMED:0.1\n",
    "#input_data = input_data.drop('INDEX', axis=1)\n",
    "\n",
    "w_avg_model_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Original data: (370, 27)\n",
      "\n",
      "Variables: Index(['TMP_HIGH', 'TMP_AVG', 'TMP_LOW', 'DP_HIGH', 'DP_AVG', 'DP_LOW',\n",
      "       'HUM_HIGH', 'HUM_AVG', 'HUM_LOW', 'SEALVL_HIGH', 'SEALVL_AVG',\n",
      "       'SEALVL_LOW', 'VIS_HIGH', 'VIS_AVG', 'VIS_LOW', 'WIND_HIGH', 'WIND_AVG',\n",
      "       'PRECIP', 'NUMDAY_RAIN', 'NUM_DAYS_SNOW', 'NUM_DAYS_FOG',\n",
      "       'NUM_DAYS_THNDRSTRM', 'PRICE', 'DISCOUNTED_PRICE'],\n",
      "      dtype='object')\n",
      "\n",
      "Shape of variable data: (370, 24)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of column names\n",
    "\n",
    "col_names = list(w_avg_model_scaled)\n",
    "\n",
    "# List all column types\n",
    "#input_data.dtypes\n",
    "print(\"Shape of Original data: {}\".format(w_avg_model_scaled.shape) + \"\\n\")\n",
    "\n",
    "input_variables = w_avg_model_scaled.iloc[:,3:27]\n",
    "print(\"Variables: \" + str(input_variables.columns) + \"\\n\")\n",
    "print(\"Shape of variable data: {}\".format(input_variables.shape) + \"\\n\")\n",
    "\n",
    "#print(input_data.iloc[:,26])\n",
    "price = w_avg_model_scaled.iloc[:,25]\n",
    "discounted_price = w_avg_model_scaled.iloc[:,26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TMP_HIGH</th>\n",
       "      <th>TMP_AVG</th>\n",
       "      <th>TMP_LOW</th>\n",
       "      <th>DP_HIGH</th>\n",
       "      <th>DP_AVG</th>\n",
       "      <th>DP_LOW</th>\n",
       "      <th>HUM_HIGH</th>\n",
       "      <th>HUM_AVG</th>\n",
       "      <th>HUM_LOW</th>\n",
       "      <th>SEALVL_HIGH</th>\n",
       "      <th>...</th>\n",
       "      <th>VIS_LOW</th>\n",
       "      <th>WIND_HIGH</th>\n",
       "      <th>WIND_AVG</th>\n",
       "      <th>PRECIP</th>\n",
       "      <th>NUMDAY_RAIN</th>\n",
       "      <th>NUM_DAYS_SNOW</th>\n",
       "      <th>NUM_DAYS_FOG</th>\n",
       "      <th>NUM_DAYS_THNDRSTRM</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNTED_PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.953658</td>\n",
       "      <td>27.617217</td>\n",
       "      <td>-2.980925</td>\n",
       "      <td>42.880246</td>\n",
       "      <td>19.992262</td>\n",
       "      <td>-8.511984</td>\n",
       "      <td>99.026800</td>\n",
       "      <td>75.092035</td>\n",
       "      <td>32.839506</td>\n",
       "      <td>30.674052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186718</td>\n",
       "      <td>29.600197</td>\n",
       "      <td>11.077783</td>\n",
       "      <td>0.479003</td>\n",
       "      <td>2.544772</td>\n",
       "      <td>7.584738</td>\n",
       "      <td>9.707042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.349417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53.838886</td>\n",
       "      <td>28.731112</td>\n",
       "      <td>0.353638</td>\n",
       "      <td>46.537659</td>\n",
       "      <td>23.958440</td>\n",
       "      <td>-4.555208</td>\n",
       "      <td>99.510367</td>\n",
       "      <td>82.286915</td>\n",
       "      <td>39.353377</td>\n",
       "      <td>30.373695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>24.679482</td>\n",
       "      <td>9.622850</td>\n",
       "      <td>1.722515</td>\n",
       "      <td>5.510271</td>\n",
       "      <td>9.231277</td>\n",
       "      <td>15.203328</td>\n",
       "      <td>0.658457</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.379410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.668458</td>\n",
       "      <td>41.937001</td>\n",
       "      <td>13.926317</td>\n",
       "      <td>51.205991</td>\n",
       "      <td>29.862688</td>\n",
       "      <td>2.748564</td>\n",
       "      <td>99.138166</td>\n",
       "      <td>65.295138</td>\n",
       "      <td>22.250432</td>\n",
       "      <td>30.590270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634101</td>\n",
       "      <td>36.374250</td>\n",
       "      <td>12.349509</td>\n",
       "      <td>15.116988</td>\n",
       "      <td>5.934169</td>\n",
       "      <td>3.868506</td>\n",
       "      <td>9.890731</td>\n",
       "      <td>1.055313</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.349417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85.651529</td>\n",
       "      <td>52.949283</td>\n",
       "      <td>25.351909</td>\n",
       "      <td>60.274139</td>\n",
       "      <td>37.049373</td>\n",
       "      <td>11.479874</td>\n",
       "      <td>99.558871</td>\n",
       "      <td>58.069374</td>\n",
       "      <td>18.204688</td>\n",
       "      <td>30.348293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474198</td>\n",
       "      <td>28.918771</td>\n",
       "      <td>10.770362</td>\n",
       "      <td>17.947352</td>\n",
       "      <td>9.195066</td>\n",
       "      <td>1.960945</td>\n",
       "      <td>7.611098</td>\n",
       "      <td>2.058521</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.339420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85.415668</td>\n",
       "      <td>62.338688</td>\n",
       "      <td>36.602102</td>\n",
       "      <td>61.467698</td>\n",
       "      <td>47.555928</td>\n",
       "      <td>27.795232</td>\n",
       "      <td>95.246992</td>\n",
       "      <td>62.031159</td>\n",
       "      <td>21.005582</td>\n",
       "      <td>30.474858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280723</td>\n",
       "      <td>33.072546</td>\n",
       "      <td>8.983943</td>\n",
       "      <td>15.212715</td>\n",
       "      <td>9.610446</td>\n",
       "      <td>0.327268</td>\n",
       "      <td>14.161837</td>\n",
       "      <td>4.663441</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.449393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>93.180721</td>\n",
       "      <td>71.263159</td>\n",
       "      <td>47.746898</td>\n",
       "      <td>67.021368</td>\n",
       "      <td>55.502022</td>\n",
       "      <td>33.450048</td>\n",
       "      <td>92.041052</td>\n",
       "      <td>62.081519</td>\n",
       "      <td>22.510682</td>\n",
       "      <td>30.306508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950015</td>\n",
       "      <td>25.666101</td>\n",
       "      <td>8.317023</td>\n",
       "      <td>10.598875</td>\n",
       "      <td>8.135348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.649299</td>\n",
       "      <td>4.995009</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.449393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>93.651100</td>\n",
       "      <td>75.325134</td>\n",
       "      <td>57.224505</td>\n",
       "      <td>71.740380</td>\n",
       "      <td>60.440695</td>\n",
       "      <td>41.882198</td>\n",
       "      <td>91.024753</td>\n",
       "      <td>65.247136</td>\n",
       "      <td>24.452377</td>\n",
       "      <td>30.263593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360831</td>\n",
       "      <td>30.924584</td>\n",
       "      <td>7.668664</td>\n",
       "      <td>11.201697</td>\n",
       "      <td>9.899297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.561325</td>\n",
       "      <td>6.286502</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.039494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>88.032940</td>\n",
       "      <td>68.355756</td>\n",
       "      <td>43.240501</td>\n",
       "      <td>70.470204</td>\n",
       "      <td>53.535626</td>\n",
       "      <td>34.387899</td>\n",
       "      <td>97.288101</td>\n",
       "      <td>63.037696</td>\n",
       "      <td>24.573892</td>\n",
       "      <td>30.342929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123844</td>\n",
       "      <td>23.558382</td>\n",
       "      <td>6.839216</td>\n",
       "      <td>1.162475</td>\n",
       "      <td>6.987723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.848591</td>\n",
       "      <td>1.872076</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.769561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>86.350515</td>\n",
       "      <td>66.418060</td>\n",
       "      <td>39.848014</td>\n",
       "      <td>66.092621</td>\n",
       "      <td>53.110278</td>\n",
       "      <td>30.557896</td>\n",
       "      <td>92.975847</td>\n",
       "      <td>67.471453</td>\n",
       "      <td>23.054672</td>\n",
       "      <td>30.374320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>31.473925</td>\n",
       "      <td>7.953490</td>\n",
       "      <td>6.651649</td>\n",
       "      <td>12.019534</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>13.926898</td>\n",
       "      <td>5.663013</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.519623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>74.910045</td>\n",
       "      <td>52.914618</td>\n",
       "      <td>29.721084</td>\n",
       "      <td>61.366736</td>\n",
       "      <td>42.196248</td>\n",
       "      <td>24.559025</td>\n",
       "      <td>98.802348</td>\n",
       "      <td>70.308474</td>\n",
       "      <td>29.572204</td>\n",
       "      <td>30.439866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103207</td>\n",
       "      <td>25.729013</td>\n",
       "      <td>7.975400</td>\n",
       "      <td>10.803140</td>\n",
       "      <td>9.754228</td>\n",
       "      <td>0.327268</td>\n",
       "      <td>16.250392</td>\n",
       "      <td>1.152677</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.389655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>63.782516</td>\n",
       "      <td>37.252792</td>\n",
       "      <td>9.713161</td>\n",
       "      <td>52.539090</td>\n",
       "      <td>29.384204</td>\n",
       "      <td>0.391326</td>\n",
       "      <td>99.354278</td>\n",
       "      <td>75.164598</td>\n",
       "      <td>35.153257</td>\n",
       "      <td>30.751126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.369823</td>\n",
       "      <td>9.360489</td>\n",
       "      <td>0.710853</td>\n",
       "      <td>8.425101</td>\n",
       "      <td>3.263099</td>\n",
       "      <td>17.081087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.469636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>49.151039</td>\n",
       "      <td>32.779945</td>\n",
       "      <td>8.565107</td>\n",
       "      <td>43.501253</td>\n",
       "      <td>26.403070</td>\n",
       "      <td>2.568691</td>\n",
       "      <td>99.867597</td>\n",
       "      <td>79.006841</td>\n",
       "      <td>44.402797</td>\n",
       "      <td>30.575714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171012</td>\n",
       "      <td>30.035203</td>\n",
       "      <td>9.005973</td>\n",
       "      <td>13.063238</td>\n",
       "      <td>4.954492</td>\n",
       "      <td>6.141817</td>\n",
       "      <td>16.710648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.539618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>51.603404</td>\n",
       "      <td>27.391903</td>\n",
       "      <td>-0.148039</td>\n",
       "      <td>38.817646</td>\n",
       "      <td>20.733436</td>\n",
       "      <td>-8.513812</td>\n",
       "      <td>98.946455</td>\n",
       "      <td>76.677755</td>\n",
       "      <td>34.704932</td>\n",
       "      <td>30.415186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353906</td>\n",
       "      <td>27.208144</td>\n",
       "      <td>9.937532</td>\n",
       "      <td>0.697591</td>\n",
       "      <td>2.158710</td>\n",
       "      <td>9.017004</td>\n",
       "      <td>14.353479</td>\n",
       "      <td>0.204561</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.529621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>55.807460</td>\n",
       "      <td>34.986696</td>\n",
       "      <td>13.475683</td>\n",
       "      <td>43.526687</td>\n",
       "      <td>24.955311</td>\n",
       "      <td>5.251212</td>\n",
       "      <td>98.128625</td>\n",
       "      <td>69.389259</td>\n",
       "      <td>30.832461</td>\n",
       "      <td>30.543604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230043</td>\n",
       "      <td>32.970059</td>\n",
       "      <td>9.232392</td>\n",
       "      <td>25.414489</td>\n",
       "      <td>3.158557</td>\n",
       "      <td>2.563075</td>\n",
       "      <td>11.369789</td>\n",
       "      <td>0.077182</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.449640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>72.278760</td>\n",
       "      <td>42.527518</td>\n",
       "      <td>16.163708</td>\n",
       "      <td>47.173214</td>\n",
       "      <td>29.577827</td>\n",
       "      <td>6.417299</td>\n",
       "      <td>99.149593</td>\n",
       "      <td>63.518915</td>\n",
       "      <td>22.710445</td>\n",
       "      <td>30.537300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171012</td>\n",
       "      <td>29.907703</td>\n",
       "      <td>11.045504</td>\n",
       "      <td>6.407882</td>\n",
       "      <td>6.858583</td>\n",
       "      <td>2.852181</td>\n",
       "      <td>10.275397</td>\n",
       "      <td>0.716611</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.509626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>85.305443</td>\n",
       "      <td>51.656418</td>\n",
       "      <td>21.970248</td>\n",
       "      <td>53.181639</td>\n",
       "      <td>35.382062</td>\n",
       "      <td>12.721515</td>\n",
       "      <td>98.923711</td>\n",
       "      <td>57.038358</td>\n",
       "      <td>16.507981</td>\n",
       "      <td>30.336701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185989</td>\n",
       "      <td>27.064239</td>\n",
       "      <td>9.509505</td>\n",
       "      <td>12.758238</td>\n",
       "      <td>8.253307</td>\n",
       "      <td>0.808174</td>\n",
       "      <td>9.891667</td>\n",
       "      <td>1.736127</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.559613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>88.717714</td>\n",
       "      <td>65.765110</td>\n",
       "      <td>38.133418</td>\n",
       "      <td>64.652678</td>\n",
       "      <td>48.141753</td>\n",
       "      <td>26.138192</td>\n",
       "      <td>95.677253</td>\n",
       "      <td>56.516483</td>\n",
       "      <td>19.106516</td>\n",
       "      <td>30.317936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593883</td>\n",
       "      <td>31.632215</td>\n",
       "      <td>8.975590</td>\n",
       "      <td>24.486700</td>\n",
       "      <td>9.510037</td>\n",
       "      <td>0.218179</td>\n",
       "      <td>9.562606</td>\n",
       "      <td>6.851201</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.719574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>94.935745</td>\n",
       "      <td>72.250194</td>\n",
       "      <td>49.378090</td>\n",
       "      <td>66.975597</td>\n",
       "      <td>55.163926</td>\n",
       "      <td>34.929410</td>\n",
       "      <td>92.224308</td>\n",
       "      <td>59.522505</td>\n",
       "      <td>22.550540</td>\n",
       "      <td>30.285208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884931</td>\n",
       "      <td>26.598362</td>\n",
       "      <td>7.892497</td>\n",
       "      <td>9.973803</td>\n",
       "      <td>7.983505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.068762</td>\n",
       "      <td>6.553994</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.749566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>94.486551</td>\n",
       "      <td>74.924089</td>\n",
       "      <td>52.515406</td>\n",
       "      <td>69.564460</td>\n",
       "      <td>60.019149</td>\n",
       "      <td>44.308975</td>\n",
       "      <td>91.503566</td>\n",
       "      <td>65.152632</td>\n",
       "      <td>31.710453</td>\n",
       "      <td>30.244792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084196</td>\n",
       "      <td>29.864555</td>\n",
       "      <td>7.596663</td>\n",
       "      <td>39.075101</td>\n",
       "      <td>11.250805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.522498</td>\n",
       "      <td>7.200001</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.659588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>95.769169</td>\n",
       "      <td>72.236452</td>\n",
       "      <td>49.913575</td>\n",
       "      <td>70.645707</td>\n",
       "      <td>57.160585</td>\n",
       "      <td>37.261179</td>\n",
       "      <td>92.336720</td>\n",
       "      <td>64.327544</td>\n",
       "      <td>26.854535</td>\n",
       "      <td>30.357503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204506</td>\n",
       "      <td>33.754199</td>\n",
       "      <td>7.664999</td>\n",
       "      <td>3.963129</td>\n",
       "      <td>10.481103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.100641</td>\n",
       "      <td>6.232162</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.529621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>86.330704</td>\n",
       "      <td>64.757891</td>\n",
       "      <td>42.685359</td>\n",
       "      <td>61.761861</td>\n",
       "      <td>49.221634</td>\n",
       "      <td>31.004195</td>\n",
       "      <td>93.210550</td>\n",
       "      <td>62.060546</td>\n",
       "      <td>26.042270</td>\n",
       "      <td>30.244478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204506</td>\n",
       "      <td>25.124887</td>\n",
       "      <td>7.115135</td>\n",
       "      <td>5.524179</td>\n",
       "      <td>7.851274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.212706</td>\n",
       "      <td>2.418011</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.529621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>75.338957</td>\n",
       "      <td>48.717642</td>\n",
       "      <td>26.178812</td>\n",
       "      <td>53.494604</td>\n",
       "      <td>35.590995</td>\n",
       "      <td>20.118526</td>\n",
       "      <td>98.458155</td>\n",
       "      <td>63.592018</td>\n",
       "      <td>24.996833</td>\n",
       "      <td>30.437811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343522</td>\n",
       "      <td>27.792394</td>\n",
       "      <td>9.120230</td>\n",
       "      <td>8.947375</td>\n",
       "      <td>8.870945</td>\n",
       "      <td>0.458632</td>\n",
       "      <td>10.156463</td>\n",
       "      <td>0.873945</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.599603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>73.387568</td>\n",
       "      <td>42.445148</td>\n",
       "      <td>11.759290</td>\n",
       "      <td>53.810402</td>\n",
       "      <td>31.334574</td>\n",
       "      <td>6.142447</td>\n",
       "      <td>99.130187</td>\n",
       "      <td>67.794578</td>\n",
       "      <td>27.402199</td>\n",
       "      <td>30.442214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087948</td>\n",
       "      <td>27.737506</td>\n",
       "      <td>10.504951</td>\n",
       "      <td>6.922821</td>\n",
       "      <td>11.552323</td>\n",
       "      <td>3.370795</td>\n",
       "      <td>11.110684</td>\n",
       "      <td>0.573445</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.679583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>54.851605</td>\n",
       "      <td>31.435940</td>\n",
       "      <td>7.705909</td>\n",
       "      <td>48.230410</td>\n",
       "      <td>24.561279</td>\n",
       "      <td>2.072395</td>\n",
       "      <td>99.310970</td>\n",
       "      <td>76.862397</td>\n",
       "      <td>39.885647</td>\n",
       "      <td>30.584784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.844028</td>\n",
       "      <td>11.646561</td>\n",
       "      <td>6.928783</td>\n",
       "      <td>9.977321</td>\n",
       "      <td>8.323399</td>\n",
       "      <td>14.877116</td>\n",
       "      <td>1.181361</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.799554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>56.407002</td>\n",
       "      <td>25.686117</td>\n",
       "      <td>-0.995032</td>\n",
       "      <td>46.273711</td>\n",
       "      <td>17.866325</td>\n",
       "      <td>-7.806793</td>\n",
       "      <td>99.867597</td>\n",
       "      <td>73.854349</td>\n",
       "      <td>34.931524</td>\n",
       "      <td>30.699720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>31.689132</td>\n",
       "      <td>11.978539</td>\n",
       "      <td>1.444931</td>\n",
       "      <td>3.289410</td>\n",
       "      <td>8.299551</td>\n",
       "      <td>7.312518</td>\n",
       "      <td>0.576510</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.809551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>59.645858</td>\n",
       "      <td>26.753927</td>\n",
       "      <td>-0.651426</td>\n",
       "      <td>40.515945</td>\n",
       "      <td>18.611068</td>\n",
       "      <td>-7.612461</td>\n",
       "      <td>98.683446</td>\n",
       "      <td>73.326261</td>\n",
       "      <td>34.011035</td>\n",
       "      <td>30.582852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171012</td>\n",
       "      <td>30.835942</td>\n",
       "      <td>11.366350</td>\n",
       "      <td>6.048729</td>\n",
       "      <td>2.780704</td>\n",
       "      <td>9.300564</td>\n",
       "      <td>9.198702</td>\n",
       "      <td>0.044134</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.899529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>69.757621</td>\n",
       "      <td>38.434919</td>\n",
       "      <td>15.024229</td>\n",
       "      <td>51.018937</td>\n",
       "      <td>27.693562</td>\n",
       "      <td>8.844748</td>\n",
       "      <td>99.867597</td>\n",
       "      <td>67.623090</td>\n",
       "      <td>25.905885</td>\n",
       "      <td>30.490657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>34.058311</td>\n",
       "      <td>11.332478</td>\n",
       "      <td>372.538106</td>\n",
       "      <td>6.725774</td>\n",
       "      <td>6.585668</td>\n",
       "      <td>10.346304</td>\n",
       "      <td>2.881596</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.909526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>80.916532</td>\n",
       "      <td>49.536814</td>\n",
       "      <td>24.445530</td>\n",
       "      <td>55.833496</td>\n",
       "      <td>33.898013</td>\n",
       "      <td>11.252577</td>\n",
       "      <td>98.650757</td>\n",
       "      <td>57.738017</td>\n",
       "      <td>20.443700</td>\n",
       "      <td>30.251877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275534</td>\n",
       "      <td>41.454133</td>\n",
       "      <td>10.788335</td>\n",
       "      <td>3.927050</td>\n",
       "      <td>8.654531</td>\n",
       "      <td>0.460086</td>\n",
       "      <td>6.650997</td>\n",
       "      <td>3.396558</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.939519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>88.648736</td>\n",
       "      <td>63.285999</td>\n",
       "      <td>38.007417</td>\n",
       "      <td>58.065659</td>\n",
       "      <td>44.089405</td>\n",
       "      <td>26.759350</td>\n",
       "      <td>95.544515</td>\n",
       "      <td>52.174506</td>\n",
       "      <td>20.090873</td>\n",
       "      <td>30.273455</td>\n",
       "      <td>...</td>\n",
       "      <td>1.358716</td>\n",
       "      <td>37.070596</td>\n",
       "      <td>8.760104</td>\n",
       "      <td>16.876689</td>\n",
       "      <td>6.618215</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>3.626469</td>\n",
       "      <td>3.238601</td>\n",
       "      <td>2.03</td>\n",
       "      <td>2.029497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>97.441783</td>\n",
       "      <td>71.362958</td>\n",
       "      <td>44.108245</td>\n",
       "      <td>65.334190</td>\n",
       "      <td>49.943229</td>\n",
       "      <td>30.379235</td>\n",
       "      <td>92.462399</td>\n",
       "      <td>49.822999</td>\n",
       "      <td>21.137929</td>\n",
       "      <td>30.305041</td>\n",
       "      <td>...</td>\n",
       "      <td>2.432953</td>\n",
       "      <td>22.162214</td>\n",
       "      <td>7.857640</td>\n",
       "      <td>4.046168</td>\n",
       "      <td>2.947843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.199814</td>\n",
       "      <td>1.358670</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.509378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>93.846979</td>\n",
       "      <td>69.533818</td>\n",
       "      <td>50.719371</td>\n",
       "      <td>74.213351</td>\n",
       "      <td>58.528124</td>\n",
       "      <td>44.680399</td>\n",
       "      <td>98.567577</td>\n",
       "      <td>70.492357</td>\n",
       "      <td>33.789182</td>\n",
       "      <td>30.300776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280616</td>\n",
       "      <td>31.631130</td>\n",
       "      <td>6.640193</td>\n",
       "      <td>2.428684</td>\n",
       "      <td>9.495698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.217399</td>\n",
       "      <td>5.176212</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.118978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>90.890957</td>\n",
       "      <td>72.263597</td>\n",
       "      <td>50.677898</td>\n",
       "      <td>76.059315</td>\n",
       "      <td>63.058648</td>\n",
       "      <td>43.453073</td>\n",
       "      <td>98.017750</td>\n",
       "      <td>75.520995</td>\n",
       "      <td>29.919041</td>\n",
       "      <td>30.187260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247688</td>\n",
       "      <td>29.820666</td>\n",
       "      <td>5.866341</td>\n",
       "      <td>4.025913</td>\n",
       "      <td>11.777075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.004267</td>\n",
       "      <td>7.259140</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.739073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>91.921477</td>\n",
       "      <td>63.870515</td>\n",
       "      <td>39.322797</td>\n",
       "      <td>71.925580</td>\n",
       "      <td>53.068572</td>\n",
       "      <td>34.902195</td>\n",
       "      <td>98.619932</td>\n",
       "      <td>70.989761</td>\n",
       "      <td>28.101687</td>\n",
       "      <td>30.382688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280616</td>\n",
       "      <td>29.115301</td>\n",
       "      <td>6.217876</td>\n",
       "      <td>3.105514</td>\n",
       "      <td>7.529903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.055058</td>\n",
       "      <td>4.412042</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.509130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>79.939694</td>\n",
       "      <td>52.377502</td>\n",
       "      <td>30.009565</td>\n",
       "      <td>63.019905</td>\n",
       "      <td>42.385667</td>\n",
       "      <td>16.371301</td>\n",
       "      <td>99.549425</td>\n",
       "      <td>71.201853</td>\n",
       "      <td>22.058912</td>\n",
       "      <td>30.382767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142584</td>\n",
       "      <td>30.957589</td>\n",
       "      <td>7.942366</td>\n",
       "      <td>2.901838</td>\n",
       "      <td>12.432674</td>\n",
       "      <td>0.437813</td>\n",
       "      <td>4.171134</td>\n",
       "      <td>2.420886</td>\n",
       "      <td>3.58</td>\n",
       "      <td>3.579112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>67.480255</td>\n",
       "      <td>35.825140</td>\n",
       "      <td>9.772755</td>\n",
       "      <td>54.625033</td>\n",
       "      <td>26.391946</td>\n",
       "      <td>0.093362</td>\n",
       "      <td>98.642157</td>\n",
       "      <td>69.935334</td>\n",
       "      <td>27.929304</td>\n",
       "      <td>30.489855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179613</td>\n",
       "      <td>31.722576</td>\n",
       "      <td>10.206118</td>\n",
       "      <td>1.708385</td>\n",
       "      <td>7.384086</td>\n",
       "      <td>7.818873</td>\n",
       "      <td>2.738053</td>\n",
       "      <td>0.033495</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>54.855890</td>\n",
       "      <td>33.104515</td>\n",
       "      <td>6.871596</td>\n",
       "      <td>47.711540</td>\n",
       "      <td>27.232756</td>\n",
       "      <td>-3.721176</td>\n",
       "      <td>99.246957</td>\n",
       "      <td>78.720917</td>\n",
       "      <td>38.324205</td>\n",
       "      <td>30.668162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>27.371385</td>\n",
       "      <td>8.089446</td>\n",
       "      <td>1.598096</td>\n",
       "      <td>7.857272</td>\n",
       "      <td>3.744419</td>\n",
       "      <td>4.276190</td>\n",
       "      <td>0.128912</td>\n",
       "      <td>3.81</td>\n",
       "      <td>3.809055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>51.130063</td>\n",
       "      <td>25.235828</td>\n",
       "      <td>-7.132326</td>\n",
       "      <td>38.658598</td>\n",
       "      <td>18.235895</td>\n",
       "      <td>-13.382659</td>\n",
       "      <td>98.593333</td>\n",
       "      <td>74.443192</td>\n",
       "      <td>37.529179</td>\n",
       "      <td>30.860636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169950</td>\n",
       "      <td>31.706604</td>\n",
       "      <td>8.372164</td>\n",
       "      <td>1.221606</td>\n",
       "      <td>6.276689</td>\n",
       "      <td>9.317115</td>\n",
       "      <td>3.029987</td>\n",
       "      <td>0.048754</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.819053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>51.533679</td>\n",
       "      <td>20.571537</td>\n",
       "      <td>-7.815383</td>\n",
       "      <td>41.202170</td>\n",
       "      <td>11.970765</td>\n",
       "      <td>-19.757363</td>\n",
       "      <td>97.397698</td>\n",
       "      <td>68.194308</td>\n",
       "      <td>28.997365</td>\n",
       "      <td>30.744742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.970867</td>\n",
       "      <td>9.334185</td>\n",
       "      <td>1.278564</td>\n",
       "      <td>1.240099</td>\n",
       "      <td>11.237240</td>\n",
       "      <td>3.037794</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.779063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>73.101701</td>\n",
       "      <td>38.194787</td>\n",
       "      <td>-1.751123</td>\n",
       "      <td>49.947801</td>\n",
       "      <td>26.901783</td>\n",
       "      <td>-8.428940</td>\n",
       "      <td>99.345740</td>\n",
       "      <td>67.129144</td>\n",
       "      <td>21.228026</td>\n",
       "      <td>30.606852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123844</td>\n",
       "      <td>28.651405</td>\n",
       "      <td>8.266480</td>\n",
       "      <td>1.465324</td>\n",
       "      <td>7.336679</td>\n",
       "      <td>2.717470</td>\n",
       "      <td>3.919565</td>\n",
       "      <td>0.523391</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.789060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>78.882895</td>\n",
       "      <td>52.573288</td>\n",
       "      <td>25.908735</td>\n",
       "      <td>61.805920</td>\n",
       "      <td>37.288333</td>\n",
       "      <td>11.739397</td>\n",
       "      <td>98.647477</td>\n",
       "      <td>60.300742</td>\n",
       "      <td>16.797870</td>\n",
       "      <td>30.364271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364541</td>\n",
       "      <td>34.562780</td>\n",
       "      <td>9.528799</td>\n",
       "      <td>2.739780</td>\n",
       "      <td>11.121298</td>\n",
       "      <td>0.274590</td>\n",
       "      <td>1.776208</td>\n",
       "      <td>4.189540</td>\n",
       "      <td>3.71</td>\n",
       "      <td>3.709080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>84.884745</td>\n",
       "      <td>63.428020</td>\n",
       "      <td>35.767946</td>\n",
       "      <td>67.599624</td>\n",
       "      <td>51.224953</td>\n",
       "      <td>29.976145</td>\n",
       "      <td>98.472728</td>\n",
       "      <td>67.507878</td>\n",
       "      <td>21.849660</td>\n",
       "      <td>30.432112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203939</td>\n",
       "      <td>31.978787</td>\n",
       "      <td>9.271944</td>\n",
       "      <td>4.577083</td>\n",
       "      <td>13.856634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.490798</td>\n",
       "      <td>5.521460</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.619102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>91.473565</td>\n",
       "      <td>70.896868</td>\n",
       "      <td>47.813800</td>\n",
       "      <td>73.311502</td>\n",
       "      <td>60.763065</td>\n",
       "      <td>41.743208</td>\n",
       "      <td>98.374647</td>\n",
       "      <td>72.855429</td>\n",
       "      <td>32.695204</td>\n",
       "      <td>30.267854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123844</td>\n",
       "      <td>40.923783</td>\n",
       "      <td>7.503916</td>\n",
       "      <td>6.465531</td>\n",
       "      <td>13.572339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.206530</td>\n",
       "      <td>10.167240</td>\n",
       "      <td>3.65</td>\n",
       "      <td>3.649095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>90.701935</td>\n",
       "      <td>72.878642</td>\n",
       "      <td>54.529864</td>\n",
       "      <td>76.958795</td>\n",
       "      <td>63.472975</td>\n",
       "      <td>49.014850</td>\n",
       "      <td>97.220065</td>\n",
       "      <td>74.227458</td>\n",
       "      <td>32.245451</td>\n",
       "      <td>30.185179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269692</td>\n",
       "      <td>31.476291</td>\n",
       "      <td>6.044702</td>\n",
       "      <td>3.820778</td>\n",
       "      <td>9.391236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.508345</td>\n",
       "      <td>4.724758</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.849045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>90.465476</td>\n",
       "      <td>70.885366</td>\n",
       "      <td>50.222892</td>\n",
       "      <td>72.164270</td>\n",
       "      <td>60.343242</td>\n",
       "      <td>41.443154</td>\n",
       "      <td>98.073867</td>\n",
       "      <td>71.737793</td>\n",
       "      <td>31.222072</td>\n",
       "      <td>30.211701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280616</td>\n",
       "      <td>25.342742</td>\n",
       "      <td>5.933962</td>\n",
       "      <td>2.931901</td>\n",
       "      <td>7.129545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.308489</td>\n",
       "      <td>4.609560</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>91.118224</td>\n",
       "      <td>68.628671</td>\n",
       "      <td>43.926815</td>\n",
       "      <td>71.261417</td>\n",
       "      <td>56.466760</td>\n",
       "      <td>36.758903</td>\n",
       "      <td>98.097529</td>\n",
       "      <td>68.738023</td>\n",
       "      <td>27.391824</td>\n",
       "      <td>30.262646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455681</td>\n",
       "      <td>28.924293</td>\n",
       "      <td>6.483753</td>\n",
       "      <td>3.815673</td>\n",
       "      <td>7.487878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.742017</td>\n",
       "      <td>4.006889</td>\n",
       "      <td>3.69</td>\n",
       "      <td>3.689085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>80.709535</td>\n",
       "      <td>55.705794</td>\n",
       "      <td>28.846762</td>\n",
       "      <td>61.902473</td>\n",
       "      <td>42.871323</td>\n",
       "      <td>17.564898</td>\n",
       "      <td>98.096296</td>\n",
       "      <td>66.261166</td>\n",
       "      <td>20.668119</td>\n",
       "      <td>30.504368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211792</td>\n",
       "      <td>29.154354</td>\n",
       "      <td>8.834588</td>\n",
       "      <td>1.599482</td>\n",
       "      <td>7.466220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.478168</td>\n",
       "      <td>0.909474</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.699083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>70.297323</td>\n",
       "      <td>44.810163</td>\n",
       "      <td>11.241853</td>\n",
       "      <td>59.617839</td>\n",
       "      <td>35.202024</td>\n",
       "      <td>4.497239</td>\n",
       "      <td>98.394890</td>\n",
       "      <td>71.360548</td>\n",
       "      <td>28.181492</td>\n",
       "      <td>30.455684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309611</td>\n",
       "      <td>36.901813</td>\n",
       "      <td>10.017259</td>\n",
       "      <td>4.086457</td>\n",
       "      <td>9.970229</td>\n",
       "      <td>1.246099</td>\n",
       "      <td>5.044528</td>\n",
       "      <td>1.015249</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.699083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>65.188431</td>\n",
       "      <td>38.381905</td>\n",
       "      <td>14.931579</td>\n",
       "      <td>58.616438</td>\n",
       "      <td>31.494061</td>\n",
       "      <td>-1.608238</td>\n",
       "      <td>99.769506</td>\n",
       "      <td>77.939941</td>\n",
       "      <td>35.794012</td>\n",
       "      <td>30.572263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.124233</td>\n",
       "      <td>10.323283</td>\n",
       "      <td>5.184984</td>\n",
       "      <td>10.398913</td>\n",
       "      <td>2.988891</td>\n",
       "      <td>6.702315</td>\n",
       "      <td>2.058452</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.789060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>56.208957</td>\n",
       "      <td>28.210967</td>\n",
       "      <td>1.826907</td>\n",
       "      <td>47.001875</td>\n",
       "      <td>20.913061</td>\n",
       "      <td>-8.831172</td>\n",
       "      <td>99.346200</td>\n",
       "      <td>74.066579</td>\n",
       "      <td>38.988302</td>\n",
       "      <td>30.494712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059394</td>\n",
       "      <td>32.207612</td>\n",
       "      <td>9.778199</td>\n",
       "      <td>1.232944</td>\n",
       "      <td>4.799819</td>\n",
       "      <td>8.511348</td>\n",
       "      <td>2.459232</td>\n",
       "      <td>0.015259</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.729075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>66.900172</td>\n",
       "      <td>33.807153</td>\n",
       "      <td>6.331705</td>\n",
       "      <td>50.693125</td>\n",
       "      <td>23.887685</td>\n",
       "      <td>-6.109805</td>\n",
       "      <td>99.734310</td>\n",
       "      <td>69.674338</td>\n",
       "      <td>26.245279</td>\n",
       "      <td>30.588809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>36.206540</td>\n",
       "      <td>9.824466</td>\n",
       "      <td>0.836330</td>\n",
       "      <td>4.224789</td>\n",
       "      <td>6.814530</td>\n",
       "      <td>3.299300</td>\n",
       "      <td>0.565800</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>71.538246</td>\n",
       "      <td>44.964954</td>\n",
       "      <td>14.037543</td>\n",
       "      <td>57.680772</td>\n",
       "      <td>35.297947</td>\n",
       "      <td>5.115327</td>\n",
       "      <td>99.868289</td>\n",
       "      <td>70.744770</td>\n",
       "      <td>22.796445</td>\n",
       "      <td>30.383500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.789749</td>\n",
       "      <td>9.629134</td>\n",
       "      <td>3.098442</td>\n",
       "      <td>11.949852</td>\n",
       "      <td>2.288162</td>\n",
       "      <td>3.608914</td>\n",
       "      <td>2.883812</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.669090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>82.089725</td>\n",
       "      <td>52.325395</td>\n",
       "      <td>25.594821</td>\n",
       "      <td>59.313656</td>\n",
       "      <td>38.906985</td>\n",
       "      <td>12.135843</td>\n",
       "      <td>99.246957</td>\n",
       "      <td>63.495779</td>\n",
       "      <td>18.660957</td>\n",
       "      <td>30.402745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.691810</td>\n",
       "      <td>39.165480</td>\n",
       "      <td>9.512475</td>\n",
       "      <td>2.662589</td>\n",
       "      <td>10.297377</td>\n",
       "      <td>1.718848</td>\n",
       "      <td>2.704257</td>\n",
       "      <td>3.283336</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.669090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>86.822760</td>\n",
       "      <td>61.317018</td>\n",
       "      <td>37.123673</td>\n",
       "      <td>67.550974</td>\n",
       "      <td>48.989342</td>\n",
       "      <td>24.687457</td>\n",
       "      <td>98.782513</td>\n",
       "      <td>67.646893</td>\n",
       "      <td>21.923764</td>\n",
       "      <td>30.271661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572066</td>\n",
       "      <td>31.258585</td>\n",
       "      <td>8.051852</td>\n",
       "      <td>2.985320</td>\n",
       "      <td>13.488475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.162346</td>\n",
       "      <td>6.827281</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.819053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>93.436156</td>\n",
       "      <td>72.669555</td>\n",
       "      <td>49.718113</td>\n",
       "      <td>73.627985</td>\n",
       "      <td>58.861639</td>\n",
       "      <td>37.094091</td>\n",
       "      <td>98.280248</td>\n",
       "      <td>65.584746</td>\n",
       "      <td>22.049334</td>\n",
       "      <td>30.241324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423862</td>\n",
       "      <td>26.449861</td>\n",
       "      <td>7.022661</td>\n",
       "      <td>2.645324</td>\n",
       "      <td>6.599050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.866101</td>\n",
       "      <td>4.338305</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.919028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>92.433461</td>\n",
       "      <td>74.195493</td>\n",
       "      <td>53.968246</td>\n",
       "      <td>77.715821</td>\n",
       "      <td>64.993758</td>\n",
       "      <td>46.259779</td>\n",
       "      <td>99.336052</td>\n",
       "      <td>76.116392</td>\n",
       "      <td>35.745000</td>\n",
       "      <td>30.261990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247688</td>\n",
       "      <td>33.613471</td>\n",
       "      <td>6.212372</td>\n",
       "      <td>5.552655</td>\n",
       "      <td>11.304979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.765973</td>\n",
       "      <td>9.193705</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>89.771611</td>\n",
       "      <td>73.976134</td>\n",
       "      <td>53.152785</td>\n",
       "      <td>75.197298</td>\n",
       "      <td>65.722541</td>\n",
       "      <td>46.370412</td>\n",
       "      <td>98.526977</td>\n",
       "      <td>77.621620</td>\n",
       "      <td>40.558208</td>\n",
       "      <td>30.249344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342538</td>\n",
       "      <td>29.407200</td>\n",
       "      <td>5.602346</td>\n",
       "      <td>4.854680</td>\n",
       "      <td>10.822233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.440266</td>\n",
       "      <td>7.746643</td>\n",
       "      <td>3.17</td>\n",
       "      <td>3.169214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>90.539780</td>\n",
       "      <td>68.526421</td>\n",
       "      <td>47.780800</td>\n",
       "      <td>72.822394</td>\n",
       "      <td>58.098483</td>\n",
       "      <td>31.084937</td>\n",
       "      <td>99.467762</td>\n",
       "      <td>72.952297</td>\n",
       "      <td>22.789047</td>\n",
       "      <td>30.309965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185766</td>\n",
       "      <td>26.270849</td>\n",
       "      <td>6.906921</td>\n",
       "      <td>3.616776</td>\n",
       "      <td>9.658813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.307687</td>\n",
       "      <td>4.115569</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.269189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>83.401525</td>\n",
       "      <td>58.012848</td>\n",
       "      <td>33.431914</td>\n",
       "      <td>65.081202</td>\n",
       "      <td>47.633134</td>\n",
       "      <td>27.152489</td>\n",
       "      <td>99.670723</td>\n",
       "      <td>70.856970</td>\n",
       "      <td>26.610079</td>\n",
       "      <td>30.418100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302619</td>\n",
       "      <td>31.092277</td>\n",
       "      <td>8.219588</td>\n",
       "      <td>1.907746</td>\n",
       "      <td>8.156643</td>\n",
       "      <td>0.022274</td>\n",
       "      <td>0.872271</td>\n",
       "      <td>3.452099</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.299182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>76.323585</td>\n",
       "      <td>44.534557</td>\n",
       "      <td>16.508950</td>\n",
       "      <td>60.101894</td>\n",
       "      <td>35.188458</td>\n",
       "      <td>10.089226</td>\n",
       "      <td>99.769506</td>\n",
       "      <td>72.396604</td>\n",
       "      <td>30.732871</td>\n",
       "      <td>30.497395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189700</td>\n",
       "      <td>35.356631</td>\n",
       "      <td>7.994466</td>\n",
       "      <td>2.237944</td>\n",
       "      <td>6.781341</td>\n",
       "      <td>1.083395</td>\n",
       "      <td>3.198530</td>\n",
       "      <td>1.929992</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.319177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>56.499747</td>\n",
       "      <td>28.025835</td>\n",
       "      <td>-5.724015</td>\n",
       "      <td>52.141124</td>\n",
       "      <td>20.537073</td>\n",
       "      <td>-14.910411</td>\n",
       "      <td>99.769506</td>\n",
       "      <td>73.808132</td>\n",
       "      <td>35.018071</td>\n",
       "      <td>30.771110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.388733</td>\n",
       "      <td>9.509581</td>\n",
       "      <td>1.391766</td>\n",
       "      <td>6.026444</td>\n",
       "      <td>7.998503</td>\n",
       "      <td>4.082039</td>\n",
       "      <td>0.110676</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.469140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TMP_HIGH    TMP_AVG    TMP_LOW    DP_HIGH     DP_AVG     DP_LOW  \\\n",
       "0    55.953658  27.617217  -2.980925  42.880246  19.992262  -8.511984   \n",
       "1    53.838886  28.731112   0.353638  46.537659  23.958440  -4.555208   \n",
       "2    77.668458  41.937001  13.926317  51.205991  29.862688   2.748564   \n",
       "3    85.651529  52.949283  25.351909  60.274139  37.049373  11.479874   \n",
       "4    85.415668  62.338688  36.602102  61.467698  47.555928  27.795232   \n",
       "5    93.180721  71.263159  47.746898  67.021368  55.502022  33.450048   \n",
       "6    93.651100  75.325134  57.224505  71.740380  60.440695  41.882198   \n",
       "7    88.032940  68.355756  43.240501  70.470204  53.535626  34.387899   \n",
       "8    86.350515  66.418060  39.848014  66.092621  53.110278  30.557896   \n",
       "9    74.910045  52.914618  29.721084  61.366736  42.196248  24.559025   \n",
       "10   63.782516  37.252792   9.713161  52.539090  29.384204   0.391326   \n",
       "11   49.151039  32.779945   8.565107  43.501253  26.403070   2.568691   \n",
       "12   51.603404  27.391903  -0.148039  38.817646  20.733436  -8.513812   \n",
       "13   55.807460  34.986696  13.475683  43.526687  24.955311   5.251212   \n",
       "14   72.278760  42.527518  16.163708  47.173214  29.577827   6.417299   \n",
       "15   85.305443  51.656418  21.970248  53.181639  35.382062  12.721515   \n",
       "16   88.717714  65.765110  38.133418  64.652678  48.141753  26.138192   \n",
       "17   94.935745  72.250194  49.378090  66.975597  55.163926  34.929410   \n",
       "18   94.486551  74.924089  52.515406  69.564460  60.019149  44.308975   \n",
       "19   95.769169  72.236452  49.913575  70.645707  57.160585  37.261179   \n",
       "20   86.330704  64.757891  42.685359  61.761861  49.221634  31.004195   \n",
       "21   75.338957  48.717642  26.178812  53.494604  35.590995  20.118526   \n",
       "22   73.387568  42.445148  11.759290  53.810402  31.334574   6.142447   \n",
       "23   54.851605  31.435940   7.705909  48.230410  24.561279   2.072395   \n",
       "24   56.407002  25.686117  -0.995032  46.273711  17.866325  -7.806793   \n",
       "25   59.645858  26.753927  -0.651426  40.515945  18.611068  -7.612461   \n",
       "26   69.757621  38.434919  15.024229  51.018937  27.693562   8.844748   \n",
       "27   80.916532  49.536814  24.445530  55.833496  33.898013  11.252577   \n",
       "28   88.648736  63.285999  38.007417  58.065659  44.089405  26.759350   \n",
       "29   97.441783  71.362958  44.108245  65.334190  49.943229  30.379235   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "340  93.846979  69.533818  50.719371  74.213351  58.528124  44.680399   \n",
       "341  90.890957  72.263597  50.677898  76.059315  63.058648  43.453073   \n",
       "342  91.921477  63.870515  39.322797  71.925580  53.068572  34.902195   \n",
       "343  79.939694  52.377502  30.009565  63.019905  42.385667  16.371301   \n",
       "344  67.480255  35.825140   9.772755  54.625033  26.391946   0.093362   \n",
       "345  54.855890  33.104515   6.871596  47.711540  27.232756  -3.721176   \n",
       "346  51.130063  25.235828  -7.132326  38.658598  18.235895 -13.382659   \n",
       "347  51.533679  20.571537  -7.815383  41.202170  11.970765 -19.757363   \n",
       "348  73.101701  38.194787  -1.751123  49.947801  26.901783  -8.428940   \n",
       "349  78.882895  52.573288  25.908735  61.805920  37.288333  11.739397   \n",
       "350  84.884745  63.428020  35.767946  67.599624  51.224953  29.976145   \n",
       "351  91.473565  70.896868  47.813800  73.311502  60.763065  41.743208   \n",
       "352  90.701935  72.878642  54.529864  76.958795  63.472975  49.014850   \n",
       "353  90.465476  70.885366  50.222892  72.164270  60.343242  41.443154   \n",
       "354  91.118224  68.628671  43.926815  71.261417  56.466760  36.758903   \n",
       "355  80.709535  55.705794  28.846762  61.902473  42.871323  17.564898   \n",
       "356  70.297323  44.810163  11.241853  59.617839  35.202024   4.497239   \n",
       "357  65.188431  38.381905  14.931579  58.616438  31.494061  -1.608238   \n",
       "358  56.208957  28.210967   1.826907  47.001875  20.913061  -8.831172   \n",
       "359  66.900172  33.807153   6.331705  50.693125  23.887685  -6.109805   \n",
       "360  71.538246  44.964954  14.037543  57.680772  35.297947   5.115327   \n",
       "361  82.089725  52.325395  25.594821  59.313656  38.906985  12.135843   \n",
       "362  86.822760  61.317018  37.123673  67.550974  48.989342  24.687457   \n",
       "363  93.436156  72.669555  49.718113  73.627985  58.861639  37.094091   \n",
       "364  92.433461  74.195493  53.968246  77.715821  64.993758  46.259779   \n",
       "365  89.771611  73.976134  53.152785  75.197298  65.722541  46.370412   \n",
       "366  90.539780  68.526421  47.780800  72.822394  58.098483  31.084937   \n",
       "367  83.401525  58.012848  33.431914  65.081202  47.633134  27.152489   \n",
       "368  76.323585  44.534557  16.508950  60.101894  35.188458  10.089226   \n",
       "369  56.499747  28.025835  -5.724015  52.141124  20.537073 -14.910411   \n",
       "\n",
       "      HUM_HIGH    HUM_AVG    HUM_LOW  SEALVL_HIGH        ...          VIS_LOW  \\\n",
       "0    99.026800  75.092035  32.839506    30.674052        ...         0.186718   \n",
       "1    99.510367  82.286915  39.353377    30.373695        ...         0.109089   \n",
       "2    99.138166  65.295138  22.250432    30.590270        ...         0.634101   \n",
       "3    99.558871  58.069374  18.204688    30.348293        ...         0.474198   \n",
       "4    95.246992  62.031159  21.005582    30.474858        ...         0.280723   \n",
       "5    92.041052  62.081519  22.510682    30.306508        ...         0.950015   \n",
       "6    91.024753  65.247136  24.452377    30.263593        ...         0.360831   \n",
       "7    97.288101  63.037696  24.573892    30.342929        ...         0.123844   \n",
       "8    92.975847  67.471453  23.054672    30.374320        ...         0.061922   \n",
       "9    98.802348  70.308474  29.572204    30.439866        ...         0.103207   \n",
       "10   99.354278  75.164598  35.153257    30.751126        ...         0.000000   \n",
       "11   99.867597  79.006841  44.402797    30.575714        ...         0.171012   \n",
       "12   98.946455  76.677755  34.704932    30.415186        ...         0.353906   \n",
       "13   98.128625  69.389259  30.832461    30.543604        ...         0.230043   \n",
       "14   99.149593  63.518915  22.710445    30.537300        ...         0.171012   \n",
       "15   98.923711  57.038358  16.507981    30.336701        ...         0.185989   \n",
       "16   95.677253  56.516483  19.106516    30.317936        ...         0.593883   \n",
       "17   92.224308  59.522505  22.550540    30.285208        ...         0.884931   \n",
       "18   91.503566  65.152632  31.710453    30.244792        ...         0.084196   \n",
       "19   92.336720  64.327544  26.854535    30.357503        ...         0.204506   \n",
       "20   93.210550  62.060546  26.042270    30.244478        ...         0.204506   \n",
       "21   98.458155  63.592018  24.996833    30.437811        ...         0.343522   \n",
       "22   99.130187  67.794578  27.402199    30.442214        ...         0.087948   \n",
       "23   99.310970  76.862397  39.885647    30.584784        ...         0.000000   \n",
       "24   99.867597  73.854349  34.931524    30.699720        ...         0.061922   \n",
       "25   98.683446  73.326261  34.011035    30.582852        ...         0.171012   \n",
       "26   99.867597  67.623090  25.905885    30.490657        ...         0.109089   \n",
       "27   98.650757  57.738017  20.443700    30.251877        ...         0.275534   \n",
       "28   95.544515  52.174506  20.090873    30.273455        ...         1.358716   \n",
       "29   92.462399  49.822999  21.137929    30.305041        ...         2.432953   \n",
       "..         ...        ...        ...          ...        ...              ...   \n",
       "340  98.567577  70.492357  33.789182    30.300776        ...         0.280616   \n",
       "341  98.017750  75.520995  29.919041    30.187260        ...         0.247688   \n",
       "342  98.619932  70.989761  28.101687    30.382688        ...         0.280616   \n",
       "343  99.549425  71.201853  22.058912    30.382767        ...         0.142584   \n",
       "344  98.642157  69.935334  27.929304    30.489855        ...         0.179613   \n",
       "345  99.246957  78.720917  38.324205    30.668162        ...         0.061922   \n",
       "346  98.593333  74.443192  37.529179    30.860636        ...         0.169950   \n",
       "347  97.397698  68.194308  28.997365    30.744742        ...         0.000000   \n",
       "348  99.345740  67.129144  21.228026    30.606852        ...         0.123844   \n",
       "349  98.647477  60.300742  16.797870    30.364271        ...         0.364541   \n",
       "350  98.472728  67.507878  21.849660    30.432112        ...         0.203939   \n",
       "351  98.374647  72.855429  32.695204    30.267854        ...         0.123844   \n",
       "352  97.220065  74.227458  32.245451    30.185179        ...         0.269692   \n",
       "353  98.073867  71.737793  31.222072    30.211701        ...         0.280616   \n",
       "354  98.097529  68.738023  27.391824    30.262646        ...         0.455681   \n",
       "355  98.096296  66.261166  20.668119    30.504368        ...         0.211792   \n",
       "356  98.394890  71.360548  28.181492    30.455684        ...         0.309611   \n",
       "357  99.769506  77.939941  35.794012    30.572263        ...         0.000000   \n",
       "358  99.346200  74.066579  38.988302    30.494712        ...         0.059394   \n",
       "359  99.734310  69.674338  26.245279    30.588809        ...         0.061922   \n",
       "360  99.868289  70.744770  22.796445    30.383500        ...         0.000000   \n",
       "361  99.246957  63.495779  18.660957    30.402745        ...         0.691810   \n",
       "362  98.782513  67.646893  21.923764    30.271661        ...         0.572066   \n",
       "363  98.280248  65.584746  22.049334    30.241324        ...         0.423862   \n",
       "364  99.336052  76.116392  35.745000    30.261990        ...         0.247688   \n",
       "365  98.526977  77.621620  40.558208    30.249344        ...         0.342538   \n",
       "366  99.467762  72.952297  22.789047    30.309965        ...         0.185766   \n",
       "367  99.670723  70.856970  26.610079    30.418100        ...         0.302619   \n",
       "368  99.769506  72.396604  30.732871    30.497395        ...         0.189700   \n",
       "369  99.769506  73.808132  35.018071    30.771110        ...         0.000000   \n",
       "\n",
       "     WIND_HIGH   WIND_AVG      PRECIP  NUMDAY_RAIN  NUM_DAYS_SNOW  \\\n",
       "0    29.600197  11.077783    0.479003     2.544772       7.584738   \n",
       "1    24.679482   9.622850    1.722515     5.510271       9.231277   \n",
       "2    36.374250  12.349509   15.116988     5.934169       3.868506   \n",
       "3    28.918771  10.770362   17.947352     9.195066       1.960945   \n",
       "4    33.072546   8.983943   15.212715     9.610446       0.327268   \n",
       "5    25.666101   8.317023   10.598875     8.135348       0.000000   \n",
       "6    30.924584   7.668664   11.201697     9.899297       0.000000   \n",
       "7    23.558382   6.839216    1.162475     6.987723       0.000000   \n",
       "8    31.473925   7.953490    6.651649    12.019534       0.109089   \n",
       "9    25.729013   7.975400   10.803140     9.754228       0.327268   \n",
       "10   27.369823   9.360489    0.710853     8.425101       3.263099   \n",
       "11   30.035203   9.005973   13.063238     4.954492       6.141817   \n",
       "12   27.208144   9.937532    0.697591     2.158710       9.017004   \n",
       "13   32.970059   9.232392   25.414489     3.158557       2.563075   \n",
       "14   29.907703  11.045504    6.407882     6.858583       2.852181   \n",
       "15   27.064239   9.509505   12.758238     8.253307       0.808174   \n",
       "16   31.632215   8.975590   24.486700     9.510037       0.218179   \n",
       "17   26.598362   7.892497    9.973803     7.983505       0.000000   \n",
       "18   29.864555   7.596663   39.075101    11.250805       0.000000   \n",
       "19   33.754199   7.664999    3.963129    10.481103       0.000000   \n",
       "20   25.124887   7.115135    5.524179     7.851274       0.000000   \n",
       "21   27.792394   9.120230    8.947375     8.870945       0.458632   \n",
       "22   27.737506  10.504951    6.922821    11.552323       3.370795   \n",
       "23   38.844028  11.646561    6.928783     9.977321       8.323399   \n",
       "24   31.689132  11.978539    1.444931     3.289410       8.299551   \n",
       "25   30.835942  11.366350    6.048729     2.780704       9.300564   \n",
       "26   34.058311  11.332478  372.538106     6.725774       6.585668   \n",
       "27   41.454133  10.788335    3.927050     8.654531       0.460086   \n",
       "28   37.070596   8.760104   16.876689     6.618215       0.109089   \n",
       "29   22.162214   7.857640    4.046168     2.947843       0.000000   \n",
       "..         ...        ...         ...          ...            ...   \n",
       "340  31.631130   6.640193    2.428684     9.495698       0.000000   \n",
       "341  29.820666   5.866341    4.025913    11.777075       0.000000   \n",
       "342  29.115301   6.217876    3.105514     7.529903       0.000000   \n",
       "343  30.957589   7.942366    2.901838    12.432674       0.437813   \n",
       "344  31.722576  10.206118    1.708385     7.384086       7.818873   \n",
       "345  27.371385   8.089446    1.598096     7.857272       3.744419   \n",
       "346  31.706604   8.372164    1.221606     6.276689       9.317115   \n",
       "347  33.970867   9.334185    1.278564     1.240099      11.237240   \n",
       "348  28.651405   8.266480    1.465324     7.336679       2.717470   \n",
       "349  34.562780   9.528799    2.739780    11.121298       0.274590   \n",
       "350  31.978787   9.271944    4.577083    13.856634       0.000000   \n",
       "351  40.923783   7.503916    6.465531    13.572339       0.000000   \n",
       "352  31.476291   6.044702    3.820778     9.391236       0.000000   \n",
       "353  25.342742   5.933962    2.931901     7.129545       0.000000   \n",
       "354  28.924293   6.483753    3.815673     7.487878       0.000000   \n",
       "355  29.154354   8.834588    1.599482     7.466220       0.000000   \n",
       "356  36.901813  10.017259    4.086457     9.970229       1.246099   \n",
       "357  38.124233  10.323283    5.184984    10.398913       2.988891   \n",
       "358  32.207612   9.778199    1.232944     4.799819       8.511348   \n",
       "359  36.206540   9.824466    0.836330     4.224789       6.814530   \n",
       "360  34.789749   9.629134    3.098442    11.949852       2.288162   \n",
       "361  39.165480   9.512475    2.662589    10.297377       1.718848   \n",
       "362  31.258585   8.051852    2.985320    13.488475       0.000000   \n",
       "363  26.449861   7.022661    2.645324     6.599050       0.000000   \n",
       "364  33.613471   6.212372    5.552655    11.304979       0.000000   \n",
       "365  29.407200   5.602346    4.854680    10.822233       0.000000   \n",
       "366  26.270849   6.906921    3.616776     9.658813       0.000000   \n",
       "367  31.092277   8.219588    1.907746     8.156643       0.022274   \n",
       "368  35.356631   7.994466    2.237944     6.781341       1.083395   \n",
       "369  29.388733   9.509581    1.391766     6.026444       7.998503   \n",
       "\n",
       "     NUM_DAYS_FOG  NUM_DAYS_THNDRSTRM  PRICE  DISCOUNTED_PRICE  \n",
       "0        9.707042            0.000000   2.35          2.349417  \n",
       "1       15.203328            0.658457   2.38          2.379410  \n",
       "2        9.890731            1.055313   2.35          2.349417  \n",
       "3        7.611098            2.058521   2.34          2.339420  \n",
       "4       14.161837            4.663441   2.45          2.449393  \n",
       "5       12.649299            4.995009   2.45          2.449393  \n",
       "6       12.561325            6.286502   2.04          2.039494  \n",
       "7       11.848591            1.872076   1.77          1.769561  \n",
       "8       13.926898            5.663013   1.52          1.519623  \n",
       "9       16.250392            1.152677   1.39          1.389655  \n",
       "10      17.081087            0.000000   1.47          1.469636  \n",
       "11      16.710648            0.000000   1.54          1.539618  \n",
       "12      14.353479            0.204561   1.53          1.529621  \n",
       "13      11.369789            0.077182   1.45          1.449640  \n",
       "14      10.275397            0.716611   1.51          1.509626  \n",
       "15       9.891667            1.736127   1.56          1.559613  \n",
       "16       9.562606            6.851201   1.72          1.719574  \n",
       "17      11.068762            6.553994   1.75          1.749566  \n",
       "18      16.522498            7.200001   1.66          1.659588  \n",
       "19      14.100641            6.232162   1.53          1.529621  \n",
       "20      12.212706            2.418011   1.53          1.529621  \n",
       "21      10.156463            0.873945   1.60          1.599603  \n",
       "22      11.110684            0.573445   1.68          1.679583  \n",
       "23      14.877116            1.181361   1.80          1.799554  \n",
       "24       7.312518            0.576510   1.81          1.809551  \n",
       "25       9.198702            0.044134   1.90          1.899529  \n",
       "26      10.346304            2.881596   1.91          1.909526  \n",
       "27       6.650997            3.396558   1.94          1.939519  \n",
       "28       3.626469            3.238601   2.03          2.029497  \n",
       "29       3.199814            1.358670   2.51          2.509378  \n",
       "..            ...                 ...    ...               ...  \n",
       "340      2.217399            5.176212   4.12          4.118978  \n",
       "341      5.004267            7.259140   3.74          3.739073  \n",
       "342      4.055058            4.412042   3.51          3.509130  \n",
       "343      4.171134            2.420886   3.58          3.579112  \n",
       "344      2.738053            0.033495   3.63          3.629100  \n",
       "345      4.276190            0.128912   3.81          3.809055  \n",
       "346      3.029987            0.048754   3.82          3.819053  \n",
       "347      3.037794            0.061922   3.78          3.779063  \n",
       "348      3.919565            0.523391   3.79          3.789060  \n",
       "349      1.776208            4.189540   3.71          3.709080  \n",
       "350      1.490798            5.521460   3.62          3.619102  \n",
       "351      4.206530           10.167240   3.65          3.649095  \n",
       "352      3.508345            4.724758   3.85          3.849045  \n",
       "353      2.308489            4.609560   3.63          3.629100  \n",
       "354      1.742017            4.006889   3.69          3.689085  \n",
       "355      2.478168            0.909474   3.70          3.699083  \n",
       "356      5.044528            1.015249   3.70          3.699083  \n",
       "357      6.702315            2.058452   3.79          3.789060  \n",
       "358      2.459232            0.015259   3.73          3.729075  \n",
       "359      3.299300            0.565800   3.63          3.629100  \n",
       "360      3.608914            2.883812   3.67          3.669090  \n",
       "361      2.704257            3.283336   3.67          3.669090  \n",
       "362      2.162346            6.827281   3.82          3.819053  \n",
       "363      1.866101            4.338305   3.92          3.919028  \n",
       "364      4.765973            9.193705   3.63          3.629100  \n",
       "365      3.440266            7.746643   3.17          3.169214  \n",
       "366      3.307687            4.115569   3.27          3.269189  \n",
       "367      0.872271            3.452099   3.30          3.299182  \n",
       "368      3.198530            1.929992   3.32          3.319177  \n",
       "369      4.082039            0.110676   3.47          3.469140  \n",
       "\n",
       "[370 rows x 24 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296, 22) (296,)\n",
      "(74, 22) (74,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Splitting Data into Training and Test\n",
    "predictors, X_test, targets, y_test = train_test_split(input_variables.iloc[:,0:22], price, test_size=0.2)\n",
    "print (predictors.shape, targets.shape)\n",
    "print (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just using average=3.060169491525426 has RMSE of 1.398388923869069\n"
     ]
    }
   ],
   "source": [
    "trainsize = int(len(targets) * 0.8)\n",
    "avg = np.mean(targets[:trainsize])\n",
    "rmse = np.sqrt(np.mean((targets[trainsize:] - avg)**2))\n",
    "print('Just using average={0} has RMSE of {1}'.format(avg, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11d430d30>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': './trained_model_linear'}\n",
      "starting to train ... this will take a while ... use verbosity=INFO to get more verbose output\n",
      "WARNING:tensorflow:From /Users/jegan/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:642: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./trained_model_linear/model.ckpt.\n",
      "INFO:tensorflow:loss = 4.92066, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1296.9\n",
      "INFO:tensorflow:loss = 0.78958, step = 101 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1466.81\n",
      "INFO:tensorflow:loss = 0.745996, step = 201 (0.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 1439.99\n",
      "INFO:tensorflow:loss = 0.733982, step = 301 (0.070 sec)\n",
      "INFO:tensorflow:global_step/sec: 1438.17\n",
      "INFO:tensorflow:loss = 0.726505, step = 401 (0.069 sec)\n",
      "INFO:tensorflow:global_step/sec: 1347.93\n",
      "INFO:tensorflow:loss = 0.719974, step = 501 (0.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 1190.71\n",
      "INFO:tensorflow:loss = 0.713891, step = 601 (0.084 sec)\n",
      "INFO:tensorflow:global_step/sec: 1123.17\n",
      "INFO:tensorflow:loss = 0.708172, step = 701 (0.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 1019.62\n",
      "INFO:tensorflow:loss = 0.702785, step = 801 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1315.2\n",
      "INFO:tensorflow:loss = 0.697709, step = 901 (0.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 1394.04\n",
      "INFO:tensorflow:loss = 0.692923, step = 1001 (0.072 sec)\n",
      "INFO:tensorflow:global_step/sec: 1426.31\n",
      "INFO:tensorflow:loss = 0.688409, step = 1101 (0.070 sec)\n",
      "INFO:tensorflow:global_step/sec: 1389.51\n",
      "INFO:tensorflow:loss = 0.68415, step = 1201 (0.072 sec)\n",
      "INFO:tensorflow:global_step/sec: 1393.96\n",
      "INFO:tensorflow:loss = 0.68013, step = 1301 (0.072 sec)\n",
      "INFO:tensorflow:global_step/sec: 1389.08\n",
      "INFO:tensorflow:loss = 0.676333, step = 1401 (0.072 sec)\n",
      "INFO:tensorflow:global_step/sec: 1344.7\n",
      "INFO:tensorflow:loss = 0.672745, step = 1501 (0.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 1217.23\n",
      "INFO:tensorflow:loss = 0.669354, step = 1601 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 1198.35\n",
      "INFO:tensorflow:loss = 0.666146, step = 1701 (0.084 sec)\n",
      "INFO:tensorflow:global_step/sec: 1110.45\n",
      "INFO:tensorflow:loss = 0.663111, step = 1801 (0.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 1198.87\n",
      "INFO:tensorflow:loss = 0.660237, step = 1901 (0.084 sec)\n",
      "INFO:tensorflow:global_step/sec: 1319.82\n",
      "INFO:tensorflow:loss = 0.657515, step = 2001 (0.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 1433.67\n",
      "INFO:tensorflow:loss = 0.654934, step = 2101 (0.070 sec)\n",
      "INFO:tensorflow:global_step/sec: 1449.72\n",
      "INFO:tensorflow:loss = 0.652486, step = 2201 (0.069 sec)\n",
      "INFO:tensorflow:global_step/sec: 1449.57\n",
      "INFO:tensorflow:loss = 0.650162, step = 2301 (0.069 sec)\n",
      "INFO:tensorflow:global_step/sec: 1435.22\n",
      "INFO:tensorflow:loss = 0.647956, step = 2401 (0.069 sec)\n",
      "INFO:tensorflow:global_step/sec: 1459.32\n",
      "INFO:tensorflow:loss = 0.645859, step = 2501 (0.069 sec)\n",
      "INFO:tensorflow:global_step/sec: 1406.96\n",
      "INFO:tensorflow:loss = 0.643865, step = 2601 (0.071 sec)\n",
      "INFO:tensorflow:global_step/sec: 1412.67\n",
      "INFO:tensorflow:loss = 0.641967, step = 2701 (0.071 sec)\n",
      "INFO:tensorflow:global_step/sec: 1353.64\n",
      "INFO:tensorflow:loss = 0.640159, step = 2801 (0.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 1395.4\n",
      "INFO:tensorflow:loss = 0.638437, step = 2901 (0.071 sec)\n",
      "INFO:tensorflow:global_step/sec: 1419.14\n",
      "INFO:tensorflow:loss = 0.636793, step = 3001 (0.071 sec)\n",
      "INFO:tensorflow:global_step/sec: 1252.21\n",
      "INFO:tensorflow:loss = 0.635225, step = 3101 (0.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 1205.6\n",
      "INFO:tensorflow:loss = 0.633727, step = 3201 (0.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 1333.33\n",
      "INFO:tensorflow:loss = 0.632294, step = 3301 (0.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 1321.88\n",
      "INFO:tensorflow:loss = 0.630924, step = 3401 (0.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 1241.71\n",
      "INFO:tensorflow:loss = 0.629611, step = 3501 (0.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 1245.86\n",
      "INFO:tensorflow:loss = 0.628353, step = 3601 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1160.05\n",
      "INFO:tensorflow:loss = 0.627146, step = 3701 (0.086 sec)\n",
      "INFO:tensorflow:global_step/sec: 1069.01\n",
      "INFO:tensorflow:loss = 0.625987, step = 3801 (0.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 1083.94\n",
      "INFO:tensorflow:loss = 0.624874, step = 3901 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 1248.35\n",
      "INFO:tensorflow:loss = 0.623803, step = 4001 (0.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 1333.83\n",
      "INFO:tensorflow:loss = 0.622772, step = 4101 (0.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 1356.19\n",
      "INFO:tensorflow:loss = 0.621779, step = 4201 (0.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 1311.49\n",
      "INFO:tensorflow:loss = 0.620821, step = 4301 (0.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 1268.2\n",
      "INFO:tensorflow:loss = 0.619897, step = 4401 (0.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 1238.15\n",
      "INFO:tensorflow:loss = 0.619004, step = 4501 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1175.85\n",
      "INFO:tensorflow:loss = 0.61814, step = 4601 (0.086 sec)\n",
      "INFO:tensorflow:global_step/sec: 1102.82\n",
      "INFO:tensorflow:loss = 0.617305, step = 4701 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1010.41\n",
      "INFO:tensorflow:loss = 0.616495, step = 4801 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1222.13\n",
      "INFO:tensorflow:loss = 0.615711, step = 4901 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 1289.97\n",
      "INFO:tensorflow:loss = 0.614949, step = 5001 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1224.44\n",
      "INFO:tensorflow:loss = 0.61421, step = 5101 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1265.29\n",
      "INFO:tensorflow:loss = 0.613492, step = 5201 (0.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 1220.48\n",
      "INFO:tensorflow:loss = 0.612793, step = 5301 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1062.09\n",
      "INFO:tensorflow:loss = 0.612113, step = 5401 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1050.32\n",
      "INFO:tensorflow:loss = 0.61145, step = 5501 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 987.12\n",
      "INFO:tensorflow:loss = 0.610804, step = 5601 (0.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 928.479\n",
      "INFO:tensorflow:loss = 0.610173, step = 5701 (0.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 1290.24\n",
      "INFO:tensorflow:loss = 0.609558, step = 5801 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1035.49\n",
      "INFO:tensorflow:loss = 0.608956, step = 5901 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 966.61\n",
      "INFO:tensorflow:loss = 0.608368, step = 6001 (0.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 911.213\n",
      "INFO:tensorflow:loss = 0.607792, step = 6101 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 919.042\n",
      "INFO:tensorflow:loss = 0.607228, step = 6201 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 1086.96\n",
      "INFO:tensorflow:loss = 0.606676, step = 6301 (0.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 1109.35\n",
      "INFO:tensorflow:loss = 0.606134, step = 6401 (0.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 1251.93\n",
      "INFO:tensorflow:loss = 0.605603, step = 6501 (0.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 1069.54\n",
      "INFO:tensorflow:loss = 0.605081, step = 6601 (0.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 1002.09\n",
      "INFO:tensorflow:loss = 0.604569, step = 6701 (0.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 1160.85\n",
      "INFO:tensorflow:loss = 0.604065, step = 6801 (0.086 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 1145.33\n",
      "INFO:tensorflow:loss = 0.60357, step = 6901 (0.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 1239.42\n",
      "INFO:tensorflow:loss = 0.603083, step = 7001 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1138.37\n",
      "INFO:tensorflow:loss = 0.602603, step = 7101 (0.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 1105.34\n",
      "INFO:tensorflow:loss = 0.602131, step = 7201 (0.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 1299.11\n",
      "INFO:tensorflow:loss = 0.601666, step = 7301 (0.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 1089.18\n",
      "INFO:tensorflow:loss = 0.601207, step = 7401 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 1240.21\n",
      "INFO:tensorflow:loss = 0.600755, step = 7501 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1269.03\n",
      "INFO:tensorflow:loss = 0.600309, step = 7601 (0.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 1263.79\n",
      "INFO:tensorflow:loss = 0.599868, step = 7701 (0.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 1233.12\n",
      "INFO:tensorflow:loss = 0.599434, step = 7801 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1246.34\n",
      "INFO:tensorflow:loss = 0.599004, step = 7901 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1202.83\n",
      "INFO:tensorflow:loss = 0.59858, step = 8001 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 1089.64\n",
      "INFO:tensorflow:loss = 0.598161, step = 8101 (0.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 972.146\n",
      "INFO:tensorflow:loss = 0.597746, step = 8201 (0.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 1225.64\n",
      "INFO:tensorflow:loss = 0.597337, step = 8301 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 1037.9\n",
      "INFO:tensorflow:loss = 0.596931, step = 8401 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1031.02\n",
      "INFO:tensorflow:loss = 0.59653, step = 8501 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1313.87\n",
      "INFO:tensorflow:loss = 0.596133, step = 8601 (0.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 1298.64\n",
      "INFO:tensorflow:loss = 0.59574, step = 8701 (0.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 1216.04\n",
      "INFO:tensorflow:loss = 0.595351, step = 8801 (0.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 1219.31\n",
      "INFO:tensorflow:loss = 0.594965, step = 8901 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1121.16\n",
      "INFO:tensorflow:loss = 0.594583, step = 9001 (0.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 1084.84\n",
      "INFO:tensorflow:loss = 0.594205, step = 9101 (0.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 1036.64\n",
      "INFO:tensorflow:loss = 0.59383, step = 9201 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1366.44\n",
      "INFO:tensorflow:loss = 0.593458, step = 9301 (0.073 sec)\n",
      "INFO:tensorflow:global_step/sec: 1340.37\n",
      "INFO:tensorflow:loss = 0.593089, step = 9401 (0.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 1368.52\n",
      "INFO:tensorflow:loss = 0.592724, step = 9501 (0.073 sec)\n",
      "INFO:tensorflow:global_step/sec: 1306.21\n",
      "INFO:tensorflow:loss = 0.592361, step = 9601 (0.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 1244.43\n",
      "INFO:tensorflow:loss = 0.592002, step = 9701 (0.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 1224.86\n",
      "INFO:tensorflow:loss = 0.591645, step = 9801 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 1117.84\n",
      "INFO:tensorflow:loss = 0.591291, step = 9901 (0.089 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into ./trained_model_linear/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.590943.\n",
      "WARNING:tensorflow:From /Users/jegan/anaconda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:355: calling LinearRegressor.predict (from tensorflow.contrib.learn.python.learn.estimators.linear) with outputs=None is deprecated and will be removed after 2017-03-01.\n",
      "Instructions for updating:\n",
      "Please switch to predict_scores, or set `outputs` argument.\n",
      "WARNING:tensorflow:From /Users/jegan/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py:832: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Restoring parameters from ./trained_model_linear/model.ckpt-10000\n",
      "LinearRegression has RMSE of 1.2818466760433809\n"
     ]
    }
   ],
   "source": [
    "scale_price = 1.5\n",
    "trainsize = int(len(targets) * 0.8)\n",
    "testsize = len(targets) - trainsize\n",
    "npredictors = len(predictors.columns)\n",
    "noutputs = 1\n",
    "shutil.rmtree('./trained_model_linear', ignore_errors=True) # so that we don't load weights from previous runs\n",
    "estimator = tf.contrib.learn.LinearRegressor(model_dir='./trained_model_linear',\n",
    "                                             feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values))\n",
    "\n",
    "print(\"starting to train ... this will take a while ... use verbosity=INFO to get more verbose output\")\n",
    "def input_fn(features, targets):\n",
    "  return tf.constant(features.values), tf.constant(targets.values.reshape(len(targets), noutputs)/scale_price)\n",
    "estimator.fit(input_fn=lambda: input_fn(predictors[:trainsize], targets[:trainsize]), steps=10000)\n",
    "\n",
    "pred = np.multiply(list(estimator.predict(predictors[trainsize:].values)), scale_price )\n",
    "rmse = np.sqrt(np.mean(np.power((targets[trainsize:].values - pred), 2)))\n",
    "print('LinearRegression has RMSE of {0}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "starting to train ... this will take a while ... use verbosity=INFO to get more verbose output\n",
      "WARNING:tensorflow:From /Users/jegan/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:642: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From /Users/jegan/anaconda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:355: calling DNNRegressor.predict (from tensorflow.contrib.learn.python.learn.estimators.dnn) with outputs=None is deprecated and will be removed after 2017-03-01.\n",
      "Instructions for updating:\n",
      "Please switch to predict_scores, or set `outputs` argument.\n",
      "WARNING:tensorflow:From /Users/jegan/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:764: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "Neural Network Regression has RMSE of 1.496965148459889\n"
     ]
    }
   ],
   "source": [
    "scale_price = 1.5\n",
    "trainsize = int(len(targets) * 0.8)\n",
    "testsize = len(targets) - trainsize\n",
    "npredictors = len(predictors.columns)\n",
    "noutputs = 1\n",
    "tf.logging.set_verbosity(tf.logging.WARN) # change to INFO to get output every 100 steps ...\n",
    "shutil.rmtree('./trained_model', ignore_errors=True) # so that we don't load weights from previous runs\n",
    "estimator = tf.contrib.learn.DNNRegressor(model_dir='./trained_model',\n",
    "                                          hidden_units=[5, 5],                             \n",
    "                                          feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values))\n",
    "\n",
    "print(\"starting to train ... this will take a while ... use verbosity=INFO to get more verbose output\")\n",
    "def input_fn(features, targets):\n",
    "  return tf.constant(features.values), tf.constant(targets.values.reshape(len(targets), noutputs)/scale_price)\n",
    "estimator.fit(input_fn=lambda: input_fn(predictors[:trainsize], targets[:trainsize]), steps=10000)\n",
    "\n",
    "pred = np.multiply(list(estimator.predict(predictors[trainsize:].values)), scale_price )\n",
    "rmse = np.sqrt(np.mean((targets[trainsize:].values - pred)**2))\n",
    "print('Neural Network Regression has RMSE of {0}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.14738249457431851"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(targets[trainsize:].values,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Must beat this r2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_weights_biases(num_layers, n_inputs, multiplier, max_nodes):\n",
    "    '''Use the inputs to create the weights and biases for a network'''\n",
    "    \n",
    "    # Empty dictionaries to store the weights and biases for each layer\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "    \n",
    "    # Create weights and biases for all layers, but the final layer\n",
    "    for layer in range(1,num_layers):\n",
    "        # The first layer needs to use the number of features that are in the dataframe\n",
    "        if layer == 1:\n",
    "            weights[\"h\"+str(layer)] = tf.Variable(tf.random_normal([num_features, n_inputs],\n",
    "                                                                   stddev=np.sqrt(1/num_features)))\n",
    "            biases[\"b\"+str(layer)] = tf.Variable(tf.random_normal([n_inputs],stddev=0))\n",
    "            # n_previous keeps track of the number of nodes in the previous layer\n",
    "            n_previous = n_inputs\n",
    "            \n",
    "        else:    \n",
    "            # To alter number of nodes in each layer, multiply n_previous by multiplier \n",
    "            n_current = int(n_previous * multiplier)\n",
    "            \n",
    "            # Limit the number of nodes to the maximum amount\n",
    "            if n_current >= max_nodes:\n",
    "                n_current = max_nodes\n",
    "                \n",
    "            weights[\"h\"+str(layer)] = tf.Variable(tf.random_normal([n_previous, n_current],\n",
    "                                                                       stddev=np.sqrt(1/n_previous)))\n",
    "            biases[\"b\"+str(layer)] = tf.Variable(tf.random_normal([n_current],stddev=0))\n",
    "            n_previous = n_current\n",
    "            \n",
    "    # Create weights for the final layer\n",
    "    n_current = int(n_previous * multiplier)\n",
    "    if n_current >= max_nodes:\n",
    "        n_current = max_nodes\n",
    "            \n",
    "    # The final layer only has 1 node since this is a regression task\n",
    "    weights[\"out\"] = tf.Variable(tf.random_normal([n_previous, 1], stddev=np.sqrt(1/n_previous)))\n",
    "    biases[\"out\"] = tf.Variable(tf.random_normal([1],stddev=0))\n",
    "                                                    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network(num_layers, n_inputs, weights, biases, rate, is_training, activation_function):\n",
    "    '''Add the required number of layers to the network'''\n",
    "    \n",
    "    for layer in range(1, num_layers):\n",
    "        if layer == 1:\n",
    "            current_layer = eval(activation_function + \"(tf.matmul(n_inputs, weights['h1']) + biases['b1'])\")\n",
    "            current_layer = tf.nn.dropout(current_layer, 1-rate)\n",
    "            previous_layer = current_layer\n",
    "        else:\n",
    "            current_layer = eval(activation_function + \"(tf.matmul(previous_layer,\\\n",
    "            weights['h'+str(layer)]) + biases['b'+str(layer)])\")\n",
    "            current_layer = tf.nn.dropout(current_layer, 1-rate)\n",
    "            previous_layer = current_layer\n",
    "\n",
    "    # Output layer with linear activation - because regression\n",
    "    out_layer = tf.matmul(previous_layer, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create placeholders for model's inputs '''\n",
    "    \n",
    "    inputs = tf.placeholder(tf.float32, [None, None], name='inputs')\n",
    "    targets = tf.placeholder(tf.float32, [None, 1], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    dropout_rate = tf.placeholder(tf.float32, name='dropout_rate')\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    \n",
    "    return inputs, targets, learning_rate, dropout_rate, is_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(num_layers,n_inputs,weights_multiplier,dropout_rate,learning_rate,max_nodes,activation_function):\n",
    "    '''Use inputs to build the graph and export the required features for training'''\n",
    "    \n",
    "    # Reset the graph to ensure it is ready for training\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Get the inputs\n",
    "    inputs, targets, learning_rate, dropout_rate, is_training = model_inputs()\n",
    "    \n",
    "    # Get the weights and biases\n",
    "    weights, biases = create_weights_biases(num_layers, n_inputs, weights_multiplier, max_nodes)\n",
    "    \n",
    "    # Construct the network\n",
    "    preds = network(num_layers, inputs, weights, biases, dropout_rate, is_training, activation_function)    \n",
    "            \n",
    "    with tf.name_scope(\"cost\"):\n",
    "        # Cost function\n",
    "        cost = tf.sqrt(tf.losses.mean_squared_error(labels=targets, predictions=preds))\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    with tf.name_scope(\"optimze\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # Merge all of the summaries\n",
    "    merged = tf.summary.merge_all()    \n",
    "\n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs','targets','dropout_rate','is_training','cost','preds','merged',\n",
    "                    'optimizer','learning_rate']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, log_string, learning_rate):\n",
    "    '''Train the Network and return the average RMSE for each iteration of the model'''\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Used to determine when to stop the training early\n",
    "        testing_loss_summary = []\n",
    "\n",
    "        iteration = 0 # Keep track of which batch iteration is being trained\n",
    "        stop_early = 0 # Keep track of how many consective epochs have not achieved a record low RMSE\n",
    "        stop = 5 # If the batch_loss_testing does not decrease in 5 consecutive epochs, stop training\n",
    "        per_epoch_training = 2 # Check training progress 2 times per epcoh\n",
    "        per_epoch_testing = 1 # Check testing progress 1 time per epoch\n",
    "        \n",
    "        # Decay learning rate after consective epochs of no improvements\n",
    "        learning_rate_decay_threshold = np.random.choice([2,3]) \n",
    "        original_learning_rate = learning_rate # Keep track of orginial learning rate for each split\n",
    "\n",
    "        print()\n",
    "        print(\"Training Model: {}\".format(log_string))\n",
    "\n",
    "        # Record progress to view with TensorBoard\n",
    "        train_writer = tf.summary.FileWriter('./logs/1/train/{}'.format(log_string), sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./logs/1/test/{}'.format(log_string))\n",
    "        \n",
    "        training_check = (len(x_train)//batch_size//per_epoch_training)-1 # Check training progress after this many batches\n",
    "        testing_check = (len(x_train)//batch_size//per_epoch_testing)-1 # Check testing results after this many batches\n",
    "\n",
    "        for epoch_i in range(1, epochs+1): \n",
    "            batch_loss = 0\n",
    "            batch_time = 0\n",
    "\n",
    "            for batch in range(int(len(x_train)/batch_size)):\n",
    "                batch_x = x_train[batch*batch_size:(1+batch)*batch_size]\n",
    "                batch_y = y_train[batch*batch_size:(1+batch)*batch_size]\n",
    "                batch_y = batch_y.reshape((1,1))\n",
    "                \n",
    "                start_time = time.time()\n",
    "\n",
    "                summary, loss, _ = sess.run([model.merged,\n",
    "                                             model.cost, \n",
    "                                             model.optimizer], \n",
    "                                             {model.inputs: batch_x,\n",
    "                                              model.targets: batch_y,\n",
    "                                              model.learning_rate: learning_rate,\n",
    "                                              model.dropout_rate: dropout_rate,\n",
    "                                              model.is_training: True})\n",
    "\n",
    "\n",
    "                batch_loss += loss\n",
    "                end_time = time.time()\n",
    "                batch_time += end_time - start_time\n",
    "\n",
    "                # Record the progress of training\n",
    "                train_writer.add_summary(summary, iteration)\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "                if batch % training_check == 0 and batch > 0:\n",
    "                    print('Epoch {:>3}/{} Batch {:>4}/{} - RMSE: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(epoch_i,\n",
    "                                  epochs, \n",
    "                                  batch, \n",
    "                                  len(x_train) // batch_size, \n",
    "                                  (batch_loss / training_check), \n",
    "                                  batch_time))\n",
    "                    batch_loss = 0\n",
    "                    batch_time = 0\n",
    "\n",
    "                #### Testing ####\n",
    "                if batch % testing_check == 0 and batch > 0:\n",
    "                    batch_loss_testing = 0\n",
    "                    batch_time_testing = 0\n",
    "                    for batch in range(int(len(x_test)/batch_size)):\n",
    "                        batch_x = x_test[batch*batch_size:(1+batch)*batch_size]\n",
    "                        batch_y = y_test[batch*batch_size:(1+batch)*batch_size]\n",
    "                        batch_y = batch_y.reshape((1,1))\n",
    "                        \n",
    "                        start_time_testing = time.time()\n",
    "                        summary, loss = sess.run([model.merged,\n",
    "                                                  model.cost], \n",
    "                                                     {model.inputs: batch_x,\n",
    "                                                      model.targets: batch_y,\n",
    "                                                      model.learning_rate: learning_rate,\n",
    "                                                      model.dropout_rate: 0,\n",
    "                                                      model.is_training: False})\n",
    "\n",
    "                        batch_loss_testing += loss\n",
    "                        end_time_testing = time.time()\n",
    "                        batch_time_testing += end_time_testing - start_time_testing\n",
    "\n",
    "                        # Record the progress of testing\n",
    "                        test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                    n_batches_testing = batch + 1\n",
    "                    print('Testing RMSE: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(batch_loss_testing / n_batches_testing, \n",
    "                                  batch_time_testing))\n",
    "\n",
    "                    batch_time_testing = 0\n",
    "\n",
    "                    # If the batch_loss_testing is at a new minimum, save the model\n",
    "                    testing_loss_summary.append(batch_loss_testing)\n",
    "                    if batch_loss_testing <= min(testing_loss_summary):\n",
    "                        print('New Record!') \n",
    "                        lowest_loss_testing = batch_loss_testing/n_batches_testing\n",
    "                        stop_early = 0 # Reset stop_early if new minimum loss is found\n",
    "                        checkpoint = \"./{}.ckpt\".format(log_string)\n",
    "                        saver = tf.train.Saver()\n",
    "                        saver.save(sess, checkpoint)\n",
    "\n",
    "                    else:\n",
    "                        print(\"No Improvement.\")\n",
    "                        stop_early += 1 # Increase stop_early if no new minimum loss is found\n",
    "                        if stop_early % learning_rate_decay_threshold == 0:\n",
    "                            learning_rate *= learning_rate_decay\n",
    "                            print(\"New learning rate = \", learning_rate)\n",
    "                        elif stop_early == stop:\n",
    "                            break\n",
    "\n",
    "            if stop_early == stop:\n",
    "                print(\"Stopping training for this iteration.\")\n",
    "                print(\"Lowest RMSE =\", lowest_loss_testing)\n",
    "                print()\n",
    "                early_stop = 0\n",
    "                testing_loss_summary = []\n",
    "                break\n",
    "        \n",
    "    return lowest_loss_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration # 1\n",
      "\n",
      "Training Model: LR=0.03717438835215774,LRD=0.4562386149915538,WM=1.9080688927806695,NI=15,NL=4,DR=0.07370120831785891,BS=1,MN=307,AF=tf.nn.elu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jegan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:37: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/50 Batch  124/251 - RMSE: 2674.059, Seconds: 0.24\n",
      "Epoch   1/50 Batch  248/251 - RMSE:  1.246, Seconds: 0.24\n",
      "Testing RMSE:  1.097, Seconds: 0.05\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jegan/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:78: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/50 Batch  124/251 - RMSE:  1.131, Seconds: 0.23\n",
      "Epoch   2/50 Batch  248/251 - RMSE:  1.216, Seconds: 0.22\n",
      "Testing RMSE:  1.159, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   3/50 Batch  124/251 - RMSE:  1.139, Seconds: 0.20\n",
      "Epoch   3/50 Batch  248/251 - RMSE:  1.200, Seconds: 0.20\n",
      "Testing RMSE:  1.140, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  0.0169603914549466\n",
      "Epoch   4/50 Batch  124/251 - RMSE:  1.056, Seconds: 0.21\n",
      "Epoch   4/50 Batch  248/251 - RMSE:  1.031, Seconds: 0.22\n",
      "Testing RMSE:  0.982, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch   5/50 Batch  124/251 - RMSE:  1.129, Seconds: 0.30\n",
      "Epoch   5/50 Batch  248/251 - RMSE:  1.057, Seconds: 0.25\n",
      "Testing RMSE:  1.055, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   6/50 Batch  124/251 - RMSE:  1.095, Seconds: 0.23\n",
      "Epoch   6/50 Batch  248/251 - RMSE:  1.067, Seconds: 0.24\n",
      "Testing RMSE:  1.101, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  0.007737985507119421\n",
      "Epoch   7/50 Batch  124/251 - RMSE:  1.085, Seconds: 0.24\n",
      "Epoch   7/50 Batch  248/251 - RMSE:  1.052, Seconds: 0.24\n",
      "Testing RMSE:  1.009, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   8/50 Batch  124/251 - RMSE:  1.035, Seconds: 0.24\n",
      "Epoch   8/50 Batch  248/251 - RMSE:  1.026, Seconds: 0.23\n",
      "Testing RMSE:  1.116, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  0.003530367790592881\n",
      "Epoch   9/50 Batch  124/251 - RMSE:  1.014, Seconds: 0.24\n",
      "Epoch   9/50 Batch  248/251 - RMSE:  0.973, Seconds: 0.27\n",
      "Testing RMSE:  0.994, Seconds: 0.04\n",
      "No Improvement.\n",
      "Stopping training for this iteration.\n",
      "Lowest RMSE = 0.982024002075\n",
      "\n",
      "Starting iteration # 2\n",
      "\n",
      "Training Model: LR=0.036202815673185514,LRD=0.3671728226509555,WM=0.9849749907965041,NI=12,NL=3,DR=0.10335396990401313,BS=1,MN=87,AF=tf.nn.relu\n",
      "Epoch   1/50 Batch  124/251 - RMSE: 162.192, Seconds: 0.22\n",
      "Epoch   1/50 Batch  248/251 - RMSE:  1.228, Seconds: 0.21\n",
      "Testing RMSE:  0.978, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   2/50 Batch  124/251 - RMSE: 1456.973, Seconds: 0.19\n",
      "Epoch   2/50 Batch  248/251 - RMSE:  0.943, Seconds: 0.20\n",
      "Testing RMSE:  0.996, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch   3/50 Batch  124/251 - RMSE:  0.966, Seconds: 0.22\n",
      "Epoch   3/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.21\n",
      "Testing RMSE:  0.996, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch   4/50 Batch  124/251 - RMSE:  0.966, Seconds: 0.22\n",
      "Epoch   4/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.20\n",
      "Testing RMSE:  0.978, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  0.013292690018635778\n",
      "Epoch   5/50 Batch  124/251 - RMSE:  0.959, Seconds: 0.20\n",
      "Epoch   5/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.20\n",
      "Testing RMSE:  0.987, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   6/50 Batch  124/251 - RMSE:  0.966, Seconds: 0.22\n",
      "Epoch   6/50 Batch  248/251 - RMSE:  0.910, Seconds: 0.22\n",
      "Testing RMSE:  0.985, Seconds: 0.04\n",
      "No Improvement.\n",
      "Stopping training for this iteration.\n",
      "Lowest RMSE = 0.978171316783\n",
      "\n",
      "Starting iteration # 3\n",
      "\n",
      "Training Model: LR=0.0716670927543766,LRD=0.3189000237438304,WM=0.788631247942609,NI=35,NL=3,DR=0.15695128623821594,BS=1,MN=102,AF=tf.nn.elu\n",
      "Epoch   1/50 Batch  124/251 - RMSE: 3976.212, Seconds: 0.20\n",
      "Epoch   1/50 Batch  248/251 - RMSE:  2.713, Seconds: 0.19\n",
      "Testing RMSE:  0.987, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   2/50 Batch  124/251 - RMSE:  1.745, Seconds: 0.17\n",
      "Epoch   2/50 Batch  248/251 - RMSE:  1.690, Seconds: 0.15\n",
      "Testing RMSE:  1.065, Seconds: 0.03\n",
      "No Improvement.\n",
      "Epoch   3/50 Batch  124/251 - RMSE:  1.762, Seconds: 0.19\n",
      "Epoch   3/50 Batch  248/251 - RMSE:  1.863, Seconds: 0.21\n",
      "Testing RMSE:  0.980, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch   4/50 Batch  124/251 - RMSE:  1.830, Seconds: 0.20\n",
      "Epoch   4/50 Batch  248/251 - RMSE:  1.521, Seconds: 0.21\n",
      "Testing RMSE:  1.054, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   5/50 Batch  124/251 - RMSE:  1.460, Seconds: 0.19\n",
      "Epoch   5/50 Batch  248/251 - RMSE:  1.602, Seconds: 0.19\n",
      "Testing RMSE:  0.997, Seconds: 0.05\n",
      "No Improvement.\n",
      "New learning rate =  0.022854637581021995\n",
      "Epoch   6/50 Batch  124/251 - RMSE:  1.632, Seconds: 0.22\n",
      "Epoch   6/50 Batch  248/251 - RMSE:  1.445, Seconds: 0.20\n",
      "Testing RMSE:  1.012, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   7/50 Batch  124/251 - RMSE:  1.848, Seconds: 0.20\n",
      "Epoch   7/50 Batch  248/251 - RMSE:  1.675, Seconds: 0.19\n",
      "Testing RMSE:  0.989, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  0.007288344467244553\n",
      "Epoch   8/50 Batch  124/251 - RMSE:  1.683, Seconds: 0.21\n",
      "Epoch   8/50 Batch  248/251 - RMSE:  1.521, Seconds: 0.20\n",
      "Testing RMSE:  1.009, Seconds: 0.04\n",
      "No Improvement.\n",
      "Stopping training for this iteration.\n",
      "Lowest RMSE = 0.979570653703\n",
      "\n",
      "Starting iteration # 4\n",
      "\n",
      "Training Model: LR=0.015063104394775585,LRD=0.25687039341872775,WM=0.5064055011751063,NI=7,NL=3,DR=0.20941968856757676,BS=1,MN=385,AF=tf.nn.elu\n",
      "Epoch   1/50 Batch  124/251 - RMSE:  2.331, Seconds: 0.22\n",
      "Epoch   1/50 Batch  248/251 - RMSE:  1.089, Seconds: 0.22\n",
      "Testing RMSE:  1.049, Seconds: 0.06\n",
      "New Record!\n",
      "Epoch   2/50 Batch  124/251 - RMSE:  1.128, Seconds: 0.20\n",
      "Epoch   2/50 Batch  248/251 - RMSE:  1.118, Seconds: 0.18\n",
      "Testing RMSE:  0.989, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch   3/50 Batch  124/251 - RMSE:  1.099, Seconds: 0.26\n",
      "Epoch   3/50 Batch  248/251 - RMSE:  0.969, Seconds: 0.23\n",
      "Testing RMSE:  1.006, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   4/50 Batch  124/251 - RMSE:  1.011, Seconds: 0.24\n",
      "Epoch   4/50 Batch  248/251 - RMSE:  0.990, Seconds: 0.24\n",
      "Testing RMSE:  1.018, Seconds: 0.05\n",
      "No Improvement.\n",
      "New learning rate =  0.0038692655519933714\n",
      "Epoch   5/50 Batch  124/251 - RMSE:  1.020, Seconds: 0.26\n",
      "Epoch   5/50 Batch  248/251 - RMSE:  0.946, Seconds: 0.25\n",
      "Testing RMSE:  1.003, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   6/50 Batch  124/251 - RMSE:  0.968, Seconds: 0.21\n",
      "Epoch   6/50 Batch  248/251 - RMSE:  0.966, Seconds: 0.25\n",
      "Testing RMSE:  0.988, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   7/50 Batch  124/251 - RMSE:  0.952, Seconds: 0.24\n",
      "Epoch   7/50 Batch  248/251 - RMSE:  1.000, Seconds: 0.25\n",
      "Testing RMSE:  0.995, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   8/50 Batch  124/251 - RMSE:  1.002, Seconds: 0.21\n",
      "Epoch   8/50 Batch  248/251 - RMSE:  0.949, Seconds: 0.18\n",
      "Testing RMSE:  0.996, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  0.0009938997645820681\n",
      "Epoch   9/50 Batch  124/251 - RMSE:  0.941, Seconds: 0.20\n",
      "Epoch   9/50 Batch  248/251 - RMSE:  0.974, Seconds: 0.17\n",
      "Testing RMSE:  0.993, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  10/50 Batch  124/251 - RMSE:  1.021, Seconds: 0.22\n",
      "Epoch  10/50 Batch  248/251 - RMSE:  0.944, Seconds: 0.23\n",
      "Testing RMSE:  0.987, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch  11/50 Batch  124/251 - RMSE:  0.976, Seconds: 0.23\n",
      "Epoch  11/50 Batch  248/251 - RMSE:  0.955, Seconds: 0.22\n",
      "Testing RMSE:  0.990, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch  12/50 Batch  124/251 - RMSE:  0.989, Seconds: 0.23\n",
      "Epoch  12/50 Batch  248/251 - RMSE:  0.952, Seconds: 0.21\n",
      "Testing RMSE:  0.989, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  0.00025530342354697675\n",
      "Epoch  13/50 Batch  124/251 - RMSE:  0.934, Seconds: 0.20\n",
      "Epoch  13/50 Batch  248/251 - RMSE:  0.947, Seconds: 0.23\n",
      "Testing RMSE:  0.990, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  14/50 Batch  124/251 - RMSE:  0.961, Seconds: 0.21\n",
      "Epoch  14/50 Batch  248/251 - RMSE:  0.969, Seconds: 0.21\n",
      "Testing RMSE:  0.992, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  6.557989084766e-05\n",
      "Epoch  15/50 Batch  124/251 - RMSE:  0.966, Seconds: 0.21\n",
      "Epoch  15/50 Batch  248/251 - RMSE:  0.963, Seconds: 0.22\n",
      "Testing RMSE:  0.992, Seconds: 0.04\n",
      "No Improvement.\n",
      "Stopping training for this iteration.\n",
      "Lowest RMSE = 0.98703215917\n",
      "\n",
      "Starting iteration # 5\n",
      "\n",
      "Training Model: LR=0.06819133807666959,LRD=0.3278610889061384,WM=1.4024908454493155,NI=28,NL=3,DR=0.26622604388351945,BS=1,MN=119,AF=tf.nn.elu\n",
      "Epoch   1/50 Batch  124/251 - RMSE: 3706.195, Seconds: 0.20\n",
      "Epoch   1/50 Batch  248/251 - RMSE:  2.125, Seconds: 0.20\n",
      "Testing RMSE:  1.348, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   2/50 Batch  124/251 - RMSE:  1.618, Seconds: 0.23\n",
      "Epoch   2/50 Batch  248/251 - RMSE:  1.649, Seconds: 0.20\n",
      "Testing RMSE:  1.121, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch   3/50 Batch  124/251 - RMSE:  1.654, Seconds: 0.19\n",
      "Epoch   3/50 Batch  248/251 - RMSE:  1.708, Seconds: 0.20\n",
      "Testing RMSE:  1.133, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   4/50 Batch  124/251 - RMSE:  1.691, Seconds: 0.21\n",
      "Epoch   4/50 Batch  248/251 - RMSE:  1.830, Seconds: 0.22\n",
      "Testing RMSE:  1.612, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  0.02235728635578351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/50 Batch  124/251 - RMSE:  1.570, Seconds: 0.23\n",
      "Epoch   5/50 Batch  248/251 - RMSE:  1.632, Seconds: 0.21\n",
      "Testing RMSE:  0.979, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   6/50 Batch  124/251 - RMSE:  1.495, Seconds: 0.18\n",
      "Epoch   6/50 Batch  248/251 - RMSE:  1.549, Seconds: 0.18\n",
      "Testing RMSE:  1.257, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   7/50 Batch  124/251 - RMSE:  1.436, Seconds: 0.20\n",
      "Epoch   7/50 Batch  248/251 - RMSE:  1.513, Seconds: 0.18\n",
      "Testing RMSE:  1.071, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  0.007330084249593533\n",
      "Epoch   8/50 Batch  124/251 - RMSE:  1.438, Seconds: 0.17\n",
      "Epoch   8/50 Batch  248/251 - RMSE:  1.370, Seconds: 0.16\n",
      "Testing RMSE:  1.147, Seconds: 0.03\n",
      "No Improvement.\n",
      "Epoch   9/50 Batch  124/251 - RMSE:  1.379, Seconds: 0.14\n",
      "Epoch   9/50 Batch  248/251 - RMSE:  1.415, Seconds: 0.16\n",
      "Testing RMSE:  0.984, Seconds: 0.05\n",
      "No Improvement.\n",
      "New learning rate =  0.00240324940384547\n",
      "Epoch  10/50 Batch  124/251 - RMSE:  1.317, Seconds: 0.20\n",
      "Epoch  10/50 Batch  248/251 - RMSE:  1.424, Seconds: 0.20\n",
      "Testing RMSE:  1.042, Seconds: 0.04\n",
      "No Improvement.\n",
      "Stopping training for this iteration.\n",
      "Lowest RMSE = 0.978620672226\n",
      "\n",
      "Starting iteration # 6\n",
      "\n",
      "Training Model: LR=0.01915636909221959,LRD=0.10223523037928564,WM=1.3739369502737473,NI=2,NL=3,DR=0.19613941049535535,BS=1,MN=182,AF=tf.nn.elu\n",
      "Epoch   1/50 Batch  124/251 - RMSE:  3.268, Seconds: 0.22\n",
      "Epoch   1/50 Batch  248/251 - RMSE:  1.014, Seconds: 0.23\n",
      "Testing RMSE:  0.979, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   2/50 Batch  124/251 - RMSE:  1.016, Seconds: 0.20\n",
      "Epoch   2/50 Batch  248/251 - RMSE:  0.980, Seconds: 0.20\n",
      "Testing RMSE:  0.983, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   3/50 Batch  124/251 - RMSE:  1.033, Seconds: 0.21\n",
      "Epoch   3/50 Batch  248/251 - RMSE:  0.991, Seconds: 0.20\n",
      "Testing RMSE:  1.003, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   4/50 Batch  124/251 - RMSE:  0.950, Seconds: 0.20\n",
      "Epoch   4/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.19\n",
      "Testing RMSE:  0.983, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  0.0019584558073736965\n",
      "Epoch   5/50 Batch  124/251 - RMSE:  0.953, Seconds: 0.21\n",
      "Epoch   5/50 Batch  248/251 - RMSE:  0.917, Seconds: 0.19\n",
      "Testing RMSE:  0.991, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   6/50 Batch  124/251 - RMSE:  0.969, Seconds: 0.19\n",
      "Epoch   6/50 Batch  248/251 - RMSE:  0.946, Seconds: 0.20\n",
      "Testing RMSE:  0.994, Seconds: 0.04\n",
      "No Improvement.\n",
      "Stopping training for this iteration.\n",
      "Lowest RMSE = 0.978795676761\n",
      "\n",
      "Starting iteration # 7\n",
      "\n",
      "Training Model: LR=0.03946575915390139,LRD=0.36186598774829426,WM=1.0462209675638026,NI=10,NL=3,DR=0.02483740925954663,BS=1,MN=45,AF=tf.nn.sigmoid\n",
      "Epoch   1/50 Batch  124/251 - RMSE:  1.090, Seconds: 0.27\n",
      "Epoch   1/50 Batch  248/251 - RMSE:  0.965, Seconds: 0.20\n",
      "Testing RMSE:  0.981, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   2/50 Batch  124/251 - RMSE:  0.996, Seconds: 0.20\n",
      "Epoch   2/50 Batch  248/251 - RMSE:  0.956, Seconds: 0.23\n",
      "Testing RMSE:  1.007, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch   3/50 Batch  124/251 - RMSE:  0.992, Seconds: 0.22\n",
      "Epoch   3/50 Batch  248/251 - RMSE:  0.954, Seconds: 0.22\n",
      "Testing RMSE:  0.992, Seconds: 0.06\n",
      "No Improvement.\n",
      "New learning rate =  0.014281315918462812\n",
      "Epoch   4/50 Batch  124/251 - RMSE:  0.980, Seconds: 0.21\n",
      "Epoch   4/50 Batch  248/251 - RMSE:  0.936, Seconds: 0.21\n",
      "Testing RMSE:  0.990, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   5/50 Batch  124/251 - RMSE:  0.979, Seconds: 0.20\n",
      "Epoch   5/50 Batch  248/251 - RMSE:  0.938, Seconds: 0.20\n",
      "Testing RMSE:  1.001, Seconds: 0.05\n",
      "No Improvement.\n",
      "New learning rate =  0.005167922491179984\n",
      "Epoch   6/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.21\n",
      "Epoch   6/50 Batch  248/251 - RMSE:  0.956, Seconds: 0.17\n",
      "Testing RMSE:  0.987, Seconds: 0.03\n",
      "No Improvement.\n",
      "Stopping training for this iteration.\n",
      "Lowest RMSE = 0.980950133006\n",
      "\n",
      "Starting iteration # 8\n",
      "\n",
      "Training Model: LR=0.00640148283224333,LRD=0.45368412412587145,WM=0.7629933082774356,NI=28,NL=3,DR=0.09718430479002073,BS=1,MN=349,AF=tf.nn.sigmoid\n",
      "Epoch   1/50 Batch  124/251 - RMSE:  1.167, Seconds: 0.21\n",
      "Epoch   1/50 Batch  248/251 - RMSE:  0.918, Seconds: 0.21\n",
      "Testing RMSE:  1.028, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch   2/50 Batch  124/251 - RMSE:  1.022, Seconds: 0.19\n",
      "Epoch   2/50 Batch  248/251 - RMSE:  0.931, Seconds: 0.19\n",
      "Testing RMSE:  0.977, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch   3/50 Batch  124/251 - RMSE:  1.038, Seconds: 0.18\n",
      "Epoch   3/50 Batch  248/251 - RMSE:  0.926, Seconds: 0.19\n",
      "Testing RMSE:  0.978, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   4/50 Batch  124/251 - RMSE:  0.971, Seconds: 0.21\n",
      "Epoch   4/50 Batch  248/251 - RMSE:  0.962, Seconds: 0.21\n",
      "Testing RMSE:  1.070, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   5/50 Batch  124/251 - RMSE:  0.985, Seconds: 0.22\n",
      "Epoch   5/50 Batch  248/251 - RMSE:  0.996, Seconds: 0.22\n",
      "Testing RMSE:  0.984, Seconds: 0.05\n",
      "No Improvement.\n",
      "New learning rate =  0.002904251131853118\n",
      "Epoch   6/50 Batch  124/251 - RMSE:  0.950, Seconds: 0.21\n",
      "Epoch   6/50 Batch  248/251 - RMSE:  0.930, Seconds: 0.21\n",
      "Testing RMSE:  0.978, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   7/50 Batch  124/251 - RMSE:  0.993, Seconds: 0.21\n",
      "Epoch   7/50 Batch  248/251 - RMSE:  0.949, Seconds: 0.21\n",
      "Testing RMSE:  0.974, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch   8/50 Batch  124/251 - RMSE:  0.998, Seconds: 0.22\n",
      "Epoch   8/50 Batch  248/251 - RMSE:  0.928, Seconds: 0.22\n",
      "Testing RMSE:  0.983, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch   9/50 Batch  124/251 - RMSE:  0.951, Seconds: 0.23\n",
      "Epoch   9/50 Batch  248/251 - RMSE:  0.927, Seconds: 0.28\n",
      "Testing RMSE:  1.000, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  10/50 Batch  124/251 - RMSE:  0.932, Seconds: 0.24\n",
      "Epoch  10/50 Batch  248/251 - RMSE:  0.929, Seconds: 0.33\n",
      "Testing RMSE:  0.989, Seconds: 0.12\n",
      "No Improvement.\n",
      "New learning rate =  0.0013176126309963525\n",
      "Epoch  11/50 Batch  124/251 - RMSE:  0.952, Seconds: 0.30\n",
      "Epoch  11/50 Batch  248/251 - RMSE:  0.939, Seconds: 0.29\n",
      "Testing RMSE:  0.995, Seconds: 0.07\n",
      "No Improvement.\n",
      "Epoch  12/50 Batch  124/251 - RMSE:  0.985, Seconds: 0.28\n",
      "Epoch  12/50 Batch  248/251 - RMSE:  0.932, Seconds: 0.29\n",
      "Testing RMSE:  1.005, Seconds: 0.06\n",
      "No Improvement.\n",
      "Stopping training for this iteration.\n",
      "Lowest RMSE = 0.973854822583\n",
      "\n",
      "Starting iteration # 9\n",
      "\n",
      "Training Model: LR=0.04016957038343443,LRD=0.19451184187238768,WM=1.6995789616150978,NI=16,NL=3,DR=0.009372178008079723,BS=1,MN=207,AF=tf.nn.relu\n",
      "Epoch   1/50 Batch  124/251 - RMSE: 24.061, Seconds: 0.25\n",
      "Epoch   1/50 Batch  248/251 - RMSE:  0.932, Seconds: 0.23\n",
      "Testing RMSE:  1.009, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   2/50 Batch  124/251 - RMSE:  0.964, Seconds: 0.21\n",
      "Epoch   2/50 Batch  248/251 - RMSE:  0.935, Seconds: 0.24\n",
      "Testing RMSE:  0.990, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch   3/50 Batch  124/251 - RMSE:  0.955, Seconds: 0.24\n",
      "Epoch   3/50 Batch  248/251 - RMSE:  0.906, Seconds: 0.22\n",
      "Testing RMSE:  0.989, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   4/50 Batch  124/251 - RMSE:  0.960, Seconds: 0.24\n",
      "Epoch   4/50 Batch  248/251 - RMSE:  0.910, Seconds: 0.20\n",
      "Testing RMSE:  0.985, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch   5/50 Batch  124/251 - RMSE:  0.972, Seconds: 0.21\n",
      "Epoch   5/50 Batch  248/251 - RMSE:  0.919, Seconds: 0.20\n",
      "Testing RMSE:  0.985, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   6/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.20\n",
      "Epoch   6/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.19\n",
      "Testing RMSE:  0.993, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  0.007813457122504346\n",
      "Epoch   7/50 Batch  124/251 - RMSE:  0.941, Seconds: 0.23\n",
      "Epoch   7/50 Batch  248/251 - RMSE:  0.910, Seconds: 0.22\n",
      "Testing RMSE:  0.993, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   8/50 Batch  124/251 - RMSE:  0.940, Seconds: 0.23\n",
      "Epoch   8/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.21\n",
      "Testing RMSE:  0.990, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  0.0015198099362892467\n",
      "Epoch   9/50 Batch  124/251 - RMSE:  0.944, Seconds: 0.22\n",
      "Epoch   9/50 Batch  248/251 - RMSE:  0.889, Seconds: 0.22\n",
      "Testing RMSE:  0.996, Seconds: 0.04\n",
      "No Improvement.\n",
      "Stopping training for this iteration.\n",
      "Lowest RMSE = 0.984503163232\n",
      "\n",
      "Starting iteration # 10\n",
      "\n",
      "Training Model: LR=0.05798437620193302,LRD=0.21590874881741634,WM=0.9537378038964466,NI=14,NL=4,DR=0.019109766284003304,BS=1,MN=106,AF=tf.nn.sigmoid\n",
      "Epoch   1/50 Batch  124/251 - RMSE:  1.069, Seconds: 0.30\n",
      "Epoch   1/50 Batch  248/251 - RMSE:  1.010, Seconds: 0.28\n",
      "Testing RMSE:  1.110, Seconds: 0.07\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/50 Batch  124/251 - RMSE:  1.046, Seconds: 0.24\n",
      "Epoch   2/50 Batch  248/251 - RMSE:  0.986, Seconds: 0.25\n",
      "Testing RMSE:  1.081, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   3/50 Batch  124/251 - RMSE:  1.040, Seconds: 0.23\n",
      "Epoch   3/50 Batch  248/251 - RMSE:  0.966, Seconds: 0.24\n",
      "Testing RMSE:  0.981, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   4/50 Batch  124/251 - RMSE:  1.020, Seconds: 0.24\n",
      "Epoch   4/50 Batch  248/251 - RMSE:  0.969, Seconds: 0.27\n",
      "Testing RMSE:  1.057, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   5/50 Batch  124/251 - RMSE:  1.021, Seconds: 0.24\n",
      "Epoch   5/50 Batch  248/251 - RMSE:  0.998, Seconds: 0.23\n",
      "Testing RMSE:  1.036, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   6/50 Batch  124/251 - RMSE:  1.048, Seconds: 0.20\n",
      "Epoch   6/50 Batch  248/251 - RMSE:  0.985, Seconds: 0.22\n",
      "Testing RMSE:  0.990, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  0.012519334116717731\n",
      "Epoch   7/50 Batch  124/251 - RMSE:  0.978, Seconds: 0.28\n",
      "Epoch   7/50 Batch  248/251 - RMSE:  0.922, Seconds: 0.26\n",
      "Testing RMSE:  0.995, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   8/50 Batch  124/251 - RMSE:  0.968, Seconds: 0.24\n",
      "Epoch   8/50 Batch  248/251 - RMSE:  0.938, Seconds: 0.26\n",
      "Testing RMSE:  0.994, Seconds: 0.05\n",
      "No Improvement.\n",
      "Stopping training for this iteration.\n",
      "Lowest RMSE = 0.981187068091\n",
      "\n",
      "Starting iteration # 11\n",
      "\n",
      "Training Model: LR=0.09477530132231682,LRD=0.3294169276978847,WM=1.1662542213002944,NI=37,NL=4,DR=0.2406740301241253,BS=1,MN=184,AF=tf.nn.relu\n",
      "Epoch   1/50 Batch  124/251 - RMSE: 63771.285, Seconds: 0.25\n",
      "Epoch   1/50 Batch  248/251 - RMSE: 227420.911, Seconds: 0.23\n",
      "Testing RMSE: 214.795, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   2/50 Batch  124/251 - RMSE: 412368.299, Seconds: 0.26\n",
      "Epoch   2/50 Batch  248/251 - RMSE: 126101.091, Seconds: 0.23\n",
      "Testing RMSE: 3364.992, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch   3/50 Batch  124/251 - RMSE: 9599379.485, Seconds: 0.25\n",
      "Epoch   3/50 Batch  248/251 - RMSE: 44.772, Seconds: 0.23\n",
      "Testing RMSE: 108.692, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   4/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.23\n",
      "Epoch   4/50 Batch  248/251 - RMSE: 561509.842, Seconds: 0.23\n",
      "Testing RMSE:  0.986, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch   5/50 Batch  124/251 - RMSE: 144652.447, Seconds: 0.24\n",
      "Epoch   5/50 Batch  248/251 - RMSE:  0.918, Seconds: 0.23\n",
      "Testing RMSE:  0.997, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   6/50 Batch  124/251 - RMSE: 239891.189, Seconds: 0.24\n",
      "Epoch   6/50 Batch  248/251 - RMSE:  0.920, Seconds: 0.25\n",
      "Testing RMSE:  0.986, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   7/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.24\n",
      "Epoch   7/50 Batch  248/251 - RMSE:  0.918, Seconds: 0.25\n",
      "Testing RMSE:  0.997, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch   8/50 Batch  124/251 - RMSE:  0.955, Seconds: 0.25\n",
      "Epoch   8/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.24\n",
      "Testing RMSE:  1.005, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch   9/50 Batch  124/251 - RMSE:  0.958, Seconds: 0.22\n",
      "Epoch   9/50 Batch  248/251 - RMSE:  0.918, Seconds: 0.22\n",
      "Testing RMSE:  0.997, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  0.031220588583238878\n",
      "Epoch  10/50 Batch  124/251 - RMSE: 80420.648, Seconds: 0.20\n",
      "Epoch  10/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.21\n",
      "Testing RMSE:  0.984, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  11/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.21\n",
      "Epoch  11/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.22\n",
      "Testing RMSE:  0.988, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  12/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.22\n",
      "Epoch  12/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.22\n",
      "Testing RMSE:  0.984, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch  13/50 Batch  124/251 - RMSE: 377998.844, Seconds: 0.23\n",
      "Epoch  13/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.24\n",
      "Testing RMSE:  0.988, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  14/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.21\n",
      "Epoch  14/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.22\n",
      "Testing RMSE:  0.984, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch  15/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.22\n",
      "Epoch  15/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.24\n",
      "Testing RMSE:  0.988, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch  16/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.25\n",
      "Epoch  16/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.24\n",
      "Testing RMSE:  0.984, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  17/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.24\n",
      "Epoch  17/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.23\n",
      "Testing RMSE:  0.988, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  18/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.23\n",
      "Epoch  18/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.24\n",
      "Testing RMSE:  0.984, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch  19/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.23\n",
      "Epoch  19/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.25\n",
      "Testing RMSE:  0.988, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  20/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.23\n",
      "Epoch  20/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.24\n",
      "Testing RMSE:  0.984, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  21/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.22\n",
      "Epoch  21/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.22\n",
      "Testing RMSE:  0.988, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch  22/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.23\n",
      "Epoch  22/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.21\n",
      "Testing RMSE:  0.984, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch  23/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.18\n",
      "Epoch  23/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.16\n",
      "Testing RMSE:  0.988, Seconds: 0.03\n",
      "No Improvement.\n",
      "Epoch  24/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.16\n",
      "Epoch  24/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.29\n",
      "Testing RMSE:  0.984, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  25/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.28\n",
      "Epoch  25/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.22\n",
      "Testing RMSE:  0.988, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  26/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.25\n",
      "Epoch  26/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.28\n",
      "Testing RMSE:  0.984, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  27/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.24\n",
      "Epoch  27/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.24\n",
      "Testing RMSE:  0.988, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch  28/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.25\n",
      "Epoch  28/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.23\n",
      "Testing RMSE:  0.984, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  29/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.27\n",
      "Epoch  29/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.26\n",
      "Testing RMSE:  0.988, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch  30/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.30\n",
      "Epoch  30/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.23\n",
      "Testing RMSE:  0.984, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  31/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.25\n",
      "Epoch  31/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.23\n",
      "Testing RMSE:  0.988, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch  32/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.24\n",
      "Epoch  32/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.27\n",
      "Testing RMSE:  0.984, Seconds: 0.07\n",
      "New Record!\n",
      "Epoch  33/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.24\n",
      "Epoch  33/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.24\n",
      "Testing RMSE:  0.988, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch  34/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.24\n",
      "Epoch  34/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.27\n",
      "Testing RMSE:  0.984, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  35/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.31\n",
      "Epoch  35/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.24\n",
      "Testing RMSE:  0.988, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch  36/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.22\n",
      "Epoch  36/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.25\n",
      "Testing RMSE:  0.984, Seconds: 0.06\n",
      "New Record!\n",
      "Epoch  37/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.21\n",
      "Epoch  37/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.24\n",
      "Testing RMSE:  0.988, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  38/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.23\n",
      "Epoch  38/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.23\n",
      "Testing RMSE:  0.984, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch  39/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.23\n",
      "Epoch  39/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.25\n",
      "Testing RMSE:  0.988, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  40/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.23\n",
      "Epoch  40/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.25\n",
      "Testing RMSE:  0.984, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  41/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  41/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.27\n",
      "Testing RMSE:  0.988, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch  42/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.25\n",
      "Epoch  42/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.23\n",
      "Testing RMSE:  0.984, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  43/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.31\n",
      "Epoch  43/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.27\n",
      "Testing RMSE:  0.988, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  44/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.24\n",
      "Epoch  44/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.25\n",
      "Testing RMSE:  0.984, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  45/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.28\n",
      "Epoch  45/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.27\n",
      "Testing RMSE:  0.988, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch  46/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.23\n",
      "Epoch  46/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.23\n",
      "Testing RMSE:  0.984, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch  47/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.26\n",
      "Epoch  47/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.31\n",
      "Testing RMSE:  0.988, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch  48/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.24\n",
      "Epoch  48/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.23\n",
      "Testing RMSE:  0.984, Seconds: 0.06\n",
      "New Record!\n",
      "Epoch  49/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.20\n",
      "Epoch  49/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.22\n",
      "Testing RMSE:  0.988, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  50/50 Batch  124/251 - RMSE:  0.942, Seconds: 0.23\n",
      "Epoch  50/50 Batch  248/251 - RMSE:  0.908, Seconds: 0.21\n",
      "Testing RMSE:  0.984, Seconds: 0.04\n",
      "New Record!\n",
      "Starting iteration # 12\n",
      "\n",
      "Training Model: LR=0.048580724041722215,LRD=0.3110637806785682,WM=1.0857049813381046,NI=17,NL=3,DR=0.15199575197889453,BS=1,MN=108,AF=tf.nn.elu\n",
      "Epoch   1/50 Batch  124/251 - RMSE: 319.682, Seconds: 0.31\n",
      "Epoch   1/50 Batch  248/251 - RMSE:  1.354, Seconds: 0.21\n",
      "Testing RMSE:  1.125, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   2/50 Batch  124/251 - RMSE:  1.326, Seconds: 0.21\n",
      "Epoch   2/50 Batch  248/251 - RMSE:  1.248, Seconds: 0.22\n",
      "Testing RMSE:  0.999, Seconds: 0.06\n",
      "New Record!\n",
      "Epoch   3/50 Batch  124/251 - RMSE:  1.179, Seconds: 0.21\n",
      "Epoch   3/50 Batch  248/251 - RMSE:  1.161, Seconds: 0.21\n",
      "Testing RMSE:  0.996, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   4/50 Batch  124/251 - RMSE:  1.146, Seconds: 0.24\n",
      "Epoch   4/50 Batch  248/251 - RMSE:  1.113, Seconds: 0.23\n",
      "Testing RMSE:  1.157, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch   5/50 Batch  124/251 - RMSE:  1.174, Seconds: 0.25\n",
      "Epoch   5/50 Batch  248/251 - RMSE:  1.095, Seconds: 0.21\n",
      "Testing RMSE:  0.979, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   6/50 Batch  124/251 - RMSE:  1.157, Seconds: 0.23\n",
      "Epoch   6/50 Batch  248/251 - RMSE:  1.077, Seconds: 0.26\n",
      "Testing RMSE:  0.984, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   7/50 Batch  124/251 - RMSE:  1.083, Seconds: 0.21\n",
      "Epoch   7/50 Batch  248/251 - RMSE:  1.024, Seconds: 0.21\n",
      "Testing RMSE:  0.994, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch   8/50 Batch  124/251 - RMSE:  1.042, Seconds: 0.22\n",
      "Epoch   8/50 Batch  248/251 - RMSE:  1.077, Seconds: 0.21\n",
      "Testing RMSE:  1.007, Seconds: 0.05\n",
      "No Improvement.\n",
      "New learning rate =  0.015111703688520325\n",
      "Epoch   9/50 Batch  124/251 - RMSE:  1.057, Seconds: 0.23\n",
      "Epoch   9/50 Batch  248/251 - RMSE:  1.031, Seconds: 0.21\n",
      "Testing RMSE:  1.006, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  10/50 Batch  124/251 - RMSE:  1.024, Seconds: 0.23\n",
      "Epoch  10/50 Batch  248/251 - RMSE:  0.995, Seconds: 0.21\n",
      "Testing RMSE:  1.000, Seconds: 0.04\n",
      "No Improvement.\n",
      "Stopping training for this iteration.\n",
      "Lowest RMSE = 0.97890218099\n",
      "\n",
      "Starting iteration # 13\n",
      "\n",
      "Training Model: LR=0.020393679714098056,LRD=0.22437839834733975,WM=1.5912560024936644,NI=6,NL=4,DR=0.2962092110785412,BS=1,MN=132,AF=tf.nn.relu\n",
      "Epoch   1/50 Batch  124/251 - RMSE: 47.009, Seconds: 0.31\n",
      "Epoch   1/50 Batch  248/251 - RMSE: 98.037, Seconds: 0.26\n",
      "Testing RMSE:  4.084, Seconds: 0.06\n",
      "New Record!\n",
      "Epoch   2/50 Batch  124/251 - RMSE: 836.956, Seconds: 0.22\n",
      "Epoch   2/50 Batch  248/251 - RMSE: 252.465, Seconds: 0.23\n",
      "Testing RMSE:  0.986, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch   3/50 Batch  124/251 - RMSE:  0.960, Seconds: 0.21\n",
      "Epoch   3/50 Batch  248/251 - RMSE:  0.919, Seconds: 0.23\n",
      "Testing RMSE:  0.991, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch   4/50 Batch  124/251 - RMSE:  0.953, Seconds: 0.24\n",
      "Epoch   4/50 Batch  248/251 - RMSE:  0.935, Seconds: 0.26\n",
      "Testing RMSE:  0.991, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch   5/50 Batch  124/251 - RMSE:  0.970, Seconds: 0.25\n",
      "Epoch   5/50 Batch  248/251 - RMSE:  0.910, Seconds: 0.26\n",
      "Testing RMSE:  0.990, Seconds: 0.04\n",
      "No Improvement.\n",
      "New learning rate =  0.004575901190657956\n",
      "Epoch   6/50 Batch  124/251 - RMSE:  0.959, Seconds: 0.24\n",
      "Epoch   6/50 Batch  248/251 - RMSE:  0.910, Seconds: 0.25\n",
      "Testing RMSE:  0.986, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch   7/50 Batch  124/251 - RMSE:  0.937, Seconds: 0.26\n",
      "Epoch   7/50 Batch  248/251 - RMSE:  0.926, Seconds: 0.25\n",
      "Testing RMSE:  0.992, Seconds: 0.07\n",
      "No Improvement.\n",
      "Stopping training for this iteration.\n",
      "Lowest RMSE = 0.985621441735\n",
      "\n",
      "Starting iteration # 14\n",
      "\n",
      "Training Model: LR=0.09203111539464011,LRD=0.3986725160672724,WM=1.8366274213672202,NI=39,NL=2,DR=0.0034009766360795666,BS=1,MN=305,AF=tf.nn.elu\n",
      "Epoch   1/50 Batch  124/251 - RMSE: 4928.600, Seconds: 0.21\n",
      "Epoch   1/50 Batch  248/251 - RMSE:  1.364, Seconds: 0.19\n",
      "Testing RMSE:  0.984, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   2/50 Batch  124/251 - RMSE:  1.191, Seconds: 0.20\n",
      "Epoch   2/50 Batch  248/251 - RMSE:  1.084, Seconds: 0.19\n",
      "Testing RMSE:  1.275, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   3/50 Batch  124/251 - RMSE:  1.297, Seconds: 0.19\n",
      "Epoch   3/50 Batch  248/251 - RMSE:  1.222, Seconds: 0.18\n",
      "Testing RMSE:  1.339, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   4/50 Batch  124/251 - RMSE:  1.451, Seconds: 0.19\n",
      "Epoch   4/50 Batch  248/251 - RMSE:  1.286, Seconds: 0.18\n",
      "Testing RMSE:  1.341, Seconds: 0.03\n",
      "No Improvement.\n",
      "New learning rate =  0.03669027633085866\n",
      "Epoch   5/50 Batch  124/251 - RMSE:  1.204, Seconds: 0.18\n",
      "Epoch   5/50 Batch  248/251 - RMSE:  0.995, Seconds: 0.18\n",
      "Testing RMSE:  1.042, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   6/50 Batch  124/251 - RMSE:  1.181, Seconds: 0.17\n",
      "Epoch   6/50 Batch  248/251 - RMSE:  1.137, Seconds: 0.18\n",
      "Testing RMSE:  1.105, Seconds: 0.03\n",
      "No Improvement.\n",
      "Stopping training for this iteration.\n",
      "Lowest RMSE = 0.983579243554\n",
      "\n",
      "Starting iteration # 15\n",
      "\n",
      "Training Model: LR=0.09612001883294748,LRD=0.4689085754934492,WM=1.3575565628355883,NI=34,NL=2,DR=0.15668312884794353,BS=1,MN=323,AF=tf.nn.relu\n",
      "Epoch   1/50 Batch  124/251 - RMSE: 6895.599, Seconds: 0.19\n",
      "Epoch   1/50 Batch  248/251 - RMSE: 18042.729, Seconds: 0.17\n",
      "Testing RMSE:  1.011, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   2/50 Batch  124/251 - RMSE: 14.731, Seconds: 0.20\n",
      "Epoch   2/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.21\n",
      "Testing RMSE:  1.001, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch   3/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.18\n",
      "Epoch   3/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.20\n",
      "Testing RMSE:  0.992, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch   4/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.20\n",
      "Epoch   4/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.24\n",
      "Testing RMSE:  1.001, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch   5/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.18\n",
      "Epoch   5/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.19\n",
      "Testing RMSE:  0.992, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch   6/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.20\n",
      "Epoch   6/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.18\n",
      "Testing RMSE:  1.001, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch   7/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.18\n",
      "Epoch   7/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.17\n",
      "Testing RMSE:  0.992, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch   8/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.17\n",
      "Epoch   8/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.18\n",
      "Testing RMSE:  1.001, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch   9/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.20\n",
      "Epoch   9/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.18\n",
      "Testing RMSE:  0.992, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch  10/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.17\n",
      "Epoch  10/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.17\n",
      "Testing RMSE:  1.001, Seconds: 0.03\n",
      "No Improvement.\n",
      "Epoch  11/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.18\n",
      "Epoch  11/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.17\n",
      "Testing RMSE:  0.992, Seconds: 0.03\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  12/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.18\n",
      "Epoch  12/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.18\n",
      "Testing RMSE:  1.001, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch  13/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.18\n",
      "Epoch  13/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.17\n",
      "Testing RMSE:  0.992, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch  14/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.21\n",
      "Epoch  14/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.21\n",
      "Testing RMSE:  1.001, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  15/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.18\n",
      "Epoch  15/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.18\n",
      "Testing RMSE:  0.992, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch  16/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.18\n",
      "Epoch  16/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.18\n",
      "Testing RMSE:  1.001, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  17/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.18\n",
      "Epoch  17/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.18\n",
      "Testing RMSE:  0.992, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  18/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.20\n",
      "Epoch  18/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.18\n",
      "Testing RMSE:  1.001, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  19/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.19\n",
      "Epoch  19/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.19\n",
      "Testing RMSE:  0.992, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch  20/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.22\n",
      "Epoch  20/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.22\n",
      "Testing RMSE:  1.001, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  21/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.20\n",
      "Epoch  21/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.22\n",
      "Testing RMSE:  0.992, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch  22/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.24\n",
      "Epoch  22/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.26\n",
      "Testing RMSE:  1.001, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  23/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.20\n",
      "Epoch  23/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.22\n",
      "Testing RMSE:  0.992, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  24/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.20\n",
      "Epoch  24/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.22\n",
      "Testing RMSE:  1.001, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  25/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.20\n",
      "Epoch  25/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.21\n",
      "Testing RMSE:  0.992, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  26/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.21\n",
      "Epoch  26/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.20\n",
      "Testing RMSE:  1.001, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  27/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.22\n",
      "Epoch  27/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.23\n",
      "Testing RMSE:  0.992, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  28/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.22\n",
      "Epoch  28/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.30\n",
      "Testing RMSE:  1.001, Seconds: 0.06\n",
      "No Improvement.\n",
      "Epoch  29/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.22\n",
      "Epoch  29/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.22\n",
      "Testing RMSE:  0.992, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  30/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.20\n",
      "Epoch  30/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.23\n",
      "Testing RMSE:  1.001, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch  31/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.23\n",
      "Epoch  31/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.27\n",
      "Testing RMSE:  0.992, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  32/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.20\n",
      "Epoch  32/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.20\n",
      "Testing RMSE:  1.001, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  33/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.20\n",
      "Epoch  33/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.22\n",
      "Testing RMSE:  0.992, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  34/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.19\n",
      "Epoch  34/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.20\n",
      "Testing RMSE:  1.001, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch  35/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.24\n",
      "Epoch  35/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.19\n",
      "Testing RMSE:  0.992, Seconds: 0.05\n",
      "New Record!\n",
      "Epoch  36/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.20\n",
      "Epoch  36/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.27\n",
      "Testing RMSE:  1.001, Seconds: 0.06\n",
      "No Improvement.\n",
      "Epoch  37/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.22\n",
      "Epoch  37/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.21\n",
      "Testing RMSE:  0.992, Seconds: 0.06\n",
      "New Record!\n",
      "Epoch  38/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.22\n",
      "Epoch  38/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.19\n",
      "Testing RMSE:  1.001, Seconds: 0.05\n",
      "No Improvement.\n",
      "Epoch  39/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.21\n",
      "Epoch  39/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.28\n",
      "Testing RMSE:  0.992, Seconds: 0.08\n",
      "New Record!\n",
      "Epoch  40/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.19\n",
      "Epoch  40/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.23\n",
      "Testing RMSE:  1.001, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  41/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.20\n",
      "Epoch  41/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.19\n",
      "Testing RMSE:  0.992, Seconds: 0.03\n",
      "New Record!\n",
      "Epoch  42/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.31\n",
      "Epoch  42/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.19\n",
      "Testing RMSE:  1.001, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  43/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.16\n",
      "Epoch  43/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.19\n",
      "Testing RMSE:  0.992, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch  44/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.20\n",
      "Epoch  44/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.21\n",
      "Testing RMSE:  1.001, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  45/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.19\n",
      "Epoch  45/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.18\n",
      "Testing RMSE:  0.992, Seconds: 0.04\n",
      "New Record!\n",
      "Epoch  46/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.16\n",
      "Epoch  46/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.18\n",
      "Testing RMSE:  1.001, Seconds: 0.03\n",
      "No Improvement.\n",
      "Epoch  47/50 Batch  124/251 - RMSE:  0.962, Seconds: 0.18\n",
      "Epoch  47/50 Batch  248/251 - RMSE:  0.914, Seconds: 0.21\n",
      "Testing RMSE:  0.992, Seconds: 0.04\n",
      "No Improvement.\n",
      "Epoch  48/50 Batch  124/251 - RMSE:  0.956, Seconds: 0.18\n",
      "Epoch  48/50 Batch  248/251 - RMSE:  0.916, Seconds: 0.22\n",
      "Testing RMSE:  1.001, Seconds: 0.05\n",
      "No Improvement.\n",
      "New learning rate =  0.04507150110736091\n",
      "Epoch  49/50 Batch  124/251 - RMSE:  0.943, Seconds: 0.18\n",
      "Epoch  49/50 Batch  248/251 - RMSE:  0.905, Seconds: 0.19\n",
      "Testing RMSE:  0.993, Seconds: 0.06\n",
      "No Improvement.\n",
      "Epoch  50/50 Batch  124/251 - RMSE:  0.945, Seconds: 0.18\n",
      "Epoch  50/50 Batch  248/251 - RMSE:  0.909, Seconds: 0.16\n",
      "Testing RMSE:  0.988, Seconds: 0.03\n",
      "New Record!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "\n",
    "from collections import namedtuple\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "\n",
    "import time\n",
    "import operator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "x_trainFinal, x_testFinal, y_trainFinal, y_testFinal = train_test_split(input_variables.iloc[:,0:22], price, test_size=0.2)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_trainFinal, \n",
    "                                                    y_trainFinal, \n",
    "                                                    test_size=0.15,\n",
    "                                                    random_state=2)\n",
    "\n",
    "# Use random search to choose the values for each iteration\n",
    "\n",
    "num_iterations = 15\n",
    "results = {} # Save the log_string and RMSE of each iteration\n",
    "for i in range(num_iterations):\n",
    "    # (Randomly) choose the value for each input\n",
    "    num_features = x_train.shape[1]\n",
    "    epochs = 50\n",
    "    learning_rate = np.random.uniform(0.001, 0.1)\n",
    "    learning_rate_decay = np.random.uniform(0.1,0.5)\n",
    "    weights_multiplier = np.random.uniform(0.5,2)\n",
    "    n_inputs = np.random.randint(int(num_features)*0.1,int(num_features)*2)\n",
    "    num_layers = np.random.choice([2,3,4])\n",
    "    dropout_rate = np.random.uniform(0,0.3)\n",
    "    batch_size = np.random.choice([1])\n",
    "    max_nodes = np.random.randint(16, 512)\n",
    "    activation_function = np.random.choice(['tf.nn.sigmoid',\n",
    "                                            'tf.nn.relu',\n",
    "                                            'tf.nn.elu'])\n",
    "\n",
    "    print(\"Starting iteration #\",i+1)\n",
    "    log_string = 'LR={},LRD={},WM={},NI={},NL={},DR={},BS={},MN={},AF={}'.format(learning_rate,\n",
    "                                                                                 learning_rate_decay,\n",
    "                                                                                 weights_multiplier,\n",
    "                                                                                 n_inputs,\n",
    "                                                                                 num_layers,\n",
    "                                                                                 dropout_rate,\n",
    "                                                                                 batch_size,\n",
    "                                                                                 max_nodes,\n",
    "                                                                                 activation_function) \n",
    "    \n",
    "    model = build_graph(num_layers, n_inputs, weights_multiplier, \n",
    "                        dropout_rate,learning_rate,max_nodes,activation_function)\n",
    "    result = train(model, epochs, log_string, learning_rate)\n",
    "    results[log_string] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_inputs(model):\n",
    "    '''Use the log_string from the model to extract the values for all of the model's inputs'''\n",
    "    \n",
    "    learning_rate_start = model.find('LR=') + 3\n",
    "    learning_rate_end = model.find(',LRD', learning_rate_start)\n",
    "    learning_rate = float(model[learning_rate_start:learning_rate_end])\n",
    "    \n",
    "    learning_rate_decay_start = model.find('LRD=') + 4\n",
    "    learning_rate_decay_end = model.find(',WM', learning_rate_decay_start)\n",
    "    learning_rate_decay = float(model[learning_rate_decay_start:learning_rate_decay_end])\n",
    "    \n",
    "    weights_multiplier_start = model.find('WM=') + 3\n",
    "    weights_multiplier_end = model.find(',NI', weights_multiplier_start)\n",
    "    weights_multiplier = float(model[weights_multiplier_start:weights_multiplier_end])\n",
    "    \n",
    "    n_inputs_start = model.find('NI=') + 3\n",
    "    n_inputs_end = model.find(',NL', n_inputs_start)\n",
    "    n_inputs = int(model[n_inputs_start:n_inputs_end])\n",
    "    \n",
    "    num_layers_start = model.find('NL=') + 3\n",
    "    num_layers_end = model.find(',DR', num_layers_start)\n",
    "    num_layers = int(model[num_layers_start:num_layers_end])\n",
    "    \n",
    "    dropout_rate_start = model.find('DR=') + 3\n",
    "    dropout_rate_end = model.find(',BS', dropout_rate_start)\n",
    "    dropout_rate = float(model[dropout_rate_start:dropout_rate_end])\n",
    "    \n",
    "    batch_size_start = model.find('BS=') + 3\n",
    "    batch_size_end = model.find(',MN', batch_size_start)\n",
    "    batch_size = int(model[batch_size_start:batch_size_end])\n",
    "    \n",
    "    max_nodes_start = model.find('MN=') + 3\n",
    "    max_nodes_end = model.find(',AF', max_nodes_start)\n",
    "    max_nodes = int(model[max_nodes_start:max_nodes_end])\n",
    "    \n",
    "    activation_function_start = model.find('AF=') + 3\n",
    "    activation_function = str(model[activation_function_start:])\n",
    "    \n",
    "    return (learning_rate, learning_rate_decay, weights_multiplier, n_inputs,\n",
    "            num_layers, dropout_rate, batch_size, max_nodes, activation_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort results by RMSE (lowest - highest)\n",
    "sorted_results_nn = sorted(results.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an empty dataframe to contain all of the inputs for each iteration of the model\n",
    "results_nn = pd.DataFrame(columns=[\"learning_rate\", \n",
    "                                   \"learning_rate_decay\", \n",
    "                                   \"weights_multiplier\", \n",
    "                                   \"n_inputs\",\n",
    "                                   \"num_layers\", \n",
    "                                   \"dropout_rate\", \n",
    "                                   \"batch_size\", \n",
    "                                   \"max_nodes\", \n",
    "                                   \"activation_function\"])\n",
    "\n",
    "for result in sorted_results_nn:\n",
    "    # Find the input values for each iteration\n",
    "    learning_rate, learning_rate_decay, weights_multiplier, n_inputs,\\\n",
    "        num_layers, dropout_rate, batch_size, max_nodes, activation_function = find_inputs(result[0])\n",
    "    \n",
    "    # Find the Mean Squared Error for each iteration\n",
    "    RMSE = result[1]\n",
    "    \n",
    "    # Create a dataframe with the values above\n",
    "    new_row = pd.DataFrame([[RMSE,\n",
    "                             learning_rate, \n",
    "                             learning_rate_decay, \n",
    "                             weights_multiplier, \n",
    "                             n_inputs,\n",
    "                             num_layers, \n",
    "                             dropout_rate, \n",
    "                             batch_size, \n",
    "                             max_nodes, \n",
    "                             activation_function]],\n",
    "                     columns = [\"RMSE\",\n",
    "                                \"learning_rate\", \n",
    "                                \"learning_rate_decay\", \n",
    "                                \"weights_multiplier\", \n",
    "                                \"n_inputs\",\n",
    "                                \"num_layers\", \n",
    "                                \"dropout_rate\", \n",
    "                                \"batch_size\", \n",
    "                                \"max_nodes\", \n",
    "                                \"activation_function\"])\n",
    "    \n",
    "    # Append the dataframe as a new row in results_df\n",
    "    results_nn = results_nn.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>activation_function</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>learning_rate_decay</th>\n",
       "      <th>max_nodes</th>\n",
       "      <th>n_inputs</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>weights_multiplier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.973855</td>\n",
       "      <td>tf.nn.sigmoid</td>\n",
       "      <td>1</td>\n",
       "      <td>0.097184</td>\n",
       "      <td>0.006401</td>\n",
       "      <td>0.453684</td>\n",
       "      <td>349</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>0.762993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.978171</td>\n",
       "      <td>tf.nn.relu</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103354</td>\n",
       "      <td>0.036203</td>\n",
       "      <td>0.367173</td>\n",
       "      <td>87</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.984975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.978621</td>\n",
       "      <td>tf.nn.elu</td>\n",
       "      <td>1</td>\n",
       "      <td>0.266226</td>\n",
       "      <td>0.068191</td>\n",
       "      <td>0.327861</td>\n",
       "      <td>119</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>1.402491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.978796</td>\n",
       "      <td>tf.nn.elu</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196139</td>\n",
       "      <td>0.019156</td>\n",
       "      <td>0.102235</td>\n",
       "      <td>182</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.373937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.978902</td>\n",
       "      <td>tf.nn.elu</td>\n",
       "      <td>1</td>\n",
       "      <td>0.151996</td>\n",
       "      <td>0.048581</td>\n",
       "      <td>0.311064</td>\n",
       "      <td>108</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>1.085705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       RMSE activation_function batch_size  dropout_rate  learning_rate  \\\n",
       "0  0.973855       tf.nn.sigmoid          1      0.097184       0.006401   \n",
       "1  0.978171          tf.nn.relu          1      0.103354       0.036203   \n",
       "2  0.978621           tf.nn.elu          1      0.266226       0.068191   \n",
       "3  0.978796           tf.nn.elu          1      0.196139       0.019156   \n",
       "4  0.978902           tf.nn.elu          1      0.151996       0.048581   \n",
       "\n",
       "   learning_rate_decay max_nodes n_inputs num_layers  weights_multiplier  \n",
       "0             0.453684       349       28          3            0.762993  \n",
       "1             0.367173        87       12          3            0.984975  \n",
       "2             0.327861       119       28          3            1.402491  \n",
       "3             0.102235       182        2          3            1.373937  \n",
       "4             0.311064       108       17          3            1.085705  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the top five iterations\n",
    "results_nn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(data, batch_size):\n",
    "    '''\n",
    "    Restore a session to make predictions, then return these predictions\n",
    "    data: the data that will be used to make predictions.\n",
    "    '''\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, checkpoint)\n",
    "        predictions = [] # record the predictions\n",
    "\n",
    "        for batch in range(int(len(data)/batch_size)):\n",
    "            batch_x = data[batch*batch_size:(1+batch)*batch_size]\n",
    "\n",
    "            batch_predictions = sess.run([model.preds],\n",
    "                                   {model.inputs: batch_x,\n",
    "                                    model.learning_rate: learning_rate,\n",
    "                                    model.dropout_rate: 0,\n",
    "                                    model.is_training: False})\n",
    "\n",
    "            for prediction in batch_predictions[0]:\n",
    "                predictions.append(prediction)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for iteration #1 is 1.655518918893339.\n",
      "\n",
      "RMSE for iteration #2 is 9170.691910104779.\n",
      "\n",
      "RMSE for iteration #3 is 1.6545363953769445.\n",
      "\n",
      "RMSE for iteration #4 is 1.6510809882936.\n",
      "\n",
      "RMSE for iteration #5 is 1.679403397302667.\n",
      "\n",
      "RMSE for iteration #6 is 1.6828090226890866.\n",
      "\n",
      "RMSE for iteration #7 is 1.6362018991867529.\n",
      "\n",
      "RMSE for iteration #8 is 1.635309829973743.\n",
      "\n",
      "RMSE for iteration #9 is 1.691081735617117.\n",
      "\n",
      "RMSE for iteration #10 is 1.626461718250581.\n",
      "\n",
      "RMSE for iteration #11 is 1.6240076248487874.\n",
      "\n",
      "RMSE for iteration #12 is 1.623045210391715.\n",
      "\n",
      "RMSE for iteration #13 is 1.6190129163750893.\n",
      "\n",
      "RMSE for iteration #14 is 1.6140011151396678.\n",
      "\n",
      "RMSE for iteration #15 is 1.610101523308531.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_preds = {} # stores the RMSE and predictions for x_testFinal\n",
    "final_preds = {} # store the predictions for testFinal, with x_testFinal's RMSE\n",
    "\n",
    "iteration = 1 \n",
    "\n",
    "for model, result in sorted_results_nn:\n",
    "    checkpoint = str(model) + \".ckpt\" \n",
    "    \n",
    "    # Aquire the inputs from the log_string\n",
    "    _, _, weights_multiplier, n_inputs, num_layers, _, _, max_nodes, activation_function = find_inputs(model)\n",
    "    \n",
    "    model = build_graph(num_layers,n_inputs,weights_multiplier,dropout_rate,\n",
    "                        learning_rate,max_nodes,activation_function)\n",
    "    \n",
    "    y_preds_nn = make_predictions(x_testFinal, 1)\n",
    "    RMSE_nn = np.sqrt(mean_squared_error(y_testFinal, y_preds_nn))\n",
    "    print(\"RMSE for iteration #{} is {}.\".format(iteration, RMSE_nn))\n",
    "    print()\n",
    "    initial_preds[RMSE_nn] = y_preds_nn\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "print(len(accuracy))\n",
    "for p in initial_preds.values():\n",
    "    acc = r2_score(y_testFinal,p)\n",
    "    if acc > -50:\n",
    "        accuracy.append(acc)\n",
    "print(len(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABm4AAANTCAYAAACn4uJWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120ed5898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import cm\n",
    "import random\n",
    "\n",
    "# Give each bar separate color\n",
    "color_code_vector = random.sample(range(1, 100), len(accuracy))\n",
    "reg_color_code_vector = map(lambda x: x/max(color_code_vector), color_code_vector)\n",
    "reg_color_code_vector = list(reg_color_code_vector)\n",
    "reg_color_code_vector\n",
    "colors = cm.hsv(reg_color_code_vector)\n",
    "\n",
    "# Expressing accuracy in percentage\n",
    "model_results_percentage = list(map(lambda x: x*100, accuracy))\n",
    "\n",
    "y_pos = np.arange(len(accuracy))\n",
    "\n",
    "plt.rcdefaults() # white background, gets rid of gray\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.bar(y_pos, model_results_percentage, align='center', alpha=0.5, width=0.2, color = colors)\n",
    "plt.xticks(y_pos, list(range(0,15)), fontsize = 15)\n",
    "plt.ylabel('Accuracy in perecentage', fontsize=20)\n",
    "plt.title('Accuracy: Linear Regression Models', fontsize=22, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "# Use RMSE instead of r2score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
