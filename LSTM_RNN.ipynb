{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/siddharthbhaduri/Desktop/Work/Fall-2017/Google-Project/Week9\n"
     ]
    }
   ],
   "source": [
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>TMP_HIGH</th>\n",
       "      <th>TMP_AVG</th>\n",
       "      <th>TMP_LOW</th>\n",
       "      <th>DP_HIGH</th>\n",
       "      <th>DP_AVG</th>\n",
       "      <th>DP_LOW</th>\n",
       "      <th>HUM_HIGH</th>\n",
       "      <th>...</th>\n",
       "      <th>VIS_HIGH</th>\n",
       "      <th>VIS_AVG</th>\n",
       "      <th>VIS_LOW</th>\n",
       "      <th>WIND_HIGH</th>\n",
       "      <th>WIND_AVG</th>\n",
       "      <th>PRECIP</th>\n",
       "      <th>NUMDAY_RAIN</th>\n",
       "      <th>NUM_DAYS_SNOW</th>\n",
       "      <th>NUM_DAYS_FOG</th>\n",
       "      <th>NUM_DAYS_THNDRSTRM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>168</td>\n",
       "      <td>2000</td>\n",
       "      <td>JAN</td>\n",
       "      <td>60.098523</td>\n",
       "      <td>24.919939</td>\n",
       "      <td>-5.750129</td>\n",
       "      <td>50.876803</td>\n",
       "      <td>18.079444</td>\n",
       "      <td>-19.152746</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.091680</td>\n",
       "      <td>8.231682</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>28.491682</td>\n",
       "      <td>9.674976</td>\n",
       "      <td>0.769395</td>\n",
       "      <td>4.527847</td>\n",
       "      <td>7.865567</td>\n",
       "      <td>3.129778</td>\n",
       "      <td>0.388589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>169</td>\n",
       "      <td>2000</td>\n",
       "      <td>FEB</td>\n",
       "      <td>53.769218</td>\n",
       "      <td>31.870521</td>\n",
       "      <td>5.378496</td>\n",
       "      <td>44.864486</td>\n",
       "      <td>25.930720</td>\n",
       "      <td>-0.570920</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.091680</td>\n",
       "      <td>7.687676</td>\n",
       "      <td>0.044134</td>\n",
       "      <td>28.925880</td>\n",
       "      <td>9.571381</td>\n",
       "      <td>287.022956</td>\n",
       "      <td>3.572751</td>\n",
       "      <td>4.803434</td>\n",
       "      <td>2.284139</td>\n",
       "      <td>0.385381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>170</td>\n",
       "      <td>2000</td>\n",
       "      <td>MAY</td>\n",
       "      <td>84.325735</td>\n",
       "      <td>62.360525</td>\n",
       "      <td>37.698853</td>\n",
       "      <td>68.870647</td>\n",
       "      <td>51.653895</td>\n",
       "      <td>25.975416</td>\n",
       "      <td>98.988685</td>\n",
       "      <td>...</td>\n",
       "      <td>11.091680</td>\n",
       "      <td>8.828890</td>\n",
       "      <td>0.570830</td>\n",
       "      <td>30.201699</td>\n",
       "      <td>9.129810</td>\n",
       "      <td>4.098408</td>\n",
       "      <td>10.428558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.610892</td>\n",
       "      <td>4.896486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>171</td>\n",
       "      <td>2000</td>\n",
       "      <td>JUN</td>\n",
       "      <td>88.622913</td>\n",
       "      <td>69.307943</td>\n",
       "      <td>52.228400</td>\n",
       "      <td>71.995094</td>\n",
       "      <td>60.166479</td>\n",
       "      <td>45.635832</td>\n",
       "      <td>99.068140</td>\n",
       "      <td>...</td>\n",
       "      <td>11.229015</td>\n",
       "      <td>9.513896</td>\n",
       "      <td>0.408795</td>\n",
       "      <td>27.953090</td>\n",
       "      <td>8.176592</td>\n",
       "      <td>3.466575</td>\n",
       "      <td>9.051719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.014093</td>\n",
       "      <td>4.340215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  YEAR MONTH   TMP_HIGH    TMP_AVG    TMP_LOW    DP_HIGH  \\\n",
       "168         168  2000   JAN  60.098523  24.919939  -5.750129  50.876803   \n",
       "169         169  2000   FEB  53.769218  31.870521   5.378496  44.864486   \n",
       "170         170  2000   MAY  84.325735  62.360525  37.698853  68.870647   \n",
       "171         171  2000   JUN  88.622913  69.307943  52.228400  71.995094   \n",
       "\n",
       "        DP_AVG     DP_LOW    HUM_HIGH         ...           VIS_HIGH  \\\n",
       "168  18.079444 -19.152746  100.000000         ...          11.091680   \n",
       "169  25.930720  -0.570920  100.000000         ...          11.091680   \n",
       "170  51.653895  25.975416   98.988685         ...          11.091680   \n",
       "171  60.166479  45.635832   99.068140         ...          11.229015   \n",
       "\n",
       "      VIS_AVG   VIS_LOW  WIND_HIGH  WIND_AVG      PRECIP  NUMDAY_RAIN  \\\n",
       "168  8.231682  0.061922  28.491682  9.674976    0.769395     4.527847   \n",
       "169  7.687676  0.044134  28.925880  9.571381  287.022956     3.572751   \n",
       "170  8.828890  0.570830  30.201699  9.129810    4.098408    10.428558   \n",
       "171  9.513896  0.408795  27.953090  8.176592    3.466575     9.051719   \n",
       "\n",
       "     NUM_DAYS_SNOW  NUM_DAYS_FOG  NUM_DAYS_THNDRSTRM  \n",
       "168       7.865567      3.129778            0.388589  \n",
       "169       4.803434      2.284139            0.385381  \n",
       "170       0.000000      1.610892            4.896486  \n",
       "171       0.000000      1.014093            4.340215  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Library Import\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# import the train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# List to store results\n",
    "# Stores the performance on test set.\n",
    "model_results = []\n",
    "model_name = []\n",
    "\n",
    "# In[413]:\n",
    "\n",
    "\n",
    "xf = pd.read_csv('input_model_weighted_average.csv')\n",
    "xf.head()\n",
    "xf.iloc[168:172]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>State</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>TMP_HIGH</th>\n",
       "      <th>TMP_AVG</th>\n",
       "      <th>TMP_LOW</th>\n",
       "      <th>DP_HIGH</th>\n",
       "      <th>DP_AVG</th>\n",
       "      <th>DP_LOW</th>\n",
       "      <th>...</th>\n",
       "      <th>VIS_LOW</th>\n",
       "      <th>WIND_HIGH</th>\n",
       "      <th>WIND_AVG</th>\n",
       "      <th>PRECIP</th>\n",
       "      <th>NUMDAY_RAIN</th>\n",
       "      <th>NUM_DAYS_SNOW</th>\n",
       "      <th>NUM_DAYS_FOG</th>\n",
       "      <th>NUM_DAYS_THNDRSTRM</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discounted_Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>JAN</td>\n",
       "      <td>55.953658</td>\n",
       "      <td>27.617217</td>\n",
       "      <td>-2.980925</td>\n",
       "      <td>42.880246</td>\n",
       "      <td>19.992262</td>\n",
       "      <td>-8.511984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186718</td>\n",
       "      <td>29.600197</td>\n",
       "      <td>11.077783</td>\n",
       "      <td>0.479003</td>\n",
       "      <td>2.544772</td>\n",
       "      <td>7.584738</td>\n",
       "      <td>9.707042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.349417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>FEB</td>\n",
       "      <td>53.838886</td>\n",
       "      <td>28.731112</td>\n",
       "      <td>0.353638</td>\n",
       "      <td>46.537659</td>\n",
       "      <td>23.958440</td>\n",
       "      <td>-4.555208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>24.679482</td>\n",
       "      <td>9.622850</td>\n",
       "      <td>1.722515</td>\n",
       "      <td>5.510271</td>\n",
       "      <td>9.231277</td>\n",
       "      <td>15.203328</td>\n",
       "      <td>0.658457</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.379410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>MAR</td>\n",
       "      <td>77.668458</td>\n",
       "      <td>41.937001</td>\n",
       "      <td>13.926317</td>\n",
       "      <td>51.205991</td>\n",
       "      <td>29.862688</td>\n",
       "      <td>2.748564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634101</td>\n",
       "      <td>36.374250</td>\n",
       "      <td>12.349509</td>\n",
       "      <td>15.116988</td>\n",
       "      <td>5.934169</td>\n",
       "      <td>3.868506</td>\n",
       "      <td>9.890731</td>\n",
       "      <td>1.055313</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.349417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>ARP</td>\n",
       "      <td>85.651529</td>\n",
       "      <td>52.949283</td>\n",
       "      <td>25.351909</td>\n",
       "      <td>60.274139</td>\n",
       "      <td>37.049373</td>\n",
       "      <td>11.479874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474198</td>\n",
       "      <td>28.918771</td>\n",
       "      <td>10.770362</td>\n",
       "      <td>17.947352</td>\n",
       "      <td>9.195066</td>\n",
       "      <td>1.960945</td>\n",
       "      <td>7.611098</td>\n",
       "      <td>2.058521</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.339420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>MAY</td>\n",
       "      <td>85.415668</td>\n",
       "      <td>62.338688</td>\n",
       "      <td>36.602102</td>\n",
       "      <td>61.467698</td>\n",
       "      <td>47.555928</td>\n",
       "      <td>27.795232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280723</td>\n",
       "      <td>33.072546</td>\n",
       "      <td>8.983943</td>\n",
       "      <td>15.212715</td>\n",
       "      <td>9.610446</td>\n",
       "      <td>0.327268</td>\n",
       "      <td>14.161837</td>\n",
       "      <td>4.663441</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.449393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>JUN</td>\n",
       "      <td>93.180721</td>\n",
       "      <td>71.263159</td>\n",
       "      <td>47.746898</td>\n",
       "      <td>67.021368</td>\n",
       "      <td>55.502022</td>\n",
       "      <td>33.450048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950015</td>\n",
       "      <td>25.666101</td>\n",
       "      <td>8.317023</td>\n",
       "      <td>10.598875</td>\n",
       "      <td>8.135348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.649299</td>\n",
       "      <td>4.995009</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.449393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>JUL</td>\n",
       "      <td>93.651100</td>\n",
       "      <td>75.325134</td>\n",
       "      <td>57.224505</td>\n",
       "      <td>71.740380</td>\n",
       "      <td>60.440695</td>\n",
       "      <td>41.882198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360831</td>\n",
       "      <td>30.924584</td>\n",
       "      <td>7.668664</td>\n",
       "      <td>11.201697</td>\n",
       "      <td>9.899297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.561325</td>\n",
       "      <td>6.286502</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.039494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>AUG</td>\n",
       "      <td>88.032940</td>\n",
       "      <td>68.355756</td>\n",
       "      <td>43.240501</td>\n",
       "      <td>70.470204</td>\n",
       "      <td>53.535626</td>\n",
       "      <td>34.387899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123844</td>\n",
       "      <td>23.558382</td>\n",
       "      <td>6.839216</td>\n",
       "      <td>1.162475</td>\n",
       "      <td>6.987723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.848591</td>\n",
       "      <td>1.872076</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.769561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>SEP</td>\n",
       "      <td>86.350515</td>\n",
       "      <td>66.418060</td>\n",
       "      <td>39.848014</td>\n",
       "      <td>66.092621</td>\n",
       "      <td>53.110278</td>\n",
       "      <td>30.557896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>31.473925</td>\n",
       "      <td>7.953490</td>\n",
       "      <td>6.651649</td>\n",
       "      <td>12.019534</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>13.926898</td>\n",
       "      <td>5.663013</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.519623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>OCT</td>\n",
       "      <td>74.910045</td>\n",
       "      <td>52.914618</td>\n",
       "      <td>29.721084</td>\n",
       "      <td>61.366736</td>\n",
       "      <td>42.196248</td>\n",
       "      <td>24.559025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103207</td>\n",
       "      <td>25.729013</td>\n",
       "      <td>7.975400</td>\n",
       "      <td>10.803140</td>\n",
       "      <td>9.754228</td>\n",
       "      <td>0.327268</td>\n",
       "      <td>16.250392</td>\n",
       "      <td>1.152677</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.389655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>NOV</td>\n",
       "      <td>63.782516</td>\n",
       "      <td>37.252792</td>\n",
       "      <td>9.713161</td>\n",
       "      <td>52.539090</td>\n",
       "      <td>29.384204</td>\n",
       "      <td>0.391326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.369823</td>\n",
       "      <td>9.360489</td>\n",
       "      <td>0.710853</td>\n",
       "      <td>8.425101</td>\n",
       "      <td>3.263099</td>\n",
       "      <td>17.081087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.469636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>DEC</td>\n",
       "      <td>49.151039</td>\n",
       "      <td>32.779945</td>\n",
       "      <td>8.565107</td>\n",
       "      <td>43.501253</td>\n",
       "      <td>26.403070</td>\n",
       "      <td>2.568691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171012</td>\n",
       "      <td>30.035203</td>\n",
       "      <td>9.005973</td>\n",
       "      <td>13.063238</td>\n",
       "      <td>4.954492</td>\n",
       "      <td>6.141817</td>\n",
       "      <td>16.710648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.539618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>JAN</td>\n",
       "      <td>51.603404</td>\n",
       "      <td>27.391903</td>\n",
       "      <td>-0.148039</td>\n",
       "      <td>38.817646</td>\n",
       "      <td>20.733436</td>\n",
       "      <td>-8.513812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353906</td>\n",
       "      <td>27.208144</td>\n",
       "      <td>9.937532</td>\n",
       "      <td>0.697591</td>\n",
       "      <td>2.158710</td>\n",
       "      <td>9.017004</td>\n",
       "      <td>14.353479</td>\n",
       "      <td>0.204561</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.529621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>FEB</td>\n",
       "      <td>55.807460</td>\n",
       "      <td>34.986696</td>\n",
       "      <td>13.475683</td>\n",
       "      <td>43.526687</td>\n",
       "      <td>24.955311</td>\n",
       "      <td>5.251212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230043</td>\n",
       "      <td>32.970059</td>\n",
       "      <td>9.232392</td>\n",
       "      <td>25.414489</td>\n",
       "      <td>3.158557</td>\n",
       "      <td>2.563075</td>\n",
       "      <td>11.369789</td>\n",
       "      <td>0.077182</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.449640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>MAR</td>\n",
       "      <td>72.278760</td>\n",
       "      <td>42.527518</td>\n",
       "      <td>16.163708</td>\n",
       "      <td>47.173214</td>\n",
       "      <td>29.577827</td>\n",
       "      <td>6.417299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171012</td>\n",
       "      <td>29.907703</td>\n",
       "      <td>11.045504</td>\n",
       "      <td>6.407882</td>\n",
       "      <td>6.858583</td>\n",
       "      <td>2.852181</td>\n",
       "      <td>10.275397</td>\n",
       "      <td>0.716611</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.509626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>ARP</td>\n",
       "      <td>85.305443</td>\n",
       "      <td>51.656418</td>\n",
       "      <td>21.970248</td>\n",
       "      <td>53.181639</td>\n",
       "      <td>35.382062</td>\n",
       "      <td>12.721515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185989</td>\n",
       "      <td>27.064239</td>\n",
       "      <td>9.509505</td>\n",
       "      <td>12.758238</td>\n",
       "      <td>8.253307</td>\n",
       "      <td>0.808174</td>\n",
       "      <td>9.891667</td>\n",
       "      <td>1.736127</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.559613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>MAY</td>\n",
       "      <td>88.717714</td>\n",
       "      <td>65.765110</td>\n",
       "      <td>38.133418</td>\n",
       "      <td>64.652678</td>\n",
       "      <td>48.141753</td>\n",
       "      <td>26.138192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593883</td>\n",
       "      <td>31.632215</td>\n",
       "      <td>8.975590</td>\n",
       "      <td>24.486700</td>\n",
       "      <td>9.510037</td>\n",
       "      <td>0.218179</td>\n",
       "      <td>9.562606</td>\n",
       "      <td>6.851201</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.719574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>JUN</td>\n",
       "      <td>94.935745</td>\n",
       "      <td>72.250194</td>\n",
       "      <td>49.378090</td>\n",
       "      <td>66.975597</td>\n",
       "      <td>55.163926</td>\n",
       "      <td>34.929410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884931</td>\n",
       "      <td>26.598362</td>\n",
       "      <td>7.892497</td>\n",
       "      <td>9.973803</td>\n",
       "      <td>7.983505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.068762</td>\n",
       "      <td>6.553994</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.749566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>JUL</td>\n",
       "      <td>94.486551</td>\n",
       "      <td>74.924089</td>\n",
       "      <td>52.515406</td>\n",
       "      <td>69.564460</td>\n",
       "      <td>60.019149</td>\n",
       "      <td>44.308975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084196</td>\n",
       "      <td>29.864555</td>\n",
       "      <td>7.596663</td>\n",
       "      <td>39.075101</td>\n",
       "      <td>11.250805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.522498</td>\n",
       "      <td>7.200001</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.659588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>AUG</td>\n",
       "      <td>95.769169</td>\n",
       "      <td>72.236452</td>\n",
       "      <td>49.913575</td>\n",
       "      <td>70.645707</td>\n",
       "      <td>57.160585</td>\n",
       "      <td>37.261179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204506</td>\n",
       "      <td>33.754199</td>\n",
       "      <td>7.664999</td>\n",
       "      <td>3.963129</td>\n",
       "      <td>10.481103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.100641</td>\n",
       "      <td>6.232162</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.529621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>SEP</td>\n",
       "      <td>86.330704</td>\n",
       "      <td>64.757891</td>\n",
       "      <td>42.685359</td>\n",
       "      <td>61.761861</td>\n",
       "      <td>49.221634</td>\n",
       "      <td>31.004195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204506</td>\n",
       "      <td>25.124887</td>\n",
       "      <td>7.115135</td>\n",
       "      <td>5.524179</td>\n",
       "      <td>7.851274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.212706</td>\n",
       "      <td>2.418011</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.529621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>OCT</td>\n",
       "      <td>75.338957</td>\n",
       "      <td>48.717642</td>\n",
       "      <td>26.178812</td>\n",
       "      <td>53.494604</td>\n",
       "      <td>35.590995</td>\n",
       "      <td>20.118526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343522</td>\n",
       "      <td>27.792394</td>\n",
       "      <td>9.120230</td>\n",
       "      <td>8.947375</td>\n",
       "      <td>8.870945</td>\n",
       "      <td>0.458632</td>\n",
       "      <td>10.156463</td>\n",
       "      <td>0.873945</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.599603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>NOV</td>\n",
       "      <td>73.387568</td>\n",
       "      <td>42.445148</td>\n",
       "      <td>11.759290</td>\n",
       "      <td>53.810402</td>\n",
       "      <td>31.334574</td>\n",
       "      <td>6.142447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087948</td>\n",
       "      <td>27.737506</td>\n",
       "      <td>10.504951</td>\n",
       "      <td>6.922821</td>\n",
       "      <td>11.552323</td>\n",
       "      <td>3.370795</td>\n",
       "      <td>11.110684</td>\n",
       "      <td>0.573445</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.679583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>IL</td>\n",
       "      <td>1987</td>\n",
       "      <td>DEC</td>\n",
       "      <td>54.851605</td>\n",
       "      <td>31.435940</td>\n",
       "      <td>7.705909</td>\n",
       "      <td>48.230410</td>\n",
       "      <td>24.561279</td>\n",
       "      <td>2.072395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.844028</td>\n",
       "      <td>11.646561</td>\n",
       "      <td>6.928783</td>\n",
       "      <td>9.977321</td>\n",
       "      <td>8.323399</td>\n",
       "      <td>14.877116</td>\n",
       "      <td>1.181361</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.799554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>IL</td>\n",
       "      <td>1988</td>\n",
       "      <td>JAN</td>\n",
       "      <td>56.407002</td>\n",
       "      <td>25.686117</td>\n",
       "      <td>-0.995032</td>\n",
       "      <td>46.273711</td>\n",
       "      <td>17.866325</td>\n",
       "      <td>-7.806793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>31.689132</td>\n",
       "      <td>11.978539</td>\n",
       "      <td>1.444931</td>\n",
       "      <td>3.289410</td>\n",
       "      <td>8.299551</td>\n",
       "      <td>7.312518</td>\n",
       "      <td>0.576510</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.809551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>IL</td>\n",
       "      <td>1988</td>\n",
       "      <td>FEB</td>\n",
       "      <td>59.645858</td>\n",
       "      <td>26.753927</td>\n",
       "      <td>-0.651426</td>\n",
       "      <td>40.515945</td>\n",
       "      <td>18.611068</td>\n",
       "      <td>-7.612461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171012</td>\n",
       "      <td>30.835942</td>\n",
       "      <td>11.366350</td>\n",
       "      <td>6.048729</td>\n",
       "      <td>2.780704</td>\n",
       "      <td>9.300564</td>\n",
       "      <td>9.198702</td>\n",
       "      <td>0.044134</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.899529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>IL</td>\n",
       "      <td>1988</td>\n",
       "      <td>MAR</td>\n",
       "      <td>69.757621</td>\n",
       "      <td>38.434919</td>\n",
       "      <td>15.024229</td>\n",
       "      <td>51.018937</td>\n",
       "      <td>27.693562</td>\n",
       "      <td>8.844748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>34.058311</td>\n",
       "      <td>11.332478</td>\n",
       "      <td>372.538106</td>\n",
       "      <td>6.725774</td>\n",
       "      <td>6.585668</td>\n",
       "      <td>10.346304</td>\n",
       "      <td>2.881596</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.909526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>IL</td>\n",
       "      <td>1988</td>\n",
       "      <td>ARP</td>\n",
       "      <td>80.916532</td>\n",
       "      <td>49.536814</td>\n",
       "      <td>24.445530</td>\n",
       "      <td>55.833496</td>\n",
       "      <td>33.898013</td>\n",
       "      <td>11.252577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275534</td>\n",
       "      <td>41.454133</td>\n",
       "      <td>10.788335</td>\n",
       "      <td>3.927050</td>\n",
       "      <td>8.654531</td>\n",
       "      <td>0.460086</td>\n",
       "      <td>6.650997</td>\n",
       "      <td>3.396558</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.939519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>IL</td>\n",
       "      <td>1988</td>\n",
       "      <td>MAY</td>\n",
       "      <td>88.648736</td>\n",
       "      <td>63.285999</td>\n",
       "      <td>38.007417</td>\n",
       "      <td>58.065659</td>\n",
       "      <td>44.089405</td>\n",
       "      <td>26.759350</td>\n",
       "      <td>...</td>\n",
       "      <td>1.358716</td>\n",
       "      <td>37.070596</td>\n",
       "      <td>8.760104</td>\n",
       "      <td>16.876689</td>\n",
       "      <td>6.618215</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>3.626469</td>\n",
       "      <td>3.238601</td>\n",
       "      <td>2.03</td>\n",
       "      <td>2.029497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>IL</td>\n",
       "      <td>1988</td>\n",
       "      <td>JUN</td>\n",
       "      <td>97.441783</td>\n",
       "      <td>71.362958</td>\n",
       "      <td>44.108245</td>\n",
       "      <td>65.334190</td>\n",
       "      <td>49.943229</td>\n",
       "      <td>30.379235</td>\n",
       "      <td>...</td>\n",
       "      <td>2.432953</td>\n",
       "      <td>22.162214</td>\n",
       "      <td>7.857640</td>\n",
       "      <td>4.046168</td>\n",
       "      <td>2.947843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.199814</td>\n",
       "      <td>1.358670</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.509378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>340</td>\n",
       "      <td>IL</td>\n",
       "      <td>2014</td>\n",
       "      <td>JUL</td>\n",
       "      <td>93.846979</td>\n",
       "      <td>69.533818</td>\n",
       "      <td>50.719371</td>\n",
       "      <td>74.213351</td>\n",
       "      <td>58.528124</td>\n",
       "      <td>44.680399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280616</td>\n",
       "      <td>31.631130</td>\n",
       "      <td>6.640193</td>\n",
       "      <td>2.428684</td>\n",
       "      <td>9.495698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.217399</td>\n",
       "      <td>5.176212</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.118978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>341</td>\n",
       "      <td>IL</td>\n",
       "      <td>2014</td>\n",
       "      <td>AUG</td>\n",
       "      <td>90.890957</td>\n",
       "      <td>72.263597</td>\n",
       "      <td>50.677898</td>\n",
       "      <td>76.059315</td>\n",
       "      <td>63.058648</td>\n",
       "      <td>43.453073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247688</td>\n",
       "      <td>29.820666</td>\n",
       "      <td>5.866341</td>\n",
       "      <td>4.025913</td>\n",
       "      <td>11.777075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.004267</td>\n",
       "      <td>7.259140</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.739073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>342</td>\n",
       "      <td>IL</td>\n",
       "      <td>2014</td>\n",
       "      <td>SEP</td>\n",
       "      <td>91.921477</td>\n",
       "      <td>63.870515</td>\n",
       "      <td>39.322797</td>\n",
       "      <td>71.925580</td>\n",
       "      <td>53.068572</td>\n",
       "      <td>34.902195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280616</td>\n",
       "      <td>29.115301</td>\n",
       "      <td>6.217876</td>\n",
       "      <td>3.105514</td>\n",
       "      <td>7.529903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.055058</td>\n",
       "      <td>4.412042</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.509130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>343</td>\n",
       "      <td>IL</td>\n",
       "      <td>2014</td>\n",
       "      <td>OCT</td>\n",
       "      <td>79.939694</td>\n",
       "      <td>52.377502</td>\n",
       "      <td>30.009565</td>\n",
       "      <td>63.019905</td>\n",
       "      <td>42.385667</td>\n",
       "      <td>16.371301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142584</td>\n",
       "      <td>30.957589</td>\n",
       "      <td>7.942366</td>\n",
       "      <td>2.901838</td>\n",
       "      <td>12.432674</td>\n",
       "      <td>0.437813</td>\n",
       "      <td>4.171134</td>\n",
       "      <td>2.420886</td>\n",
       "      <td>3.58</td>\n",
       "      <td>3.579112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>344</td>\n",
       "      <td>IL</td>\n",
       "      <td>2014</td>\n",
       "      <td>NOV</td>\n",
       "      <td>67.480255</td>\n",
       "      <td>35.825140</td>\n",
       "      <td>9.772755</td>\n",
       "      <td>54.625033</td>\n",
       "      <td>26.391946</td>\n",
       "      <td>0.093362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179613</td>\n",
       "      <td>31.722576</td>\n",
       "      <td>10.206118</td>\n",
       "      <td>1.708385</td>\n",
       "      <td>7.384086</td>\n",
       "      <td>7.818873</td>\n",
       "      <td>2.738053</td>\n",
       "      <td>0.033495</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>345</td>\n",
       "      <td>IL</td>\n",
       "      <td>2014</td>\n",
       "      <td>DEC</td>\n",
       "      <td>54.855890</td>\n",
       "      <td>33.104515</td>\n",
       "      <td>6.871596</td>\n",
       "      <td>47.711540</td>\n",
       "      <td>27.232756</td>\n",
       "      <td>-3.721176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>27.371385</td>\n",
       "      <td>8.089446</td>\n",
       "      <td>1.598096</td>\n",
       "      <td>7.857272</td>\n",
       "      <td>3.744419</td>\n",
       "      <td>4.276190</td>\n",
       "      <td>0.128912</td>\n",
       "      <td>3.81</td>\n",
       "      <td>3.809055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>346</td>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>JAN</td>\n",
       "      <td>51.130063</td>\n",
       "      <td>25.235828</td>\n",
       "      <td>-7.132326</td>\n",
       "      <td>38.658598</td>\n",
       "      <td>18.235895</td>\n",
       "      <td>-13.382659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169950</td>\n",
       "      <td>31.706604</td>\n",
       "      <td>8.372164</td>\n",
       "      <td>1.221606</td>\n",
       "      <td>6.276689</td>\n",
       "      <td>9.317115</td>\n",
       "      <td>3.029987</td>\n",
       "      <td>0.048754</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.819053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>347</td>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>FEB</td>\n",
       "      <td>51.533679</td>\n",
       "      <td>20.571537</td>\n",
       "      <td>-7.815383</td>\n",
       "      <td>41.202170</td>\n",
       "      <td>11.970765</td>\n",
       "      <td>-19.757363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.970867</td>\n",
       "      <td>9.334185</td>\n",
       "      <td>1.278564</td>\n",
       "      <td>1.240099</td>\n",
       "      <td>11.237240</td>\n",
       "      <td>3.037794</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.779063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>348</td>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>MAR</td>\n",
       "      <td>73.101701</td>\n",
       "      <td>38.194787</td>\n",
       "      <td>-1.751123</td>\n",
       "      <td>49.947801</td>\n",
       "      <td>26.901783</td>\n",
       "      <td>-8.428940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123844</td>\n",
       "      <td>28.651405</td>\n",
       "      <td>8.266480</td>\n",
       "      <td>1.465324</td>\n",
       "      <td>7.336679</td>\n",
       "      <td>2.717470</td>\n",
       "      <td>3.919565</td>\n",
       "      <td>0.523391</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.789060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>349</td>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>ARP</td>\n",
       "      <td>78.882895</td>\n",
       "      <td>52.573288</td>\n",
       "      <td>25.908735</td>\n",
       "      <td>61.805920</td>\n",
       "      <td>37.288333</td>\n",
       "      <td>11.739397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364541</td>\n",
       "      <td>34.562780</td>\n",
       "      <td>9.528799</td>\n",
       "      <td>2.739780</td>\n",
       "      <td>11.121298</td>\n",
       "      <td>0.274590</td>\n",
       "      <td>1.776208</td>\n",
       "      <td>4.189540</td>\n",
       "      <td>3.71</td>\n",
       "      <td>3.709080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>350</td>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>MAY</td>\n",
       "      <td>84.884745</td>\n",
       "      <td>63.428020</td>\n",
       "      <td>35.767946</td>\n",
       "      <td>67.599624</td>\n",
       "      <td>51.224953</td>\n",
       "      <td>29.976145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203939</td>\n",
       "      <td>31.978787</td>\n",
       "      <td>9.271944</td>\n",
       "      <td>4.577083</td>\n",
       "      <td>13.856634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.490798</td>\n",
       "      <td>5.521460</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.619102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>351</td>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>JUN</td>\n",
       "      <td>91.473565</td>\n",
       "      <td>70.896868</td>\n",
       "      <td>47.813800</td>\n",
       "      <td>73.311502</td>\n",
       "      <td>60.763065</td>\n",
       "      <td>41.743208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123844</td>\n",
       "      <td>40.923783</td>\n",
       "      <td>7.503916</td>\n",
       "      <td>6.465531</td>\n",
       "      <td>13.572339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.206530</td>\n",
       "      <td>10.167240</td>\n",
       "      <td>3.65</td>\n",
       "      <td>3.649095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>352</td>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>JUL</td>\n",
       "      <td>90.701935</td>\n",
       "      <td>72.878642</td>\n",
       "      <td>54.529864</td>\n",
       "      <td>76.958795</td>\n",
       "      <td>63.472975</td>\n",
       "      <td>49.014850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269692</td>\n",
       "      <td>31.476291</td>\n",
       "      <td>6.044702</td>\n",
       "      <td>3.820778</td>\n",
       "      <td>9.391236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.508345</td>\n",
       "      <td>4.724758</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.849045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>353</td>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>AUG</td>\n",
       "      <td>90.465476</td>\n",
       "      <td>70.885366</td>\n",
       "      <td>50.222892</td>\n",
       "      <td>72.164270</td>\n",
       "      <td>60.343242</td>\n",
       "      <td>41.443154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280616</td>\n",
       "      <td>25.342742</td>\n",
       "      <td>5.933962</td>\n",
       "      <td>2.931901</td>\n",
       "      <td>7.129545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.308489</td>\n",
       "      <td>4.609560</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>354</td>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>SEP</td>\n",
       "      <td>91.118224</td>\n",
       "      <td>68.628671</td>\n",
       "      <td>43.926815</td>\n",
       "      <td>71.261417</td>\n",
       "      <td>56.466760</td>\n",
       "      <td>36.758903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455681</td>\n",
       "      <td>28.924293</td>\n",
       "      <td>6.483753</td>\n",
       "      <td>3.815673</td>\n",
       "      <td>7.487878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.742017</td>\n",
       "      <td>4.006889</td>\n",
       "      <td>3.69</td>\n",
       "      <td>3.689085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>355</td>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>OCT</td>\n",
       "      <td>80.709535</td>\n",
       "      <td>55.705794</td>\n",
       "      <td>28.846762</td>\n",
       "      <td>61.902473</td>\n",
       "      <td>42.871323</td>\n",
       "      <td>17.564898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211792</td>\n",
       "      <td>29.154354</td>\n",
       "      <td>8.834588</td>\n",
       "      <td>1.599482</td>\n",
       "      <td>7.466220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.478168</td>\n",
       "      <td>0.909474</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.699083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>356</td>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>NOV</td>\n",
       "      <td>70.297323</td>\n",
       "      <td>44.810163</td>\n",
       "      <td>11.241853</td>\n",
       "      <td>59.617839</td>\n",
       "      <td>35.202024</td>\n",
       "      <td>4.497239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309611</td>\n",
       "      <td>36.901813</td>\n",
       "      <td>10.017259</td>\n",
       "      <td>4.086457</td>\n",
       "      <td>9.970229</td>\n",
       "      <td>1.246099</td>\n",
       "      <td>5.044528</td>\n",
       "      <td>1.015249</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.699083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>357</td>\n",
       "      <td>IL</td>\n",
       "      <td>2015</td>\n",
       "      <td>DEC</td>\n",
       "      <td>65.188431</td>\n",
       "      <td>38.381905</td>\n",
       "      <td>14.931579</td>\n",
       "      <td>58.616438</td>\n",
       "      <td>31.494061</td>\n",
       "      <td>-1.608238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.124233</td>\n",
       "      <td>10.323283</td>\n",
       "      <td>5.184984</td>\n",
       "      <td>10.398913</td>\n",
       "      <td>2.988891</td>\n",
       "      <td>6.702315</td>\n",
       "      <td>2.058452</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.789060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>358</td>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>JAN</td>\n",
       "      <td>56.208957</td>\n",
       "      <td>28.210967</td>\n",
       "      <td>1.826907</td>\n",
       "      <td>47.001875</td>\n",
       "      <td>20.913061</td>\n",
       "      <td>-8.831172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059394</td>\n",
       "      <td>32.207612</td>\n",
       "      <td>9.778199</td>\n",
       "      <td>1.232944</td>\n",
       "      <td>4.799819</td>\n",
       "      <td>8.511348</td>\n",
       "      <td>2.459232</td>\n",
       "      <td>0.015259</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.729075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>359</td>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>FEB</td>\n",
       "      <td>66.900172</td>\n",
       "      <td>33.807153</td>\n",
       "      <td>6.331705</td>\n",
       "      <td>50.693125</td>\n",
       "      <td>23.887685</td>\n",
       "      <td>-6.109805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061922</td>\n",
       "      <td>36.206540</td>\n",
       "      <td>9.824466</td>\n",
       "      <td>0.836330</td>\n",
       "      <td>4.224789</td>\n",
       "      <td>6.814530</td>\n",
       "      <td>3.299300</td>\n",
       "      <td>0.565800</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>360</td>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>MAR</td>\n",
       "      <td>71.538246</td>\n",
       "      <td>44.964954</td>\n",
       "      <td>14.037543</td>\n",
       "      <td>57.680772</td>\n",
       "      <td>35.297947</td>\n",
       "      <td>5.115327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.789749</td>\n",
       "      <td>9.629134</td>\n",
       "      <td>3.098442</td>\n",
       "      <td>11.949852</td>\n",
       "      <td>2.288162</td>\n",
       "      <td>3.608914</td>\n",
       "      <td>2.883812</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.669090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>361</td>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>ARP</td>\n",
       "      <td>82.089725</td>\n",
       "      <td>52.325395</td>\n",
       "      <td>25.594821</td>\n",
       "      <td>59.313656</td>\n",
       "      <td>38.906985</td>\n",
       "      <td>12.135843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.691810</td>\n",
       "      <td>39.165480</td>\n",
       "      <td>9.512475</td>\n",
       "      <td>2.662589</td>\n",
       "      <td>10.297377</td>\n",
       "      <td>1.718848</td>\n",
       "      <td>2.704257</td>\n",
       "      <td>3.283336</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.669090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>362</td>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>MAY</td>\n",
       "      <td>86.822760</td>\n",
       "      <td>61.317018</td>\n",
       "      <td>37.123673</td>\n",
       "      <td>67.550974</td>\n",
       "      <td>48.989342</td>\n",
       "      <td>24.687457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572066</td>\n",
       "      <td>31.258585</td>\n",
       "      <td>8.051852</td>\n",
       "      <td>2.985320</td>\n",
       "      <td>13.488475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.162346</td>\n",
       "      <td>6.827281</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.819053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>363</td>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>JUN</td>\n",
       "      <td>93.436156</td>\n",
       "      <td>72.669555</td>\n",
       "      <td>49.718113</td>\n",
       "      <td>73.627985</td>\n",
       "      <td>58.861639</td>\n",
       "      <td>37.094091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423862</td>\n",
       "      <td>26.449861</td>\n",
       "      <td>7.022661</td>\n",
       "      <td>2.645324</td>\n",
       "      <td>6.599050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.866101</td>\n",
       "      <td>4.338305</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.919028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>364</td>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>JUL</td>\n",
       "      <td>92.433461</td>\n",
       "      <td>74.195493</td>\n",
       "      <td>53.968246</td>\n",
       "      <td>77.715821</td>\n",
       "      <td>64.993758</td>\n",
       "      <td>46.259779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247688</td>\n",
       "      <td>33.613471</td>\n",
       "      <td>6.212372</td>\n",
       "      <td>5.552655</td>\n",
       "      <td>11.304979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.765973</td>\n",
       "      <td>9.193705</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>365</td>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>AUG</td>\n",
       "      <td>89.771611</td>\n",
       "      <td>73.976134</td>\n",
       "      <td>53.152785</td>\n",
       "      <td>75.197298</td>\n",
       "      <td>65.722541</td>\n",
       "      <td>46.370412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342538</td>\n",
       "      <td>29.407200</td>\n",
       "      <td>5.602346</td>\n",
       "      <td>4.854680</td>\n",
       "      <td>10.822233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.440266</td>\n",
       "      <td>7.746643</td>\n",
       "      <td>3.17</td>\n",
       "      <td>3.169214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>366</td>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>SEP</td>\n",
       "      <td>90.539780</td>\n",
       "      <td>68.526421</td>\n",
       "      <td>47.780800</td>\n",
       "      <td>72.822394</td>\n",
       "      <td>58.098483</td>\n",
       "      <td>31.084937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185766</td>\n",
       "      <td>26.270849</td>\n",
       "      <td>6.906921</td>\n",
       "      <td>3.616776</td>\n",
       "      <td>9.658813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.307687</td>\n",
       "      <td>4.115569</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.269189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>367</td>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>OCT</td>\n",
       "      <td>83.401525</td>\n",
       "      <td>58.012848</td>\n",
       "      <td>33.431914</td>\n",
       "      <td>65.081202</td>\n",
       "      <td>47.633134</td>\n",
       "      <td>27.152489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302619</td>\n",
       "      <td>31.092277</td>\n",
       "      <td>8.219588</td>\n",
       "      <td>1.907746</td>\n",
       "      <td>8.156643</td>\n",
       "      <td>0.022274</td>\n",
       "      <td>0.872271</td>\n",
       "      <td>3.452099</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.299182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>368</td>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>NOV</td>\n",
       "      <td>76.323585</td>\n",
       "      <td>44.534557</td>\n",
       "      <td>16.508950</td>\n",
       "      <td>60.101894</td>\n",
       "      <td>35.188458</td>\n",
       "      <td>10.089226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189700</td>\n",
       "      <td>35.356631</td>\n",
       "      <td>7.994466</td>\n",
       "      <td>2.237944</td>\n",
       "      <td>6.781341</td>\n",
       "      <td>1.083395</td>\n",
       "      <td>3.198530</td>\n",
       "      <td>1.929992</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.319177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>369</td>\n",
       "      <td>IL</td>\n",
       "      <td>2016</td>\n",
       "      <td>DEC</td>\n",
       "      <td>56.499747</td>\n",
       "      <td>28.025835</td>\n",
       "      <td>-5.724015</td>\n",
       "      <td>52.141124</td>\n",
       "      <td>20.537073</td>\n",
       "      <td>-14.910411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.388733</td>\n",
       "      <td>9.509581</td>\n",
       "      <td>1.391766</td>\n",
       "      <td>6.026444</td>\n",
       "      <td>7.998503</td>\n",
       "      <td>4.082039</td>\n",
       "      <td>0.110676</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.469140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 State  YEAR MONTH   TMP_HIGH    TMP_AVG    TMP_LOW    DP_HIGH  \\\n",
       "0             0    IL  1986   JAN  55.953658  27.617217  -2.980925  42.880246   \n",
       "1             1    IL  1986   FEB  53.838886  28.731112   0.353638  46.537659   \n",
       "2             2    IL  1986   MAR  77.668458  41.937001  13.926317  51.205991   \n",
       "3             3    IL  1986   ARP  85.651529  52.949283  25.351909  60.274139   \n",
       "4             4    IL  1986   MAY  85.415668  62.338688  36.602102  61.467698   \n",
       "5             5    IL  1986   JUN  93.180721  71.263159  47.746898  67.021368   \n",
       "6             6    IL  1986   JUL  93.651100  75.325134  57.224505  71.740380   \n",
       "7             7    IL  1986   AUG  88.032940  68.355756  43.240501  70.470204   \n",
       "8             8    IL  1986   SEP  86.350515  66.418060  39.848014  66.092621   \n",
       "9             9    IL  1986   OCT  74.910045  52.914618  29.721084  61.366736   \n",
       "10           10    IL  1986   NOV  63.782516  37.252792   9.713161  52.539090   \n",
       "11           11    IL  1986   DEC  49.151039  32.779945   8.565107  43.501253   \n",
       "12           12    IL  1987   JAN  51.603404  27.391903  -0.148039  38.817646   \n",
       "13           13    IL  1987   FEB  55.807460  34.986696  13.475683  43.526687   \n",
       "14           14    IL  1987   MAR  72.278760  42.527518  16.163708  47.173214   \n",
       "15           15    IL  1987   ARP  85.305443  51.656418  21.970248  53.181639   \n",
       "16           16    IL  1987   MAY  88.717714  65.765110  38.133418  64.652678   \n",
       "17           17    IL  1987   JUN  94.935745  72.250194  49.378090  66.975597   \n",
       "18           18    IL  1987   JUL  94.486551  74.924089  52.515406  69.564460   \n",
       "19           19    IL  1987   AUG  95.769169  72.236452  49.913575  70.645707   \n",
       "20           20    IL  1987   SEP  86.330704  64.757891  42.685359  61.761861   \n",
       "21           21    IL  1987   OCT  75.338957  48.717642  26.178812  53.494604   \n",
       "22           22    IL  1987   NOV  73.387568  42.445148  11.759290  53.810402   \n",
       "23           23    IL  1987   DEC  54.851605  31.435940   7.705909  48.230410   \n",
       "24           24    IL  1988   JAN  56.407002  25.686117  -0.995032  46.273711   \n",
       "25           25    IL  1988   FEB  59.645858  26.753927  -0.651426  40.515945   \n",
       "26           26    IL  1988   MAR  69.757621  38.434919  15.024229  51.018937   \n",
       "27           27    IL  1988   ARP  80.916532  49.536814  24.445530  55.833496   \n",
       "28           28    IL  1988   MAY  88.648736  63.285999  38.007417  58.065659   \n",
       "29           29    IL  1988   JUN  97.441783  71.362958  44.108245  65.334190   \n",
       "..          ...   ...   ...   ...        ...        ...        ...        ...   \n",
       "340         340    IL  2014   JUL  93.846979  69.533818  50.719371  74.213351   \n",
       "341         341    IL  2014   AUG  90.890957  72.263597  50.677898  76.059315   \n",
       "342         342    IL  2014   SEP  91.921477  63.870515  39.322797  71.925580   \n",
       "343         343    IL  2014   OCT  79.939694  52.377502  30.009565  63.019905   \n",
       "344         344    IL  2014   NOV  67.480255  35.825140   9.772755  54.625033   \n",
       "345         345    IL  2014   DEC  54.855890  33.104515   6.871596  47.711540   \n",
       "346         346    IL  2015   JAN  51.130063  25.235828  -7.132326  38.658598   \n",
       "347         347    IL  2015   FEB  51.533679  20.571537  -7.815383  41.202170   \n",
       "348         348    IL  2015   MAR  73.101701  38.194787  -1.751123  49.947801   \n",
       "349         349    IL  2015   ARP  78.882895  52.573288  25.908735  61.805920   \n",
       "350         350    IL  2015   MAY  84.884745  63.428020  35.767946  67.599624   \n",
       "351         351    IL  2015   JUN  91.473565  70.896868  47.813800  73.311502   \n",
       "352         352    IL  2015   JUL  90.701935  72.878642  54.529864  76.958795   \n",
       "353         353    IL  2015   AUG  90.465476  70.885366  50.222892  72.164270   \n",
       "354         354    IL  2015   SEP  91.118224  68.628671  43.926815  71.261417   \n",
       "355         355    IL  2015   OCT  80.709535  55.705794  28.846762  61.902473   \n",
       "356         356    IL  2015   NOV  70.297323  44.810163  11.241853  59.617839   \n",
       "357         357    IL  2015   DEC  65.188431  38.381905  14.931579  58.616438   \n",
       "358         358    IL  2016   JAN  56.208957  28.210967   1.826907  47.001875   \n",
       "359         359    IL  2016   FEB  66.900172  33.807153   6.331705  50.693125   \n",
       "360         360    IL  2016   MAR  71.538246  44.964954  14.037543  57.680772   \n",
       "361         361    IL  2016   ARP  82.089725  52.325395  25.594821  59.313656   \n",
       "362         362    IL  2016   MAY  86.822760  61.317018  37.123673  67.550974   \n",
       "363         363    IL  2016   JUN  93.436156  72.669555  49.718113  73.627985   \n",
       "364         364    IL  2016   JUL  92.433461  74.195493  53.968246  77.715821   \n",
       "365         365    IL  2016   AUG  89.771611  73.976134  53.152785  75.197298   \n",
       "366         366    IL  2016   SEP  90.539780  68.526421  47.780800  72.822394   \n",
       "367         367    IL  2016   OCT  83.401525  58.012848  33.431914  65.081202   \n",
       "368         368    IL  2016   NOV  76.323585  44.534557  16.508950  60.101894   \n",
       "369         369    IL  2016   DEC  56.499747  28.025835  -5.724015  52.141124   \n",
       "\n",
       "        DP_AVG     DP_LOW        ...          VIS_LOW  WIND_HIGH   WIND_AVG  \\\n",
       "0    19.992262  -8.511984        ...         0.186718  29.600197  11.077783   \n",
       "1    23.958440  -4.555208        ...         0.109089  24.679482   9.622850   \n",
       "2    29.862688   2.748564        ...         0.634101  36.374250  12.349509   \n",
       "3    37.049373  11.479874        ...         0.474198  28.918771  10.770362   \n",
       "4    47.555928  27.795232        ...         0.280723  33.072546   8.983943   \n",
       "5    55.502022  33.450048        ...         0.950015  25.666101   8.317023   \n",
       "6    60.440695  41.882198        ...         0.360831  30.924584   7.668664   \n",
       "7    53.535626  34.387899        ...         0.123844  23.558382   6.839216   \n",
       "8    53.110278  30.557896        ...         0.061922  31.473925   7.953490   \n",
       "9    42.196248  24.559025        ...         0.103207  25.729013   7.975400   \n",
       "10   29.384204   0.391326        ...         0.000000  27.369823   9.360489   \n",
       "11   26.403070   2.568691        ...         0.171012  30.035203   9.005973   \n",
       "12   20.733436  -8.513812        ...         0.353906  27.208144   9.937532   \n",
       "13   24.955311   5.251212        ...         0.230043  32.970059   9.232392   \n",
       "14   29.577827   6.417299        ...         0.171012  29.907703  11.045504   \n",
       "15   35.382062  12.721515        ...         0.185989  27.064239   9.509505   \n",
       "16   48.141753  26.138192        ...         0.593883  31.632215   8.975590   \n",
       "17   55.163926  34.929410        ...         0.884931  26.598362   7.892497   \n",
       "18   60.019149  44.308975        ...         0.084196  29.864555   7.596663   \n",
       "19   57.160585  37.261179        ...         0.204506  33.754199   7.664999   \n",
       "20   49.221634  31.004195        ...         0.204506  25.124887   7.115135   \n",
       "21   35.590995  20.118526        ...         0.343522  27.792394   9.120230   \n",
       "22   31.334574   6.142447        ...         0.087948  27.737506  10.504951   \n",
       "23   24.561279   2.072395        ...         0.000000  38.844028  11.646561   \n",
       "24   17.866325  -7.806793        ...         0.061922  31.689132  11.978539   \n",
       "25   18.611068  -7.612461        ...         0.171012  30.835942  11.366350   \n",
       "26   27.693562   8.844748        ...         0.109089  34.058311  11.332478   \n",
       "27   33.898013  11.252577        ...         0.275534  41.454133  10.788335   \n",
       "28   44.089405  26.759350        ...         1.358716  37.070596   8.760104   \n",
       "29   49.943229  30.379235        ...         2.432953  22.162214   7.857640   \n",
       "..         ...        ...        ...              ...        ...        ...   \n",
       "340  58.528124  44.680399        ...         0.280616  31.631130   6.640193   \n",
       "341  63.058648  43.453073        ...         0.247688  29.820666   5.866341   \n",
       "342  53.068572  34.902195        ...         0.280616  29.115301   6.217876   \n",
       "343  42.385667  16.371301        ...         0.142584  30.957589   7.942366   \n",
       "344  26.391946   0.093362        ...         0.179613  31.722576  10.206118   \n",
       "345  27.232756  -3.721176        ...         0.061922  27.371385   8.089446   \n",
       "346  18.235895 -13.382659        ...         0.169950  31.706604   8.372164   \n",
       "347  11.970765 -19.757363        ...         0.000000  33.970867   9.334185   \n",
       "348  26.901783  -8.428940        ...         0.123844  28.651405   8.266480   \n",
       "349  37.288333  11.739397        ...         0.364541  34.562780   9.528799   \n",
       "350  51.224953  29.976145        ...         0.203939  31.978787   9.271944   \n",
       "351  60.763065  41.743208        ...         0.123844  40.923783   7.503916   \n",
       "352  63.472975  49.014850        ...         0.269692  31.476291   6.044702   \n",
       "353  60.343242  41.443154        ...         0.280616  25.342742   5.933962   \n",
       "354  56.466760  36.758903        ...         0.455681  28.924293   6.483753   \n",
       "355  42.871323  17.564898        ...         0.211792  29.154354   8.834588   \n",
       "356  35.202024   4.497239        ...         0.309611  36.901813  10.017259   \n",
       "357  31.494061  -1.608238        ...         0.000000  38.124233  10.323283   \n",
       "358  20.913061  -8.831172        ...         0.059394  32.207612   9.778199   \n",
       "359  23.887685  -6.109805        ...         0.061922  36.206540   9.824466   \n",
       "360  35.297947   5.115327        ...         0.000000  34.789749   9.629134   \n",
       "361  38.906985  12.135843        ...         0.691810  39.165480   9.512475   \n",
       "362  48.989342  24.687457        ...         0.572066  31.258585   8.051852   \n",
       "363  58.861639  37.094091        ...         0.423862  26.449861   7.022661   \n",
       "364  64.993758  46.259779        ...         0.247688  33.613471   6.212372   \n",
       "365  65.722541  46.370412        ...         0.342538  29.407200   5.602346   \n",
       "366  58.098483  31.084937        ...         0.185766  26.270849   6.906921   \n",
       "367  47.633134  27.152489        ...         0.302619  31.092277   8.219588   \n",
       "368  35.188458  10.089226        ...         0.189700  35.356631   7.994466   \n",
       "369  20.537073 -14.910411        ...         0.000000  29.388733   9.509581   \n",
       "\n",
       "         PRECIP  NUMDAY_RAIN  NUM_DAYS_SNOW  NUM_DAYS_FOG  NUM_DAYS_THNDRSTRM  \\\n",
       "0      0.479003     2.544772       7.584738      9.707042            0.000000   \n",
       "1      1.722515     5.510271       9.231277     15.203328            0.658457   \n",
       "2     15.116988     5.934169       3.868506      9.890731            1.055313   \n",
       "3     17.947352     9.195066       1.960945      7.611098            2.058521   \n",
       "4     15.212715     9.610446       0.327268     14.161837            4.663441   \n",
       "5     10.598875     8.135348       0.000000     12.649299            4.995009   \n",
       "6     11.201697     9.899297       0.000000     12.561325            6.286502   \n",
       "7      1.162475     6.987723       0.000000     11.848591            1.872076   \n",
       "8      6.651649    12.019534       0.109089     13.926898            5.663013   \n",
       "9     10.803140     9.754228       0.327268     16.250392            1.152677   \n",
       "10     0.710853     8.425101       3.263099     17.081087            0.000000   \n",
       "11    13.063238     4.954492       6.141817     16.710648            0.000000   \n",
       "12     0.697591     2.158710       9.017004     14.353479            0.204561   \n",
       "13    25.414489     3.158557       2.563075     11.369789            0.077182   \n",
       "14     6.407882     6.858583       2.852181     10.275397            0.716611   \n",
       "15    12.758238     8.253307       0.808174      9.891667            1.736127   \n",
       "16    24.486700     9.510037       0.218179      9.562606            6.851201   \n",
       "17     9.973803     7.983505       0.000000     11.068762            6.553994   \n",
       "18    39.075101    11.250805       0.000000     16.522498            7.200001   \n",
       "19     3.963129    10.481103       0.000000     14.100641            6.232162   \n",
       "20     5.524179     7.851274       0.000000     12.212706            2.418011   \n",
       "21     8.947375     8.870945       0.458632     10.156463            0.873945   \n",
       "22     6.922821    11.552323       3.370795     11.110684            0.573445   \n",
       "23     6.928783     9.977321       8.323399     14.877116            1.181361   \n",
       "24     1.444931     3.289410       8.299551      7.312518            0.576510   \n",
       "25     6.048729     2.780704       9.300564      9.198702            0.044134   \n",
       "26   372.538106     6.725774       6.585668     10.346304            2.881596   \n",
       "27     3.927050     8.654531       0.460086      6.650997            3.396558   \n",
       "28    16.876689     6.618215       0.109089      3.626469            3.238601   \n",
       "29     4.046168     2.947843       0.000000      3.199814            1.358670   \n",
       "..          ...          ...            ...           ...                 ...   \n",
       "340    2.428684     9.495698       0.000000      2.217399            5.176212   \n",
       "341    4.025913    11.777075       0.000000      5.004267            7.259140   \n",
       "342    3.105514     7.529903       0.000000      4.055058            4.412042   \n",
       "343    2.901838    12.432674       0.437813      4.171134            2.420886   \n",
       "344    1.708385     7.384086       7.818873      2.738053            0.033495   \n",
       "345    1.598096     7.857272       3.744419      4.276190            0.128912   \n",
       "346    1.221606     6.276689       9.317115      3.029987            0.048754   \n",
       "347    1.278564     1.240099      11.237240      3.037794            0.061922   \n",
       "348    1.465324     7.336679       2.717470      3.919565            0.523391   \n",
       "349    2.739780    11.121298       0.274590      1.776208            4.189540   \n",
       "350    4.577083    13.856634       0.000000      1.490798            5.521460   \n",
       "351    6.465531    13.572339       0.000000      4.206530           10.167240   \n",
       "352    3.820778     9.391236       0.000000      3.508345            4.724758   \n",
       "353    2.931901     7.129545       0.000000      2.308489            4.609560   \n",
       "354    3.815673     7.487878       0.000000      1.742017            4.006889   \n",
       "355    1.599482     7.466220       0.000000      2.478168            0.909474   \n",
       "356    4.086457     9.970229       1.246099      5.044528            1.015249   \n",
       "357    5.184984    10.398913       2.988891      6.702315            2.058452   \n",
       "358    1.232944     4.799819       8.511348      2.459232            0.015259   \n",
       "359    0.836330     4.224789       6.814530      3.299300            0.565800   \n",
       "360    3.098442    11.949852       2.288162      3.608914            2.883812   \n",
       "361    2.662589    10.297377       1.718848      2.704257            3.283336   \n",
       "362    2.985320    13.488475       0.000000      2.162346            6.827281   \n",
       "363    2.645324     6.599050       0.000000      1.866101            4.338305   \n",
       "364    5.552655    11.304979       0.000000      4.765973            9.193705   \n",
       "365    4.854680    10.822233       0.000000      3.440266            7.746643   \n",
       "366    3.616776     9.658813       0.000000      3.307687            4.115569   \n",
       "367    1.907746     8.156643       0.022274      0.872271            3.452099   \n",
       "368    2.237944     6.781341       1.083395      3.198530            1.929992   \n",
       "369    1.391766     6.026444       7.998503      4.082039            0.110676   \n",
       "\n",
       "     Price  Discounted_Price  \n",
       "0     2.35          2.349417  \n",
       "1     2.38          2.379410  \n",
       "2     2.35          2.349417  \n",
       "3     2.34          2.339420  \n",
       "4     2.45          2.449393  \n",
       "5     2.45          2.449393  \n",
       "6     2.04          2.039494  \n",
       "7     1.77          1.769561  \n",
       "8     1.52          1.519623  \n",
       "9     1.39          1.389655  \n",
       "10    1.47          1.469636  \n",
       "11    1.54          1.539618  \n",
       "12    1.53          1.529621  \n",
       "13    1.45          1.449640  \n",
       "14    1.51          1.509626  \n",
       "15    1.56          1.559613  \n",
       "16    1.72          1.719574  \n",
       "17    1.75          1.749566  \n",
       "18    1.66          1.659588  \n",
       "19    1.53          1.529621  \n",
       "20    1.53          1.529621  \n",
       "21    1.60          1.599603  \n",
       "22    1.68          1.679583  \n",
       "23    1.80          1.799554  \n",
       "24    1.81          1.809551  \n",
       "25    1.90          1.899529  \n",
       "26    1.91          1.909526  \n",
       "27    1.94          1.939519  \n",
       "28    2.03          2.029497  \n",
       "29    2.51          2.509378  \n",
       "..     ...               ...  \n",
       "340   4.12          4.118978  \n",
       "341   3.74          3.739073  \n",
       "342   3.51          3.509130  \n",
       "343   3.58          3.579112  \n",
       "344   3.63          3.629100  \n",
       "345   3.81          3.809055  \n",
       "346   3.82          3.819053  \n",
       "347   3.78          3.779063  \n",
       "348   3.79          3.789060  \n",
       "349   3.71          3.709080  \n",
       "350   3.62          3.619102  \n",
       "351   3.65          3.649095  \n",
       "352   3.85          3.849045  \n",
       "353   3.63          3.629100  \n",
       "354   3.69          3.689085  \n",
       "355   3.70          3.699083  \n",
       "356   3.70          3.699083  \n",
       "357   3.79          3.789060  \n",
       "358   3.73          3.729075  \n",
       "359   3.63          3.629100  \n",
       "360   3.67          3.669090  \n",
       "361   3.67          3.669090  \n",
       "362   3.82          3.819053  \n",
       "363   3.92          3.919028  \n",
       "364   3.63          3.629100  \n",
       "365   3.17          3.169214  \n",
       "366   3.27          3.269189  \n",
       "367   3.30          3.299182  \n",
       "368   3.32          3.319177  \n",
       "369   3.47          3.469140  \n",
       "\n",
       "[370 rows x 28 columns]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_update = pd.read_csv('IL_Corn_Price_Updated_1986_2016.csv')\n",
    "p_update['Period'] = pd.Categorical(p_update['Period'], ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC'])\n",
    "\n",
    "p_update['Discounted_Value'] = p_update['Value'] /1.000248 \n",
    "#ps\n",
    "#Risk-free rate for 1986 - 2016 : 9.34% ( 30-year Treasury Constant Maturity Rate at 1986 )\n",
    "# Monthly discount rate = 1/1.000248 \n",
    "\n",
    "p_update = p_update[['Year', 'Period','Value', 'Discounted_Value']]\n",
    "price_updated = p_update.dropna(axis=0, how='any')\n",
    "#price_updated = p_update[p_update.Period != 'MARKETING YEAR']\n",
    "\n",
    "price_updated = price_updated.sort_values(['Year', 'Period'])\n",
    "price_updated = price_updated.reset_index(drop=True)\n",
    "#price_updated.iloc[168:174]\n",
    "\n",
    "price_updated = price_updated[~((price_updated.Year == 2000) & ((price_updated.Period == 'MAR') | (price_updated.Period == 'APR')))]\n",
    "price_updated = price_updated.reset_index(drop=True)\n",
    "price_updated.iloc[168:172]\n",
    "\n",
    "\n",
    "xf['Price'] = price_updated['Value']\n",
    "xf['Discounted_Price'] = price_updated['Discounted_Value']\n",
    "xf['State'] = 'IL'\n",
    "\n",
    "# Reorder State Column\n",
    "cols = xf.columns.tolist()\n",
    "#cols\n",
    "\n",
    "cols.insert(1, cols.pop(cols.index('State')))\n",
    "\n",
    "xf = xf.reindex(columns = cols)\n",
    "#df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "w_avg_model_scaled = xf\n",
    "w_avg_model_scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0 State  YEAR MONTH   TMP_HIGH    TMP_AVG    TMP_LOW    DP_HIGH  \\\n",
      "168         168    IL  2000   JAN  60.098523  24.919939  -5.750129  50.876803   \n",
      "169         169    IL  2000   FEB  53.769218  31.870521   5.378496  44.864486   \n",
      "170         170    IL  2000   MAY  84.325735  62.360525  37.698853  68.870647   \n",
      "171         171    IL  2000   JUN  88.622913  69.307943  52.228400  71.995094   \n",
      "172         172    IL  2000   JUL  87.461845  70.466481  51.066055  74.704992   \n",
      "173         173    IL  2000   AUG  89.361775  71.781012  56.695409  74.533066   \n",
      "174         174    IL  2000   SEP  90.167238  63.849101  36.705700  71.408262   \n",
      "175         175    IL  2000   OCT  81.999760  54.605762  25.520160  62.377842   \n",
      "176         176    IL  2000   NOV  74.379046  37.371940   9.233327  59.321373   \n",
      "177         177    IL  2000   DEC  45.127840  19.968293  -7.283182  37.862855   \n",
      "\n",
      "        DP_AVG     DP_LOW        ...          VIS_LOW  WIND_HIGH  WIND_AVG  \\\n",
      "168  18.079444 -19.152746        ...         0.061922  28.491682  9.674976   \n",
      "169  25.930720  -0.570920        ...         0.044134  28.925880  9.571381   \n",
      "170  51.653895  25.975416        ...         0.570830  30.201699  9.129810   \n",
      "171  60.166479  45.635832        ...         0.408795  27.953090  8.176592   \n",
      "172  62.448314  46.322380        ...         0.364541  34.450706  5.869163   \n",
      "173  64.284915  49.179987        ...         0.540686  22.741620  6.606653   \n",
      "174  52.829779  30.919987        ...         0.390551  23.160926  7.234548   \n",
      "175  44.455570  19.058809        ...         0.000000  25.779084  6.906690   \n",
      "176  29.790357   2.151880        ...         0.464038  26.935924  9.159310   \n",
      "177  14.533100 -28.467988        ...         0.061922  29.538696  9.032580   \n",
      "\n",
      "         PRECIP  NUMDAY_RAIN  NUM_DAYS_SNOW  NUM_DAYS_FOG  NUM_DAYS_THNDRSTRM  \\\n",
      "168    0.769395     4.527847       7.865567      3.129778            0.388589   \n",
      "169  287.022956     3.572751       4.803434      2.284139            0.385381   \n",
      "170    4.098408    10.428558       0.000000      1.610892            4.896486   \n",
      "171    3.466575     9.051719       0.000000      1.014093            4.340215   \n",
      "172    1.547238     7.626151       0.000000      2.554090            3.622915   \n",
      "173    0.749923     3.978840       0.000000      1.372018            2.996176   \n",
      "174    2.481654     7.601901       0.000000      1.483107            2.869120   \n",
      "175    1.183908     5.560382       0.171634      5.240789            1.311068   \n",
      "176    1.951554     8.234563       5.222024      1.607092            0.741233   \n",
      "177    0.743131     2.996964      15.732234      4.902815            0.125816   \n",
      "\n",
      "     Price  Discounted_Price  \n",
      "168   1.97          1.969512  \n",
      "169   2.03          2.029497  \n",
      "170   2.20          2.199455  \n",
      "171   1.89          1.889531  \n",
      "172   1.66          1.659588  \n",
      "173   1.54          1.539618  \n",
      "174   1.64          1.639593  \n",
      "175   1.80          1.799554  \n",
      "176   1.92          1.919524  \n",
      "177   2.03          2.029497  \n",
      "\n",
      "[10 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "w_group = w_avg_model_scaled.groupby([\"YEAR\"])\n",
    "year_list = w_avg_model_scaled[\"YEAR\"].drop_duplicates()\n",
    "\n",
    "for year in year_list:\n",
    "    temp_df = w_group.get_group(year)\n",
    "    #print(str(year) + \": \" +str(len(temp_df[\"PRICE\"].dropna())))\n",
    "\n",
    "print(w_group.get_group(2000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>TMP_HIGH</th>\n",
       "      <th>TMP_AVG</th>\n",
       "      <th>TMP_LOW</th>\n",
       "      <th>DP_HIGH</th>\n",
       "      <th>DP_AVG</th>\n",
       "      <th>DP_LOW</th>\n",
       "      <th>HUM_HIGH</th>\n",
       "      <th>...</th>\n",
       "      <th>VIS_LOW</th>\n",
       "      <th>WIND_HIGH</th>\n",
       "      <th>WIND_AVG</th>\n",
       "      <th>PRECIP</th>\n",
       "      <th>NUMDAY_RAIN</th>\n",
       "      <th>NUM_DAYS_SNOW</th>\n",
       "      <th>NUM_DAYS_FOG</th>\n",
       "      <th>NUM_DAYS_THNDRSTRM</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNTED_PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>JAN</td>\n",
       "      <td>55.953658</td>\n",
       "      <td>27.617217</td>\n",
       "      <td>-2.980925</td>\n",
       "      <td>42.880246</td>\n",
       "      <td>19.992262</td>\n",
       "      <td>-8.511984</td>\n",
       "      <td>99.026800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186718</td>\n",
       "      <td>29.600197</td>\n",
       "      <td>11.077783</td>\n",
       "      <td>0.479003</td>\n",
       "      <td>2.544772</td>\n",
       "      <td>7.584738</td>\n",
       "      <td>9.707042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.349417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>FEB</td>\n",
       "      <td>53.838886</td>\n",
       "      <td>28.731112</td>\n",
       "      <td>0.353638</td>\n",
       "      <td>46.537659</td>\n",
       "      <td>23.958440</td>\n",
       "      <td>-4.555208</td>\n",
       "      <td>99.510367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>24.679482</td>\n",
       "      <td>9.622850</td>\n",
       "      <td>1.722515</td>\n",
       "      <td>5.510271</td>\n",
       "      <td>9.231277</td>\n",
       "      <td>15.203328</td>\n",
       "      <td>0.658457</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.379410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>MAR</td>\n",
       "      <td>77.668458</td>\n",
       "      <td>41.937001</td>\n",
       "      <td>13.926317</td>\n",
       "      <td>51.205991</td>\n",
       "      <td>29.862688</td>\n",
       "      <td>2.748564</td>\n",
       "      <td>99.138166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634101</td>\n",
       "      <td>36.374250</td>\n",
       "      <td>12.349509</td>\n",
       "      <td>15.116988</td>\n",
       "      <td>5.934169</td>\n",
       "      <td>3.868506</td>\n",
       "      <td>9.890731</td>\n",
       "      <td>1.055313</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.349417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>ARP</td>\n",
       "      <td>85.651529</td>\n",
       "      <td>52.949283</td>\n",
       "      <td>25.351909</td>\n",
       "      <td>60.274139</td>\n",
       "      <td>37.049373</td>\n",
       "      <td>11.479874</td>\n",
       "      <td>99.558871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474198</td>\n",
       "      <td>28.918771</td>\n",
       "      <td>10.770362</td>\n",
       "      <td>17.947352</td>\n",
       "      <td>9.195066</td>\n",
       "      <td>1.960945</td>\n",
       "      <td>7.611098</td>\n",
       "      <td>2.058521</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.339420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>MAY</td>\n",
       "      <td>85.415668</td>\n",
       "      <td>62.338688</td>\n",
       "      <td>36.602102</td>\n",
       "      <td>61.467698</td>\n",
       "      <td>47.555928</td>\n",
       "      <td>27.795232</td>\n",
       "      <td>95.246992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280723</td>\n",
       "      <td>33.072546</td>\n",
       "      <td>8.983943</td>\n",
       "      <td>15.212715</td>\n",
       "      <td>9.610446</td>\n",
       "      <td>0.327268</td>\n",
       "      <td>14.161837</td>\n",
       "      <td>4.663441</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.449393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  STATE  YEAR MONTH   TMP_HIGH    TMP_AVG    TMP_LOW    DP_HIGH     DP_AVG  \\\n",
       "0    IL  1986   JAN  55.953658  27.617217  -2.980925  42.880246  19.992262   \n",
       "1    IL  1986   FEB  53.838886  28.731112   0.353638  46.537659  23.958440   \n",
       "2    IL  1986   MAR  77.668458  41.937001  13.926317  51.205991  29.862688   \n",
       "3    IL  1986   ARP  85.651529  52.949283  25.351909  60.274139  37.049373   \n",
       "4    IL  1986   MAY  85.415668  62.338688  36.602102  61.467698  47.555928   \n",
       "\n",
       "      DP_LOW   HUM_HIGH        ...          VIS_LOW  WIND_HIGH   WIND_AVG  \\\n",
       "0  -8.511984  99.026800        ...         0.186718  29.600197  11.077783   \n",
       "1  -4.555208  99.510367        ...         0.109089  24.679482   9.622850   \n",
       "2   2.748564  99.138166        ...         0.634101  36.374250  12.349509   \n",
       "3  11.479874  99.558871        ...         0.474198  28.918771  10.770362   \n",
       "4  27.795232  95.246992        ...         0.280723  33.072546   8.983943   \n",
       "\n",
       "      PRECIP  NUMDAY_RAIN  NUM_DAYS_SNOW  NUM_DAYS_FOG  NUM_DAYS_THNDRSTRM  \\\n",
       "0   0.479003     2.544772       7.584738      9.707042            0.000000   \n",
       "1   1.722515     5.510271       9.231277     15.203328            0.658457   \n",
       "2  15.116988     5.934169       3.868506      9.890731            1.055313   \n",
       "3  17.947352     9.195066       1.960945      7.611098            2.058521   \n",
       "4  15.212715     9.610446       0.327268     14.161837            4.663441   \n",
       "\n",
       "   PRICE  DISCOUNTED_PRICE  \n",
       "0   2.35          2.349417  \n",
       "1   2.38          2.379410  \n",
       "2   2.35          2.349417  \n",
       "3   2.34          2.339420  \n",
       "4   2.45          2.449393  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List to store results\n",
    "# Stores the performance on test set.\n",
    "model_results = []\n",
    "#  Stores the name of model\n",
    "model_name = []\n",
    "\n",
    "# Making all column names UPPER CASE --> inline with the weather data file\n",
    "w_avg_model_scaled.columns = map(str.upper, w_avg_model_scaled.columns)\n",
    "\n",
    "# rename UNNAMED:0 to INDEX\n",
    "w_avg_model_scaled.rename(columns={'UNNAMED: 0': 'INDEX'}, inplace=True)\n",
    "\n",
    "# drop UNNAMED:0.1\n",
    "w_avg_model_scaled = w_avg_model_scaled.drop('INDEX', axis=1)\n",
    "\n",
    "w_avg_model_scaled.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_avg_model_scaled.to_csv(\"final_input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Original data: (370, 27)\n",
      "\n",
      "Variables: Index(['TMP_HIGH', 'TMP_AVG', 'TMP_LOW', 'DP_HIGH', 'DP_AVG', 'DP_LOW',\n",
      "       'HUM_HIGH', 'HUM_AVG', 'HUM_LOW', 'SEALVL_HIGH', 'SEALVL_AVG',\n",
      "       'SEALVL_LOW', 'VIS_HIGH', 'VIS_AVG', 'VIS_LOW', 'WIND_HIGH', 'WIND_AVG',\n",
      "       'PRECIP', 'NUMDAY_RAIN', 'NUM_DAYS_SNOW', 'NUM_DAYS_FOG',\n",
      "       'NUM_DAYS_THNDRSTRM', 'PRICE', 'DISCOUNTED_PRICE'],\n",
      "      dtype='object')\n",
      "\n",
      "Shape of variable data: (370, 24)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_names = list(w_avg_model_scaled)\n",
    "\n",
    "# List all column types\n",
    "#input_data.dtypes\n",
    "print(\"Shape of Original data: {}\".format(w_avg_model_scaled.shape) + \"\\n\")\n",
    "\n",
    "input_variables = w_avg_model_scaled.iloc[:,3:27]\n",
    "print(\"Variables: \" + str(input_variables.columns) + \"\\n\")\n",
    "print(\"Shape of variable data: {}\".format(input_variables.shape) + \"\\n\")\n",
    "\n",
    "#print(input_data.iloc[:,26])\n",
    "price = w_avg_model_scaled.iloc[:,25]\n",
    "discounted_price = w_avg_model_scaled.iloc[:,26]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>TMP_HIGH</th>\n",
       "      <th>TMP_AVG</th>\n",
       "      <th>TMP_LOW</th>\n",
       "      <th>DP_HIGH</th>\n",
       "      <th>DP_AVG</th>\n",
       "      <th>DP_LOW</th>\n",
       "      <th>HUM_HIGH</th>\n",
       "      <th>...</th>\n",
       "      <th>VIS_LOW</th>\n",
       "      <th>WIND_HIGH</th>\n",
       "      <th>WIND_AVG</th>\n",
       "      <th>PRECIP</th>\n",
       "      <th>NUMDAY_RAIN</th>\n",
       "      <th>NUM_DAYS_SNOW</th>\n",
       "      <th>NUM_DAYS_FOG</th>\n",
       "      <th>NUM_DAYS_THNDRSTRM</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNTED_PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>JAN</td>\n",
       "      <td>55.953658</td>\n",
       "      <td>27.617217</td>\n",
       "      <td>-2.980925</td>\n",
       "      <td>42.880246</td>\n",
       "      <td>19.992262</td>\n",
       "      <td>-8.511984</td>\n",
       "      <td>99.026800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186718</td>\n",
       "      <td>29.600197</td>\n",
       "      <td>11.077783</td>\n",
       "      <td>0.479003</td>\n",
       "      <td>2.544772</td>\n",
       "      <td>7.584738</td>\n",
       "      <td>9.707042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.349417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>FEB</td>\n",
       "      <td>53.838886</td>\n",
       "      <td>28.731112</td>\n",
       "      <td>0.353638</td>\n",
       "      <td>46.537659</td>\n",
       "      <td>23.958440</td>\n",
       "      <td>-4.555208</td>\n",
       "      <td>99.510367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>24.679482</td>\n",
       "      <td>9.622850</td>\n",
       "      <td>1.722515</td>\n",
       "      <td>5.510271</td>\n",
       "      <td>9.231277</td>\n",
       "      <td>15.203328</td>\n",
       "      <td>0.658457</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.379410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>MAR</td>\n",
       "      <td>77.668458</td>\n",
       "      <td>41.937001</td>\n",
       "      <td>13.926317</td>\n",
       "      <td>51.205991</td>\n",
       "      <td>29.862688</td>\n",
       "      <td>2.748564</td>\n",
       "      <td>99.138166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634101</td>\n",
       "      <td>36.374250</td>\n",
       "      <td>12.349509</td>\n",
       "      <td>15.116988</td>\n",
       "      <td>5.934169</td>\n",
       "      <td>3.868506</td>\n",
       "      <td>9.890731</td>\n",
       "      <td>1.055313</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.349417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>ARP</td>\n",
       "      <td>85.651529</td>\n",
       "      <td>52.949283</td>\n",
       "      <td>25.351909</td>\n",
       "      <td>60.274139</td>\n",
       "      <td>37.049373</td>\n",
       "      <td>11.479874</td>\n",
       "      <td>99.558871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474198</td>\n",
       "      <td>28.918771</td>\n",
       "      <td>10.770362</td>\n",
       "      <td>17.947352</td>\n",
       "      <td>9.195066</td>\n",
       "      <td>1.960945</td>\n",
       "      <td>7.611098</td>\n",
       "      <td>2.058521</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.339420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IL</td>\n",
       "      <td>1986</td>\n",
       "      <td>MAY</td>\n",
       "      <td>85.415668</td>\n",
       "      <td>62.338688</td>\n",
       "      <td>36.602102</td>\n",
       "      <td>61.467698</td>\n",
       "      <td>47.555928</td>\n",
       "      <td>27.795232</td>\n",
       "      <td>95.246992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280723</td>\n",
       "      <td>33.072546</td>\n",
       "      <td>8.983943</td>\n",
       "      <td>15.212715</td>\n",
       "      <td>9.610446</td>\n",
       "      <td>0.327268</td>\n",
       "      <td>14.161837</td>\n",
       "      <td>4.663441</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.449393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  STATE  YEAR MONTH   TMP_HIGH    TMP_AVG    TMP_LOW    DP_HIGH     DP_AVG  \\\n",
       "0    IL  1986   JAN  55.953658  27.617217  -2.980925  42.880246  19.992262   \n",
       "1    IL  1986   FEB  53.838886  28.731112   0.353638  46.537659  23.958440   \n",
       "2    IL  1986   MAR  77.668458  41.937001  13.926317  51.205991  29.862688   \n",
       "3    IL  1986   ARP  85.651529  52.949283  25.351909  60.274139  37.049373   \n",
       "4    IL  1986   MAY  85.415668  62.338688  36.602102  61.467698  47.555928   \n",
       "\n",
       "      DP_LOW   HUM_HIGH        ...          VIS_LOW  WIND_HIGH   WIND_AVG  \\\n",
       "0  -8.511984  99.026800        ...         0.186718  29.600197  11.077783   \n",
       "1  -4.555208  99.510367        ...         0.109089  24.679482   9.622850   \n",
       "2   2.748564  99.138166        ...         0.634101  36.374250  12.349509   \n",
       "3  11.479874  99.558871        ...         0.474198  28.918771  10.770362   \n",
       "4  27.795232  95.246992        ...         0.280723  33.072546   8.983943   \n",
       "\n",
       "      PRECIP  NUMDAY_RAIN  NUM_DAYS_SNOW  NUM_DAYS_FOG  NUM_DAYS_THNDRSTRM  \\\n",
       "0   0.479003     2.544772       7.584738      9.707042            0.000000   \n",
       "1   1.722515     5.510271       9.231277     15.203328            0.658457   \n",
       "2  15.116988     5.934169       3.868506      9.890731            1.055313   \n",
       "3  17.947352     9.195066       1.960945      7.611098            2.058521   \n",
       "4  15.212715     9.610446       0.327268     14.161837            4.663441   \n",
       "\n",
       "   PRICE  DISCOUNTED_PRICE  \n",
       "0   2.35          2.349417  \n",
       "1   2.38          2.379410  \n",
       "2   2.35          2.349417  \n",
       "3   2.34          2.339420  \n",
       "4   2.45          2.449393  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lnn =  pd.read_csv('final_input.csv')\n",
    "#rename UNNAMED:0 to INDEX\n",
    "lnn.rename(columns={'Unnamed: 0': 'INDEX'}, inplace=True)\n",
    "\n",
    "# drop UNNAMED:0.1\n",
    "lnn = lnn.drop('INDEX', axis=1)\n",
    "lnn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from lstm_predictior_updated import lstm_model\n",
    "#from lstm_predictior_updated import lstm_model, load_csvdata\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.contrib import learn as tflearn\n",
    "from tensorflow.contrib import layers as tflayers\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "from tensorflow.contrib.learn.python import SKCompat\n",
    "\n",
    "LOG_DIR = ''\n",
    "TIMESTEPS = 5\n",
    "RNN_LAYERS = [{'num_units': 4}]\n",
    "DENSE_LAYERS = [10, 10]\n",
    "TRAINING_STEPS = 50000\n",
    "PRINT_STEPS = TRAINING_STEPS / 10\n",
    "BATCH_SIZE = 37\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_data(data, time_steps, labels=False):\n",
    "    \"\"\"\n",
    "    creates new data frame based on previous observation\n",
    "      * example:\n",
    "        l = [1, 2, 3, 4, 5]\n",
    "        time_steps = 2\n",
    "        -> labels == False [[1, 2], [2, 3], [3, 4]] #Data frame for input with 2 timesteps\n",
    "        -> labels == True [3, 4, 5] # labels for predicting the next timestep\n",
    "    \"\"\"\n",
    "    rnn_df = []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        if labels:\n",
    "            try:\n",
    "                rnn_df.append(data.iloc[i + time_steps].as_matrix())\n",
    "            except AttributeError:\n",
    "                rnn_df.append(data.iloc[i + time_steps])\n",
    "        else:\n",
    "            data_ = data.iloc[i: i + time_steps].as_matrix()\n",
    "            rnn_df.append(data_ if len(data_.shape) > 1 else [[i] for i in data_])\n",
    "\n",
    "    return np.array(rnn_df, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "def split_data(data, val_size=0.1, test_size=0.1):\n",
    "    \"\"\"\n",
    "    splits data to training, validation and testing parts\n",
    "    \"\"\"\n",
    "    ntest = int(round(len(data) * (1 - test_size)))\n",
    "    nval = int(round(len(data.iloc[:ntest]) * (1 - val_size)))\n",
    "\n",
    "    df_train, df_val, df_test = data.iloc[:nval], data.iloc[nval:ntest], data.iloc[ntest:]\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(data, time_steps, labels=False, val_size=0.1, test_size=0.1):\n",
    "    \"\"\"\n",
    "    Given the number of `time_steps` and some data,\n",
    "    prepares training, validation and test data for an lstm cell.\n",
    "    \"\"\"\n",
    "    df_train, df_val, df_test = split_data(data, val_size, test_size)\n",
    "    return (rnn_data(df_train, time_steps, labels=labels),\n",
    "            rnn_data(df_val, time_steps, labels=labels),\n",
    "            rnn_data(df_test, time_steps, labels=labels))\n",
    "\n",
    "def load_csvdata(rawdata, unraw, time_steps, seperate=False):\n",
    "    data = rawdata\n",
    "    dataa = unraw\n",
    "\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        data = pd.DataFrame(data)\n",
    "        \n",
    "    if not isinstance(dataa, pd.DataFrame):\n",
    "        dataa = pd.DataFrame(dataa)\n",
    "\n",
    "    #train_x, val_x, test_x = prepare_data(data['a'] if seperate else data, time_steps)\n",
    "    #train_y, val_y, test_y = prepare_data(data['b'] if seperate else dataa, time_steps, labels=True)\n",
    "    \n",
    "    train_x, val_x, test_x = prepare_data(data, time_steps)\n",
    "    train_y, val_y, test_y = prepare_data(dataa, time_steps, labels=True)\n",
    "    return dict(train=train_x, val=val_x, test=test_x), dict(train=train_y, val=val_y, test=test_y)\n",
    "\n",
    "def generate_data(fct, x, time_steps, seperate=False):\n",
    "    \"\"\"generates data with based on a function fct\"\"\"\n",
    "    data = fct(x)\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        data = pd.DataFrame(data)\n",
    "    train_x, val_x, test_x = prepare_data(data['a'] if seperate else data, time_steps)\n",
    "    train_y, val_y, test_y = prepare_data(data['b'] if seperate else data, time_steps, labels=True)\n",
    "    return dict(train=train_x, val=val_x, test=test_x), dict(train=train_y, val=val_y, test=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dateparse = lambda dates: pd.datetime.strptime(dates, '%d/%m/%Y %H:%M')\n",
    "\n",
    "#rawdata = pd.read_csv(\"/Users/siddharthbhaduri/Desktop/Work/Fall-2017/Google-Project/Week9/RealMarketPriceDataPT.csv\", \n",
    "#                  parse_dates={'timeline': ['date', '(UTC)']}, \n",
    "#                   index_col='timeline', date_parser=dateparse)\n",
    "r_data = lnn.iloc[:,3:25]\n",
    "un_data = lnn.iloc[:,26]\n",
    "\n",
    "X, y = load_csvdata(rawdata = r_data, unraw = un_data, time_steps = TIMESTEPS, seperate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/qm/ssr8wzks7cj5449yqx9mtkhr0000gn/T/tmpvcfqjzpq\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x121876518>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/var/folders/qm/ssr8wzks7cj5449yqx9mtkhr0000gn/T/tmpvcfqjzpq'}\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:269: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "[[[ 55.95365906  27.61721611  -2.98092484 ...,   7.58473778   9.70704174\n",
      "     0.        ]\n",
      "  [ 53.83888626  28.73111153   0.35363761 ...,   9.23127747  15.20332813\n",
      "     0.6584565 ]\n",
      "  [ 77.66845703  41.93700027  13.92631721 ...,   3.86850572   9.89073086\n",
      "     1.05531263]\n",
      "  [ 85.6515274   52.9492836   25.35190964 ...,   1.96094465   7.61109781\n",
      "     2.05852056]\n",
      "  [ 85.41566467  62.3386879   36.60210419 ...,   0.32726842  14.16183758\n",
      "     4.6634407 ]]\n",
      "\n",
      " [[ 53.83888626  28.73111153   0.35363761 ...,   9.23127747  15.20332813\n",
      "     0.6584565 ]\n",
      "  [ 77.66845703  41.93700027  13.92631721 ...,   3.86850572   9.89073086\n",
      "     1.05531263]\n",
      "  [ 85.6515274   52.9492836   25.35190964 ...,   1.96094465   7.61109781\n",
      "     2.05852056]\n",
      "  [ 85.41566467  62.3386879   36.60210419 ...,   0.32726842  14.16183758\n",
      "     4.6634407 ]\n",
      "  [ 93.18071747  71.26316071  47.74689865 ...,   0.          12.64929867\n",
      "     4.99500847]]\n",
      "\n",
      " [[ 77.66845703  41.93700027  13.92631721 ...,   3.86850572   9.89073086\n",
      "     1.05531263]\n",
      "  [ 85.6515274   52.9492836   25.35190964 ...,   1.96094465   7.61109781\n",
      "     2.05852056]\n",
      "  [ 85.41566467  62.3386879   36.60210419 ...,   0.32726842  14.16183758\n",
      "     4.6634407 ]\n",
      "  [ 93.18071747  71.26316071  47.74689865 ...,   0.          12.64929867\n",
      "     4.99500847]\n",
      "  [ 93.65110016  75.32513428  57.22450638 ...,   0.          12.56132507\n",
      "     6.28650236]]\n",
      "\n",
      " ..., \n",
      " [[ 91.37134552  74.71672058  54.08141327 ...,   0.           1.89207804\n",
      "     6.48630857]\n",
      "  [ 92.56842041  74.31201172  51.95959091 ...,   0.           1.83388102\n",
      "     4.55277491]\n",
      "  [ 90.94846344  65.46694183  41.00561523 ...,   0.           0.76875407\n",
      "     4.33805037]\n",
      "  [ 83.71923065  54.55018997  27.14562988 ...,   0.           0.39051202\n",
      "     2.40926647]\n",
      "  [ 71.65087891  41.58517838  17.17705727 ...,   1.34205961   1.56263173\n",
      "     1.40740919]]\n",
      "\n",
      " [[ 92.56842041  74.31201172  51.95959091 ...,   0.           1.83388102\n",
      "     4.55277491]\n",
      "  [ 90.94846344  65.46694183  41.00561523 ...,   0.           0.76875407\n",
      "     4.33805037]\n",
      "  [ 83.71923065  54.55018997  27.14562988 ...,   0.           0.39051202\n",
      "     2.40926647]\n",
      "  [ 71.65087891  41.58517838  17.17705727 ...,   1.34205961   1.56263173\n",
      "     1.40740919]\n",
      "  [ 56.06632614  22.97768974  -1.27758086 ...,  11.87644196   5.88995934\n",
      "     0.53336781]]\n",
      "\n",
      " [[ 90.94846344  65.46694183  41.00561523 ...,   0.           0.76875407\n",
      "     4.33805037]\n",
      "  [ 83.71923065  54.55018997  27.14562988 ...,   0.           0.39051202\n",
      "     2.40926647]\n",
      "  [ 71.65087891  41.58517838  17.17705727 ...,   1.34205961   1.56263173\n",
      "     1.40740919]\n",
      "  [ 56.06632614  22.97768974  -1.27758086 ...,  11.87644196   5.88995934\n",
      "     0.53336781]\n",
      "  [ 44.4429512   22.8907299   -7.05665302 ...,  12.94300079   2.82974696\n",
      "     0.        ]]]\n",
      "[[ 2.44939256]\n",
      " [ 2.03949428]\n",
      " [ 1.76956117]\n",
      " [ 1.51962316]\n",
      " [ 1.38965535]\n",
      " [ 1.46963549]\n",
      " [ 1.53961813]\n",
      " [ 1.52962065]\n",
      " [ 1.44964051]\n",
      " [ 1.50962555]\n",
      " [ 1.55961323]\n",
      " [ 1.7195735 ]\n",
      " [ 1.74956608]\n",
      " [ 1.65958846]\n",
      " [ 1.52962065]\n",
      " [ 1.52962065]\n",
      " [ 1.5996033 ]\n",
      " [ 1.67958343]\n",
      " [ 1.79955375]\n",
      " [ 1.80955124]\n",
      " [ 1.89952886]\n",
      " [ 1.90952647]\n",
      " [ 1.93951905]\n",
      " [ 2.02949667]\n",
      " [ 2.50937772]\n",
      " [ 2.7693131 ]\n",
      " [ 2.67933559]\n",
      " [ 2.62934804]\n",
      " [ 2.56936288]\n",
      " [ 2.54936767]\n",
      " [ 2.52937269]\n",
      " [ 2.61935043]\n",
      " [ 2.67933559]\n",
      " [ 2.66933799]\n",
      " [ 2.62934804]\n",
      " [ 2.67933559]\n",
      " [ 2.57936025]\n",
      " [ 2.55936527]\n",
      " [ 2.30942726]\n",
      " [ 2.33941984]\n",
      " [ 2.2494421 ]\n",
      " [ 2.27943468]\n",
      " [ 2.32942224]\n",
      " [ 2.30942726]\n",
      " [ 2.33941984]\n",
      " [ 2.43939495]\n",
      " [ 2.60935283]\n",
      " [ 2.70932817]\n",
      " [ 2.73932076]\n",
      " [ 2.70932817]\n",
      " [ 2.54936767]\n",
      " [ 2.36941242]\n",
      " [ 2.25943971]\n",
      " [ 2.21944952]\n",
      " [ 2.28943229]\n",
      " [ 2.32942224]\n",
      " [ 2.3894074 ]\n",
      " [ 2.46938753]\n",
      " [ 2.51937509]\n",
      " [ 2.47938514]\n",
      " [ 2.399405  ]\n",
      " [ 2.36941242]\n",
      " [ 2.47938514]\n",
      " [ 2.3894074 ]\n",
      " [ 2.41939998]\n",
      " [ 2.37940979]\n",
      " [ 2.43939495]\n",
      " [ 2.47938514]\n",
      " [ 2.57936025]\n",
      " [ 2.62934804]\n",
      " [ 2.55936527]\n",
      " [ 2.54936767]\n",
      " [ 2.52937269]\n",
      " [ 2.34941745]\n",
      " [ 2.19945455]\n",
      " [ 2.15946436]\n",
      " [ 1.99950409]\n",
      " [ 2.0095017 ]\n",
      " [ 2.05948925]\n",
      " [ 2.07948422]\n",
      " [ 2.04949164]\n",
      " [ 2.14946699]\n",
      " [ 2.21944952]\n",
      " [ 2.20945215]\n",
      " [ 2.1094768 ]\n",
      " [ 2.22944713]\n",
      " [ 2.25943971]\n",
      " [ 2.22944713]\n",
      " [ 2.30942726]\n",
      " [ 2.43939495]\n",
      " [ 2.68933296]\n",
      " [ 2.70932817]\n",
      " [ 2.85929084]\n",
      " [ 2.81930089]\n",
      " [ 2.74931812]\n",
      " [ 2.66933799]\n",
      " [ 2.70932817]\n",
      " [ 2.30942726]\n",
      " [ 2.16946197]\n",
      " [ 2.17945957]\n",
      " [ 2.02949667]\n",
      " [ 1.99950409]\n",
      " [ 2.16946197]\n",
      " [ 2.19945455]\n",
      " [ 2.25943971]\n",
      " [ 2.31942487]\n",
      " [ 2.42939758]\n",
      " [ 2.46938753]\n",
      " [ 2.60935283]\n",
      " [ 2.73932076]\n",
      " [ 2.65934038]\n",
      " [ 2.70932817]\n",
      " [ 2.85929084]\n",
      " [ 2.99925613]\n",
      " [ 3.08923388]\n",
      " [ 3.13922143]\n",
      " [ 3.52912474]\n",
      " [ 3.72907519]\n",
      " [ 4.08898592]\n",
      " [ 4.44889688]\n",
      " [ 4.4189043 ]\n",
      " [ 4.6988349 ]\n",
      " [ 4.50888157]\n",
      " [ 3.49913216]\n",
      " [ 2.93927097]\n",
      " [ 2.72932315]\n",
      " [ 2.7793107 ]\n",
      " [ 2.7693131 ]\n",
      " [ 2.71932554]\n",
      " [ 2.86928844]\n",
      " [ 2.89928102]\n",
      " [ 2.79930568]\n",
      " [ 2.63934541]\n",
      " [ 2.48938274]\n",
      " [ 2.64934301]\n",
      " [ 2.67933559]\n",
      " [ 2.63934541]\n",
      " [ 2.60935283]\n",
      " [ 2.64934301]\n",
      " [ 2.67933559]\n",
      " [ 2.66933799]\n",
      " [ 2.64934301]\n",
      " [ 2.48938274]\n",
      " [ 2.399405  ]\n",
      " [ 2.3894074 ]\n",
      " [ 2.29942966]\n",
      " [ 2.0095017 ]\n",
      " [ 1.87953389]\n",
      " [ 1.9895066 ]\n",
      " [ 2.02949667]\n",
      " [ 2.12947178]\n",
      " [ 2.11947441]\n",
      " [ 2.12947178]\n",
      " [ 2.13946939]\n",
      " [ 2.11947441]\n",
      " [ 2.07948422]\n",
      " [ 2.07948422]\n",
      " [ 1.78955615]\n",
      " [ 1.83954382]\n",
      " [ 1.84954131]\n",
      " [ 1.76956117]\n",
      " [ 1.80955124]\n",
      " [ 1.94951653]\n",
      " [ 1.96951151]\n",
      " [ 2.02949667]\n",
      " [ 2.19945455]\n",
      " [ 1.88953137]\n",
      " [ 1.65958846]\n",
      " [ 1.53961813]\n",
      " [ 1.63959336]\n",
      " [ 1.79955375]\n",
      " [ 1.91952395]\n",
      " [ 2.02949667]\n",
      " [ 1.96951151]\n",
      " [ 1.99950409]\n",
      " [ 1.99950409]\n",
      " [ 1.91952395]\n",
      " [ 1.85953879]\n",
      " [ 1.82954621]\n",
      " [ 1.94951653]\n",
      " [ 1.9895066 ]\n",
      " [ 1.93951905]\n",
      " [ 1.84954131]\n",
      " [ 1.90952647]\n",
      " [ 2.05948925]\n",
      " [ 2.0095017 ]\n",
      " [ 1.97950912]\n",
      " [ 1.97950912]\n",
      " [ 1.95951402]\n",
      " [ 1.99950409]\n",
      " [ 2.04949164]\n",
      " [ 2.23944473]\n",
      " [ 2.46938753]\n",
      " [ 2.49938011]\n",
      " [ 2.35941482]\n",
      " [ 2.32942224]\n",
      " [ 2.36941242]\n",
      " [ 2.36941242]\n",
      " [ 2.34941745]\n",
      " [ 2.35941482]\n",
      " [ 2.399405  ]\n",
      " [ 2.42939758]\n",
      " [ 2.3894074 ]\n",
      " [ 2.19945455]\n",
      " [ 2.15946436]\n",
      " [ 2.17945957]\n",
      " [ 2.12947178]\n",
      " [ 2.20945215]\n",
      " [ 2.33941984]\n",
      " [ 2.36941242]\n",
      " [ 2.64934301]\n",
      " [ 2.7693131 ]\n",
      " [ 2.96926355]\n",
      " [ 2.92927361]\n",
      " [ 2.86928844]\n",
      " [ 2.48938274]\n",
      " [ 2.34941745]\n",
      " [ 2.19945455]\n",
      " [ 2.21944952]\n",
      " [ 2.07948422]\n",
      " [ 2.16946197]\n",
      " [ 2.22944713]\n",
      " [ 2.01949906]\n",
      " [ 2.08948183]\n",
      " [ 2.08948183]\n",
      " [ 2.04949164]\n",
      " [ 2.11947441]\n",
      " [ 2.22944713]\n",
      " [ 2.02949667]\n",
      " [ 1.91952395]\n",
      " [ 1.82954621]\n",
      " [ 1.8695364 ]\n",
      " [ 2.0095017 ]\n",
      " [ 2.05948925]\n",
      " [ 2.07948422]\n",
      " [ 2.11947441]\n",
      " [ 2.19945455]\n",
      " [ 2.25943971]\n",
      " [ 2.27943468]\n",
      " [ 2.27943468]\n",
      " [ 2.14946699]\n",
      " [ 2.20945215]\n",
      " [ 2.61935043]\n",
      " [ 3.01925111]\n",
      " [ 2.99925613]\n",
      " [ 3.00925374]\n",
      " [ 3.439147  ]\n",
      " [ 3.51912737]\n",
      " [ 3.47913718]\n",
      " [ 3.51912737]\n",
      " [ 3.63909745]\n",
      " [ 3.31917691]\n",
      " [ 3.1592164 ]\n",
      " [ 3.2092042 ]\n",
      " [ 3.30917931]\n",
      " [ 3.52912474]\n",
      " [ 3.72907519]\n",
      " [ 3.8790381 ]\n",
      " [ 4.57886457]\n",
      " [ 4.68883705]\n",
      " [ 5.14872313]\n",
      " [ 5.24869823]\n",
      " [ 5.70858431]\n",
      " [ 5.42865372]\n",
      " [ 5.16871834]\n",
      " [ 4.98876286]\n",
      " [ 4.28893614]\n",
      " [ 4.10898113]\n",
      " [ 4.08898592]\n",
      " [ 4.33892393]\n",
      " [ 3.74907017]\n",
      " [ 3.7390728 ]\n",
      " [ 3.80905533]\n",
      " [ 3.94902062]\n",
      " [ 4.00900555]\n",
      " [ 3.55911732]\n",
      " [ 3.31917691]\n",
      " [ 3.26918936]\n",
      " [ 3.5891099 ]\n",
      " [ 3.54911971]\n",
      " [ 3.56911492]\n",
      " [ 3.62910008]\n",
      " [ 3.47913718]\n",
      " [ 3.52912474]\n",
      " [ 3.41915202]\n",
      " [ 3.48913479]\n",
      " [ 3.40915442]\n",
      " [ 3.50912976]\n",
      " [ 3.72907519]\n",
      " [ 4.04899597]\n",
      " [ 4.50888157]\n",
      " [ 4.61885452]\n",
      " [ 4.83879995]\n",
      " [ 4.93877506]\n",
      " [ 5.85854721]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regressor = SKCompat(learn.Estimator(model_fn=lstm_model(TIMESTEPS, RNN_LAYERS, DENSE_LAYERS),))\n",
    "\n",
    "validation_monitor = learn.monitors.ValidationMonitor(X['val'], y['val'],\n",
    "                                                     every_n_steps=PRINT_STEPS,\n",
    "                                                     early_stopping_rounds=1000)\n",
    "print(X['train'])\n",
    "print(y['train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/models.py:107: mean_squared_error_regressor (from tensorflow.contrib.learn.python.learn.ops.losses_ops) is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Use `tf.contrib.losses.mean_squared_error` and explicit logits computation.\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/ops/losses_ops.py:39: mean_squared_error (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.mean_squared_error instead.\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:539: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/qm/ssr8wzks7cj5449yqx9mtkhr0000gn/T/tmpvcfqjzpq/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.20029, step = 1\n",
      "INFO:tensorflow:global_step/sec: 242.806\n",
      "INFO:tensorflow:loss = 0.634493, step = 101 (0.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.097\n",
      "INFO:tensorflow:loss = 0.794609, step = 201 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.202\n",
      "INFO:tensorflow:loss = 0.531353, step = 301 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 285.873\n",
      "INFO:tensorflow:loss = 0.882455, step = 401 (0.350 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.596\n",
      "INFO:tensorflow:loss = 0.402585, step = 501 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.069\n",
      "INFO:tensorflow:loss = 0.730458, step = 601 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.971\n",
      "INFO:tensorflow:loss = 0.564658, step = 701 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.558\n",
      "INFO:tensorflow:loss = 0.261175, step = 801 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.032\n",
      "INFO:tensorflow:loss = 0.935764, step = 901 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.747\n",
      "INFO:tensorflow:loss = 0.618632, step = 1001 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.908\n",
      "INFO:tensorflow:loss = 0.392167, step = 1101 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.33\n",
      "INFO:tensorflow:loss = 0.339281, step = 1201 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.888\n",
      "INFO:tensorflow:loss = 0.762876, step = 1301 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.438\n",
      "INFO:tensorflow:loss = 0.903011, step = 1401 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.662\n",
      "INFO:tensorflow:loss = 0.893767, step = 1501 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.423\n",
      "INFO:tensorflow:loss = 0.703927, step = 1601 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.913\n",
      "INFO:tensorflow:loss = 0.601089, step = 1701 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.634\n",
      "INFO:tensorflow:loss = 0.691612, step = 1801 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.737\n",
      "INFO:tensorflow:loss = 0.423075, step = 1901 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.575\n",
      "INFO:tensorflow:loss = 0.670395, step = 2001 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.393\n",
      "INFO:tensorflow:loss = 0.7576, step = 2101 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.597\n",
      "INFO:tensorflow:loss = 0.369998, step = 2201 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.051\n",
      "INFO:tensorflow:loss = 0.376613, step = 2301 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.596\n",
      "INFO:tensorflow:loss = 0.36306, step = 2401 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.716\n",
      "INFO:tensorflow:loss = 1.15353, step = 2501 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.954\n",
      "INFO:tensorflow:loss = 0.863841, step = 2601 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.481\n",
      "INFO:tensorflow:loss = 0.657408, step = 2701 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.605\n",
      "INFO:tensorflow:loss = 0.50918, step = 2801 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.92\n",
      "INFO:tensorflow:loss = 0.571004, step = 2901 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.931\n",
      "INFO:tensorflow:loss = 0.607291, step = 3001 (0.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.634\n",
      "INFO:tensorflow:loss = 0.63229, step = 3101 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.886\n",
      "INFO:tensorflow:loss = 0.358101, step = 3201 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.093\n",
      "INFO:tensorflow:loss = 0.831905, step = 3301 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.237\n",
      "INFO:tensorflow:loss = 0.623114, step = 3401 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.881\n",
      "INFO:tensorflow:loss = 0.671604, step = 3501 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.665\n",
      "INFO:tensorflow:loss = 0.4456, step = 3601 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.559\n",
      "INFO:tensorflow:loss = 0.899012, step = 3701 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.919\n",
      "INFO:tensorflow:loss = 0.531793, step = 3801 (0.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.424\n",
      "INFO:tensorflow:loss = 0.92481, step = 3901 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.272\n",
      "INFO:tensorflow:loss = 0.586629, step = 4001 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.818\n",
      "INFO:tensorflow:loss = 0.709025, step = 4101 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.177\n",
      "INFO:tensorflow:loss = 0.846979, step = 4201 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.844\n",
      "INFO:tensorflow:loss = 0.894604, step = 4301 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.915\n",
      "INFO:tensorflow:loss = 0.957989, step = 4401 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.328\n",
      "INFO:tensorflow:loss = 0.484263, step = 4501 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 240.424\n",
      "INFO:tensorflow:loss = 0.499331, step = 4601 (0.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.15\n",
      "INFO:tensorflow:loss = 0.46823, step = 4701 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.183\n",
      "INFO:tensorflow:loss = 0.806603, step = 4801 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.861\n",
      "INFO:tensorflow:loss = 0.41631, step = 4901 (0.396 sec)\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:672: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:672: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/models.py:107: mean_squared_error_regressor (from tensorflow.contrib.learn.python.learn.ops.losses_ops) is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Use `tf.contrib.losses.mean_squared_error` and explicit logits computation.\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/ops/losses_ops.py:39: mean_squared_error (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.mean_squared_error instead.\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:539: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-09-02:18:10\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/qm/ssr8wzks7cj5449yqx9mtkhr0000gn/T/tmpvcfqjzpq/model.ckpt-1\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-09-02:18:10\n",
      "INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 17.8152\n",
      "INFO:tensorflow:Validation (step 5000): loss = 17.8152, global_step = 1\n",
      "INFO:tensorflow:global_step/sec: 24.4292\n",
      "INFO:tensorflow:loss = 0.539327, step = 5001 (4.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.137\n",
      "INFO:tensorflow:loss = 0.445459, step = 5101 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.287\n",
      "INFO:tensorflow:loss = 0.664003, step = 5201 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.866\n",
      "INFO:tensorflow:loss = 0.446332, step = 5301 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.603\n",
      "INFO:tensorflow:loss = 0.706117, step = 5401 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.208\n",
      "INFO:tensorflow:loss = 0.573796, step = 5501 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.275\n",
      "INFO:tensorflow:loss = 0.820925, step = 5601 (0.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.121\n",
      "INFO:tensorflow:loss = 1.33767, step = 5701 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.018\n",
      "INFO:tensorflow:loss = 0.693048, step = 5801 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.058\n",
      "INFO:tensorflow:loss = 0.443369, step = 5901 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.252\n",
      "INFO:tensorflow:loss = 0.944375, step = 6001 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.358\n",
      "INFO:tensorflow:loss = 0.48983, step = 6101 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.066\n",
      "INFO:tensorflow:loss = 0.865484, step = 6201 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.16\n",
      "INFO:tensorflow:loss = 0.455932, step = 6301 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.719\n",
      "INFO:tensorflow:loss = 0.756205, step = 6401 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.847\n",
      "INFO:tensorflow:loss = 0.677697, step = 6501 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.588\n",
      "INFO:tensorflow:loss = 0.61419, step = 6601 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.107\n",
      "INFO:tensorflow:loss = 0.741803, step = 6701 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.886\n",
      "INFO:tensorflow:loss = 0.245587, step = 6801 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.921\n",
      "INFO:tensorflow:loss = 0.794335, step = 6901 (0.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 283.646\n",
      "INFO:tensorflow:loss = 0.630248, step = 7001 (0.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.862\n",
      "INFO:tensorflow:loss = 0.585544, step = 7101 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.786\n",
      "INFO:tensorflow:loss = 0.648437, step = 7201 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.068\n",
      "INFO:tensorflow:loss = 0.811347, step = 7301 (0.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.372\n",
      "INFO:tensorflow:loss = 0.429538, step = 7401 (0.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.251\n",
      "INFO:tensorflow:loss = 0.687389, step = 7501 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.051\n",
      "INFO:tensorflow:loss = 0.453958, step = 7601 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.45\n",
      "INFO:tensorflow:loss = 0.391379, step = 7701 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.793\n",
      "INFO:tensorflow:loss = 0.885308, step = 7801 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.619\n",
      "INFO:tensorflow:loss = 1.14757, step = 7901 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.094\n",
      "INFO:tensorflow:loss = 0.547967, step = 8001 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.036\n",
      "INFO:tensorflow:loss = 0.591914, step = 8101 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.163\n",
      "INFO:tensorflow:loss = 0.871545, step = 8201 (0.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.724\n",
      "INFO:tensorflow:loss = 0.476875, step = 8301 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.019\n",
      "INFO:tensorflow:loss = 0.298057, step = 8401 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.552\n",
      "INFO:tensorflow:loss = 0.622239, step = 8501 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.998\n",
      "INFO:tensorflow:loss = 0.748647, step = 8601 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 287.982\n",
      "INFO:tensorflow:loss = 0.457412, step = 8701 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.071\n",
      "INFO:tensorflow:loss = 0.50688, step = 8801 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.953\n",
      "INFO:tensorflow:loss = 0.752267, step = 8901 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.828\n",
      "INFO:tensorflow:loss = 0.71785, step = 9001 (0.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.038\n",
      "INFO:tensorflow:loss = 0.558748, step = 9101 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.231\n",
      "INFO:tensorflow:loss = 0.86678, step = 9201 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.501\n",
      "INFO:tensorflow:loss = 0.497924, step = 9301 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.938\n",
      "INFO:tensorflow:loss = 0.717127, step = 9401 (0.356 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.467\n",
      "INFO:tensorflow:loss = 0.748605, step = 9501 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.485\n",
      "INFO:tensorflow:loss = 0.453119, step = 9601 (0.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 276.984\n",
      "INFO:tensorflow:loss = 0.379547, step = 9701 (0.361 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.359\n",
      "INFO:tensorflow:loss = 0.677961, step = 9801 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.702\n",
      "INFO:tensorflow:loss = 0.587846, step = 9901 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.094\n",
      "INFO:tensorflow:loss = 0.233318, step = 10001 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.358\n",
      "INFO:tensorflow:loss = 0.921972, step = 10101 (0.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.128\n",
      "INFO:tensorflow:loss = 0.957448, step = 10201 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.478\n",
      "INFO:tensorflow:loss = 0.528104, step = 10301 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.054\n",
      "INFO:tensorflow:loss = 0.535128, step = 10401 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.294\n",
      "INFO:tensorflow:loss = 0.608208, step = 10501 (0.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.669\n",
      "INFO:tensorflow:loss = 0.480131, step = 10601 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.291\n",
      "INFO:tensorflow:loss = 0.950955, step = 10701 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.448\n",
      "INFO:tensorflow:loss = 0.616785, step = 10801 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.858\n",
      "INFO:tensorflow:loss = 0.648309, step = 10901 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.299\n",
      "INFO:tensorflow:loss = 0.389896, step = 11001 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.829\n",
      "INFO:tensorflow:loss = 0.328843, step = 11101 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.556\n",
      "INFO:tensorflow:loss = 0.724248, step = 11201 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.048\n",
      "INFO:tensorflow:loss = 0.653498, step = 11301 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.909\n",
      "INFO:tensorflow:loss = 0.825675, step = 11401 (0.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.076\n",
      "INFO:tensorflow:loss = 0.426016, step = 11501 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.38\n",
      "INFO:tensorflow:loss = 0.420845, step = 11601 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.346\n",
      "INFO:tensorflow:loss = 0.40524, step = 11701 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.944\n",
      "INFO:tensorflow:loss = 0.560979, step = 11801 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.389\n",
      "INFO:tensorflow:loss = 1.10156, step = 11901 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.339\n",
      "INFO:tensorflow:loss = 0.610403, step = 12001 (0.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.231\n",
      "INFO:tensorflow:loss = 0.487315, step = 12101 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.672\n",
      "INFO:tensorflow:loss = 0.297561, step = 12201 (0.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.066\n",
      "INFO:tensorflow:loss = 0.26451, step = 12301 (0.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.279\n",
      "INFO:tensorflow:loss = 0.514751, step = 12401 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.699\n",
      "INFO:tensorflow:loss = 1.01508, step = 12501 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5776, step = 12601 (0.500 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.113\n",
      "INFO:tensorflow:loss = 1.0246, step = 12701 (0.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.194\n",
      "INFO:tensorflow:loss = 0.456644, step = 12801 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.622\n",
      "INFO:tensorflow:loss = 0.497331, step = 12901 (0.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.276\n",
      "INFO:tensorflow:loss = 0.655147, step = 13001 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.209\n",
      "INFO:tensorflow:loss = 0.53614, step = 13101 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.089\n",
      "INFO:tensorflow:loss = 0.40142, step = 13201 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.75\n",
      "INFO:tensorflow:loss = 0.978905, step = 13301 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.778\n",
      "INFO:tensorflow:loss = 0.926362, step = 13401 (0.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.498\n",
      "INFO:tensorflow:loss = 0.486307, step = 13501 (0.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.988\n",
      "INFO:tensorflow:loss = 0.758971, step = 13601 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.417\n",
      "INFO:tensorflow:loss = 0.425865, step = 13701 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.386\n",
      "INFO:tensorflow:loss = 0.888622, step = 13801 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.435\n",
      "INFO:tensorflow:loss = 0.518478, step = 13901 (0.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.361\n",
      "INFO:tensorflow:loss = 0.806392, step = 14001 (0.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.997\n",
      "INFO:tensorflow:loss = 0.444155, step = 14101 (0.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.92\n",
      "INFO:tensorflow:loss = 0.511312, step = 14201 (0.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.07\n",
      "INFO:tensorflow:loss = 0.529338, step = 14301 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.897\n",
      "INFO:tensorflow:loss = 0.304578, step = 14401 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.689\n",
      "INFO:tensorflow:loss = 1.07711, step = 14501 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.827\n",
      "INFO:tensorflow:loss = 0.826077, step = 14601 (0.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.703\n",
      "INFO:tensorflow:loss = 0.886628, step = 14701 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.69\n",
      "INFO:tensorflow:loss = 0.474204, step = 14801 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.74\n",
      "INFO:tensorflow:loss = 0.983287, step = 14901 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.926\n",
      "INFO:tensorflow:loss = 0.678643, step = 15001 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.075\n",
      "INFO:tensorflow:loss = 0.44435, step = 15101 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.913\n",
      "INFO:tensorflow:loss = 0.540897, step = 15201 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.482\n",
      "INFO:tensorflow:loss = 0.5575, step = 15301 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.112\n",
      "INFO:tensorflow:loss = 0.682868, step = 15401 (0.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.619\n",
      "INFO:tensorflow:loss = 0.725334, step = 15501 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.665\n",
      "INFO:tensorflow:loss = 0.573505, step = 15601 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.775\n",
      "INFO:tensorflow:loss = 0.291524, step = 15701 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.978\n",
      "INFO:tensorflow:loss = 0.380309, step = 15801 (0.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.468\n",
      "INFO:tensorflow:loss = 0.725597, step = 15901 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.039\n",
      "INFO:tensorflow:loss = 0.565179, step = 16001 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.24\n",
      "INFO:tensorflow:loss = 0.249719, step = 16101 (0.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.024\n",
      "INFO:tensorflow:loss = 0.342697, step = 16201 (0.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.611\n",
      "INFO:tensorflow:loss = 0.732152, step = 16301 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.421\n",
      "INFO:tensorflow:loss = 0.279736, step = 16401 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.054\n",
      "INFO:tensorflow:loss = 0.987983, step = 16501 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.63\n",
      "INFO:tensorflow:loss = 0.589721, step = 16601 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.568\n",
      "INFO:tensorflow:loss = 0.975084, step = 16701 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.874\n",
      "INFO:tensorflow:loss = 0.450509, step = 16801 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.061\n",
      "INFO:tensorflow:loss = 0.70732, step = 16901 (0.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.213\n",
      "INFO:tensorflow:loss = 0.488507, step = 17001 (0.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.571\n",
      "INFO:tensorflow:loss = 0.562314, step = 17101 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.124\n",
      "INFO:tensorflow:loss = 0.461837, step = 17201 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.883\n",
      "INFO:tensorflow:loss = 0.414959, step = 17301 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.229\n",
      "INFO:tensorflow:loss = 1.14503, step = 17401 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.741\n",
      "INFO:tensorflow:loss = 0.49899, step = 17501 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.349\n",
      "INFO:tensorflow:loss = 0.400517, step = 17601 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.42\n",
      "INFO:tensorflow:loss = 0.717369, step = 17701 (0.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 240.699\n",
      "INFO:tensorflow:loss = 0.663343, step = 17801 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.985\n",
      "INFO:tensorflow:loss = 0.275677, step = 17901 (0.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.677\n",
      "INFO:tensorflow:loss = 0.32483, step = 18001 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.807\n",
      "INFO:tensorflow:loss = 0.969298, step = 18101 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.66\n",
      "INFO:tensorflow:loss = 0.824306, step = 18201 (0.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.605\n",
      "INFO:tensorflow:loss = 0.540939, step = 18301 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.043\n",
      "INFO:tensorflow:loss = 0.990717, step = 18401 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.504\n",
      "INFO:tensorflow:loss = 0.673513, step = 18501 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.13\n",
      "INFO:tensorflow:loss = 0.593887, step = 18601 (0.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.242\n",
      "INFO:tensorflow:loss = 0.772664, step = 18701 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.307\n",
      "INFO:tensorflow:loss = 0.944419, step = 18801 (0.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.434\n",
      "INFO:tensorflow:loss = 0.608597, step = 18901 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.915\n",
      "INFO:tensorflow:loss = 0.896861, step = 19001 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.782\n",
      "INFO:tensorflow:loss = 1.23771, step = 19101 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.833\n",
      "INFO:tensorflow:loss = 0.704991, step = 19201 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.15\n",
      "INFO:tensorflow:loss = 0.603997, step = 19301 (0.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.881\n",
      "INFO:tensorflow:loss = 0.940714, step = 19401 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.031\n",
      "INFO:tensorflow:loss = 0.70816, step = 19501 (0.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.918\n",
      "INFO:tensorflow:loss = 0.578602, step = 19601 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.64\n",
      "INFO:tensorflow:loss = 0.503385, step = 19701 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.206\n",
      "INFO:tensorflow:loss = 0.95931, step = 19801 (0.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.886\n",
      "INFO:tensorflow:loss = 0.927418, step = 19901 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.168\n",
      "INFO:tensorflow:loss = 0.235855, step = 20001 (0.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.747\n",
      "INFO:tensorflow:loss = 0.717013, step = 20101 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.233\n",
      "INFO:tensorflow:loss = 0.490512, step = 20201 (0.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 240.559\n",
      "INFO:tensorflow:loss = 0.502953, step = 20301 (0.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.564\n",
      "INFO:tensorflow:loss = 0.70325, step = 20401 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.733\n",
      "INFO:tensorflow:loss = 0.709616, step = 20501 (0.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.006\n",
      "INFO:tensorflow:loss = 0.297041, step = 20601 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.278\n",
      "INFO:tensorflow:loss = 0.44801, step = 20701 (0.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.601\n",
      "INFO:tensorflow:loss = 0.425846, step = 20801 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.091\n",
      "INFO:tensorflow:loss = 0.735123, step = 20901 (0.374 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 266.577\n",
      "INFO:tensorflow:loss = 0.676544, step = 21001 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.5\n",
      "INFO:tensorflow:loss = 0.694752, step = 21101 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.527\n",
      "INFO:tensorflow:loss = 1.31003, step = 21201 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.056\n",
      "INFO:tensorflow:loss = 0.394899, step = 21301 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.077\n",
      "INFO:tensorflow:loss = 0.382128, step = 21401 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.806\n",
      "INFO:tensorflow:loss = 0.484739, step = 21501 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.416\n",
      "INFO:tensorflow:loss = 0.592366, step = 21601 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.114\n",
      "INFO:tensorflow:loss = 0.454789, step = 21701 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 269.795\n",
      "INFO:tensorflow:loss = 0.738466, step = 21801 (0.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.043\n",
      "INFO:tensorflow:loss = 0.652862, step = 21901 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.533\n",
      "INFO:tensorflow:loss = 0.751868, step = 22001 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.09\n",
      "INFO:tensorflow:loss = 0.917146, step = 22101 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.412\n",
      "INFO:tensorflow:loss = 0.515427, step = 22201 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.732\n",
      "INFO:tensorflow:loss = 0.391829, step = 22301 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.228\n",
      "INFO:tensorflow:loss = 0.737421, step = 22401 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.4\n",
      "INFO:tensorflow:loss = 0.641636, step = 22501 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.091\n",
      "INFO:tensorflow:loss = 0.20576, step = 22601 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.376\n",
      "INFO:tensorflow:loss = 0.840316, step = 22701 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.821\n",
      "INFO:tensorflow:loss = 0.544085, step = 22801 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.158\n",
      "INFO:tensorflow:loss = 0.824445, step = 22901 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.643\n",
      "INFO:tensorflow:loss = 0.984955, step = 23001 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.527\n",
      "INFO:tensorflow:loss = 0.842118, step = 23101 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.359\n",
      "INFO:tensorflow:loss = 0.634433, step = 23201 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.039\n",
      "INFO:tensorflow:loss = 0.828387, step = 23301 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.208\n",
      "INFO:tensorflow:loss = 0.874846, step = 23401 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.621\n",
      "INFO:tensorflow:loss = 0.376126, step = 23501 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.152\n",
      "INFO:tensorflow:loss = 0.576311, step = 23601 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.415\n",
      "INFO:tensorflow:loss = 0.57336, step = 23701 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.457\n",
      "INFO:tensorflow:loss = 0.62887, step = 23801 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.529\n",
      "INFO:tensorflow:loss = 0.53902, step = 23901 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.227\n",
      "INFO:tensorflow:loss = 0.874645, step = 24001 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.65\n",
      "INFO:tensorflow:loss = 0.482294, step = 24101 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.493\n",
      "INFO:tensorflow:loss = 0.503136, step = 24201 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.2\n",
      "INFO:tensorflow:loss = 0.805219, step = 24301 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.025\n",
      "INFO:tensorflow:loss = 0.565755, step = 24401 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.411\n",
      "INFO:tensorflow:loss = 0.841308, step = 24501 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.296\n",
      "INFO:tensorflow:loss = 0.480825, step = 24601 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.639\n",
      "INFO:tensorflow:loss = 0.748799, step = 24701 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.96\n",
      "INFO:tensorflow:loss = 0.532326, step = 24801 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.59\n",
      "INFO:tensorflow:loss = 0.866423, step = 24901 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.639\n",
      "INFO:tensorflow:loss = 0.341861, step = 25001 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.822\n",
      "INFO:tensorflow:loss = 0.491706, step = 25101 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.873\n",
      "INFO:tensorflow:loss = 1.13348, step = 25201 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.147\n",
      "INFO:tensorflow:loss = 0.886259, step = 25301 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.385\n",
      "INFO:tensorflow:loss = 0.691072, step = 25401 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.27\n",
      "INFO:tensorflow:loss = 0.638443, step = 25501 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.218\n",
      "INFO:tensorflow:loss = 0.445962, step = 25601 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.251\n",
      "INFO:tensorflow:loss = 0.533742, step = 25701 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.791\n",
      "INFO:tensorflow:loss = 0.584805, step = 25801 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.266\n",
      "INFO:tensorflow:loss = 0.654989, step = 25901 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.174\n",
      "INFO:tensorflow:loss = 0.312089, step = 26001 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.274\n",
      "INFO:tensorflow:loss = 0.53184, step = 26101 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.816\n",
      "INFO:tensorflow:loss = 0.774239, step = 26201 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.197\n",
      "INFO:tensorflow:loss = 1.00241, step = 26301 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.67\n",
      "INFO:tensorflow:loss = 0.995293, step = 26401 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.102\n",
      "INFO:tensorflow:loss = 0.520362, step = 26501 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.674\n",
      "INFO:tensorflow:loss = 0.720238, step = 26601 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.401\n",
      "INFO:tensorflow:loss = 0.851608, step = 26701 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.954\n",
      "INFO:tensorflow:loss = 1.04062, step = 26801 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.681\n",
      "INFO:tensorflow:loss = 0.297446, step = 26901 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.259\n",
      "INFO:tensorflow:loss = 0.602805, step = 27001 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.126\n",
      "INFO:tensorflow:loss = 0.489097, step = 27101 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.17\n",
      "INFO:tensorflow:loss = 0.373939, step = 27201 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.105\n",
      "INFO:tensorflow:loss = 0.395348, step = 27301 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.407\n",
      "INFO:tensorflow:loss = 0.687876, step = 27401 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.524\n",
      "INFO:tensorflow:loss = 0.584498, step = 27501 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.841\n",
      "INFO:tensorflow:loss = 1.10201, step = 27601 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.287\n",
      "INFO:tensorflow:loss = 0.798091, step = 27701 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.083\n",
      "INFO:tensorflow:loss = 0.515655, step = 27801 (0.368 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.186\n",
      "INFO:tensorflow:loss = 0.964311, step = 27901 (0.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.462\n",
      "INFO:tensorflow:loss = 0.867198, step = 28001 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.894\n",
      "INFO:tensorflow:loss = 0.257769, step = 28101 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.096\n",
      "INFO:tensorflow:loss = 0.575552, step = 28201 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.495\n",
      "INFO:tensorflow:loss = 0.649577, step = 28301 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.853\n",
      "INFO:tensorflow:loss = 0.683424, step = 28401 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.625\n",
      "INFO:tensorflow:loss = 1.09012, step = 28501 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.422\n",
      "INFO:tensorflow:loss = 0.316512, step = 28601 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.877\n",
      "INFO:tensorflow:loss = 0.864295, step = 28701 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.696\n",
      "INFO:tensorflow:loss = 0.618016, step = 28801 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.241\n",
      "INFO:tensorflow:loss = 0.545587, step = 28901 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.818\n",
      "INFO:tensorflow:loss = 0.722961, step = 29001 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.498\n",
      "INFO:tensorflow:loss = 0.398977, step = 29101 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.538\n",
      "INFO:tensorflow:loss = 0.462435, step = 29201 (0.372 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 266.121\n",
      "INFO:tensorflow:loss = 1.00564, step = 29301 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.311\n",
      "INFO:tensorflow:loss = 0.447664, step = 29401 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.94\n",
      "INFO:tensorflow:loss = 0.977106, step = 29501 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.232\n",
      "INFO:tensorflow:loss = 0.481638, step = 29601 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.804\n",
      "INFO:tensorflow:loss = 0.546448, step = 29701 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.84\n",
      "INFO:tensorflow:loss = 0.795682, step = 29801 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.521\n",
      "INFO:tensorflow:loss = 0.946715, step = 29901 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.66\n",
      "INFO:tensorflow:loss = 0.628856, step = 30001 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.1\n",
      "INFO:tensorflow:loss = 0.553241, step = 30101 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.488\n",
      "INFO:tensorflow:loss = 0.35084, step = 30201 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.793\n",
      "INFO:tensorflow:loss = 0.797305, step = 30301 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.795\n",
      "INFO:tensorflow:loss = 0.691399, step = 30401 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.331\n",
      "INFO:tensorflow:loss = 0.429939, step = 30501 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.301\n",
      "INFO:tensorflow:loss = 0.685749, step = 30601 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.43\n",
      "INFO:tensorflow:loss = 0.701586, step = 30701 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.609\n",
      "INFO:tensorflow:loss = 0.411135, step = 30801 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.946\n",
      "INFO:tensorflow:loss = 0.592104, step = 30901 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.678\n",
      "INFO:tensorflow:loss = 0.297609, step = 31001 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.969\n",
      "INFO:tensorflow:loss = 0.724576, step = 31101 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.654\n",
      "INFO:tensorflow:loss = 0.332217, step = 31201 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.878\n",
      "INFO:tensorflow:loss = 0.695167, step = 31301 (0.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.566\n",
      "INFO:tensorflow:loss = 0.547675, step = 31401 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.103\n",
      "INFO:tensorflow:loss = 0.543016, step = 31501 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.012\n",
      "INFO:tensorflow:loss = 0.39358, step = 31601 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.712\n",
      "INFO:tensorflow:loss = 0.713911, step = 31701 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.015\n",
      "INFO:tensorflow:loss = 0.675261, step = 31801 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.294\n",
      "INFO:tensorflow:loss = 0.80127, step = 31901 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.221\n",
      "INFO:tensorflow:loss = 0.79929, step = 32001 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.187\n",
      "INFO:tensorflow:loss = 1.01943, step = 32101 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.177\n",
      "INFO:tensorflow:loss = 0.834899, step = 32201 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.315\n",
      "INFO:tensorflow:loss = 1.03043, step = 32301 (0.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.276\n",
      "INFO:tensorflow:loss = 0.597382, step = 32401 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.122\n",
      "INFO:tensorflow:loss = 0.71658, step = 32501 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.492\n",
      "INFO:tensorflow:loss = 0.935766, step = 32601 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.224\n",
      "INFO:tensorflow:loss = 0.58179, step = 32701 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.06\n",
      "INFO:tensorflow:loss = 1.31772, step = 32801 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.081\n",
      "INFO:tensorflow:loss = 0.740317, step = 32901 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.649\n",
      "INFO:tensorflow:loss = 0.864968, step = 33001 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.041\n",
      "INFO:tensorflow:loss = 0.80702, step = 33101 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.026\n",
      "INFO:tensorflow:loss = 0.837636, step = 33201 (0.385 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.243\n",
      "INFO:tensorflow:loss = 0.615516, step = 33301 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.974\n",
      "INFO:tensorflow:loss = 0.370564, step = 33401 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.58\n",
      "INFO:tensorflow:loss = 0.506972, step = 33501 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.622\n",
      "INFO:tensorflow:loss = 1.10285, step = 33601 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.878\n",
      "INFO:tensorflow:loss = 1.12533, step = 33701 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.102\n",
      "INFO:tensorflow:loss = 1.09189, step = 33801 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.491\n",
      "INFO:tensorflow:loss = 0.5155, step = 33901 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.39\n",
      "INFO:tensorflow:loss = 0.501443, step = 34001 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.206\n",
      "INFO:tensorflow:loss = 0.554871, step = 34101 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.895\n",
      "INFO:tensorflow:loss = 0.472005, step = 34201 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.788\n",
      "INFO:tensorflow:loss = 0.451364, step = 34301 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.408\n",
      "INFO:tensorflow:loss = 0.60916, step = 34401 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.912\n",
      "INFO:tensorflow:loss = 0.833562, step = 34501 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.013\n",
      "INFO:tensorflow:loss = 0.468628, step = 34601 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.746\n",
      "INFO:tensorflow:loss = 0.30959, step = 34701 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.615\n",
      "INFO:tensorflow:loss = 0.729588, step = 34801 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.647\n",
      "INFO:tensorflow:loss = 0.653441, step = 34901 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.485\n",
      "INFO:tensorflow:loss = 0.657258, step = 35001 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.844\n",
      "INFO:tensorflow:loss = 0.702471, step = 35101 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.66\n",
      "INFO:tensorflow:loss = 0.469284, step = 35201 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.165\n",
      "INFO:tensorflow:loss = 1.03079, step = 35301 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.895\n",
      "INFO:tensorflow:loss = 0.421543, step = 35401 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.35\n",
      "INFO:tensorflow:loss = 0.949965, step = 35501 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.386\n",
      "INFO:tensorflow:loss = 0.812652, step = 35601 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 276.027\n",
      "INFO:tensorflow:loss = 0.794198, step = 35701 (0.362 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.788\n",
      "INFO:tensorflow:loss = 0.998489, step = 35801 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.282\n",
      "INFO:tensorflow:loss = 0.571925, step = 35901 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.883\n",
      "INFO:tensorflow:loss = 0.51182, step = 36001 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.331\n",
      "INFO:tensorflow:loss = 0.484196, step = 36101 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.937\n",
      "INFO:tensorflow:loss = 0.654828, step = 36201 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.473\n",
      "INFO:tensorflow:loss = 0.55706, step = 36301 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.12\n",
      "INFO:tensorflow:loss = 1.18123, step = 36401 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.141\n",
      "INFO:tensorflow:loss = 0.549513, step = 36501 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.018\n",
      "INFO:tensorflow:loss = 0.873267, step = 36601 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.59\n",
      "INFO:tensorflow:loss = 0.497381, step = 36701 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.866\n",
      "INFO:tensorflow:loss = 0.5363, step = 36801 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.117\n",
      "INFO:tensorflow:loss = 0.553964, step = 36901 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.447\n",
      "INFO:tensorflow:loss = 0.429669, step = 37001 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.667\n",
      "INFO:tensorflow:loss = 0.412592, step = 37101 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.617\n",
      "INFO:tensorflow:loss = 0.682772, step = 37201 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.595\n",
      "INFO:tensorflow:loss = 0.765527, step = 37301 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.494\n",
      "INFO:tensorflow:loss = 0.568392, step = 37401 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.827\n",
      "INFO:tensorflow:loss = 0.8389, step = 37501 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.514687, step = 37601 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.23\n",
      "INFO:tensorflow:loss = 0.622732, step = 37701 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.653\n",
      "INFO:tensorflow:loss = 0.473422, step = 37801 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.751\n",
      "INFO:tensorflow:loss = 0.61438, step = 37901 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.605\n",
      "INFO:tensorflow:loss = 0.36326, step = 38001 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.066\n",
      "INFO:tensorflow:loss = 0.612443, step = 38101 (0.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.046\n",
      "INFO:tensorflow:loss = 0.403074, step = 38201 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.093\n",
      "INFO:tensorflow:loss = 0.848704, step = 38301 (0.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.842\n",
      "INFO:tensorflow:loss = 0.804724, step = 38401 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.278\n",
      "INFO:tensorflow:loss = 0.714613, step = 38501 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.235\n",
      "INFO:tensorflow:loss = 0.405334, step = 38601 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.244\n",
      "INFO:tensorflow:loss = 0.6637, step = 38701 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.974\n",
      "INFO:tensorflow:loss = 0.713414, step = 38801 (0.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.471\n",
      "INFO:tensorflow:loss = 0.523435, step = 38901 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.367\n",
      "INFO:tensorflow:loss = 0.37973, step = 39001 (0.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.502\n",
      "INFO:tensorflow:loss = 0.40357, step = 39101 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 269.496\n",
      "INFO:tensorflow:loss = 0.649589, step = 39201 (0.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.094\n",
      "INFO:tensorflow:loss = 0.628142, step = 39301 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.033\n",
      "INFO:tensorflow:loss = 0.601779, step = 39401 (0.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.959\n",
      "INFO:tensorflow:loss = 0.71571, step = 39501 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.726\n",
      "INFO:tensorflow:loss = 0.971292, step = 39601 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.656\n",
      "INFO:tensorflow:loss = 0.673674, step = 39701 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.801\n",
      "INFO:tensorflow:loss = 0.71984, step = 39801 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.508\n",
      "INFO:tensorflow:loss = 0.530446, step = 39901 (0.385 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.714\n",
      "INFO:tensorflow:loss = 0.499992, step = 40001 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.707\n",
      "INFO:tensorflow:loss = 0.48019, step = 40101 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 287.647\n",
      "INFO:tensorflow:loss = 0.392777, step = 40201 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.28\n",
      "INFO:tensorflow:loss = 0.402216, step = 40301 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.723\n",
      "INFO:tensorflow:loss = 0.83067, step = 40401 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.942\n",
      "INFO:tensorflow:loss = 0.767846, step = 40501 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.133\n",
      "INFO:tensorflow:loss = 0.769139, step = 40601 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.769\n",
      "INFO:tensorflow:loss = 0.896661, step = 40701 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.36\n",
      "INFO:tensorflow:loss = 0.633213, step = 40801 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.967\n",
      "INFO:tensorflow:loss = 0.724021, step = 40901 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 277.818\n",
      "INFO:tensorflow:loss = 0.864636, step = 41001 (0.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.013\n",
      "INFO:tensorflow:loss = 1.1133, step = 41101 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.099\n",
      "INFO:tensorflow:loss = 0.46481, step = 41201 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.788\n",
      "INFO:tensorflow:loss = 0.577334, step = 41301 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.116\n",
      "INFO:tensorflow:loss = 0.571892, step = 41401 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.129\n",
      "INFO:tensorflow:loss = 0.701851, step = 41501 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.08\n",
      "INFO:tensorflow:loss = 0.582707, step = 41601 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.988\n",
      "INFO:tensorflow:loss = 0.350977, step = 41701 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.18\n",
      "INFO:tensorflow:loss = 0.558879, step = 41801 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.032\n",
      "INFO:tensorflow:loss = 0.824966, step = 41901 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.629\n",
      "INFO:tensorflow:loss = 0.468124, step = 42001 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.958\n",
      "INFO:tensorflow:loss = 0.448199, step = 42101 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.407\n",
      "INFO:tensorflow:loss = 0.470391, step = 42201 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.998\n",
      "INFO:tensorflow:loss = 0.516034, step = 42301 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.74\n",
      "INFO:tensorflow:loss = 0.587549, step = 42401 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.763\n",
      "INFO:tensorflow:loss = 0.62815, step = 42501 (0.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.541\n",
      "INFO:tensorflow:loss = 0.696157, step = 42601 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.474\n",
      "INFO:tensorflow:loss = 0.522895, step = 42701 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.343\n",
      "INFO:tensorflow:loss = 0.903896, step = 42801 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.103\n",
      "INFO:tensorflow:loss = 0.559054, step = 42901 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.102\n",
      "INFO:tensorflow:loss = 0.481175, step = 43001 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.229\n",
      "INFO:tensorflow:loss = 0.63535, step = 43101 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 270.229\n",
      "INFO:tensorflow:loss = 0.660069, step = 43201 (0.370 sec)\n",
      "INFO:tensorflow:global_step/sec: 297.394\n",
      "INFO:tensorflow:loss = 0.747956, step = 43301 (0.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.604\n",
      "INFO:tensorflow:loss = 0.605718, step = 43401 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.475\n",
      "INFO:tensorflow:loss = 0.877416, step = 43501 (0.334 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.605\n",
      "INFO:tensorflow:loss = 0.905716, step = 43601 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.204\n",
      "INFO:tensorflow:loss = 0.304408, step = 43701 (0.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.272\n",
      "INFO:tensorflow:loss = 0.550689, step = 43801 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.979\n",
      "INFO:tensorflow:loss = 0.527352, step = 43901 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.293\n",
      "INFO:tensorflow:loss = 0.74869, step = 44001 (0.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.816\n",
      "INFO:tensorflow:loss = 0.461304, step = 44101 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.564\n",
      "INFO:tensorflow:loss = 0.731764, step = 44201 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.696\n",
      "INFO:tensorflow:loss = 1.07987, step = 44301 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.239\n",
      "INFO:tensorflow:loss = 0.780768, step = 44401 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.779\n",
      "INFO:tensorflow:loss = 0.546735, step = 44501 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.771\n",
      "INFO:tensorflow:loss = 0.538616, step = 44601 (0.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.027\n",
      "INFO:tensorflow:loss = 0.5062, step = 44701 (0.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 270.981\n",
      "INFO:tensorflow:loss = 1.06985, step = 44801 (0.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.594\n",
      "INFO:tensorflow:loss = 0.617039, step = 44901 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 290.583\n",
      "INFO:tensorflow:loss = 0.657063, step = 45001 (0.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.001\n",
      "INFO:tensorflow:loss = 0.502489, step = 45101 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.286\n",
      "INFO:tensorflow:loss = 0.685097, step = 45201 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.941\n",
      "INFO:tensorflow:loss = 0.722832, step = 45301 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.681\n",
      "INFO:tensorflow:loss = 0.479304, step = 45401 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.728\n",
      "INFO:tensorflow:loss = 0.401686, step = 45501 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.629\n",
      "INFO:tensorflow:loss = 0.609591, step = 45601 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.777\n",
      "INFO:tensorflow:loss = 0.704766, step = 45701 (0.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 274.637\n",
      "INFO:tensorflow:loss = 0.846691, step = 45801 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.124\n",
      "INFO:tensorflow:loss = 0.587964, step = 45901 (0.366 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 328.795\n",
      "INFO:tensorflow:loss = 0.375162, step = 46001 (0.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.951\n",
      "INFO:tensorflow:loss = 0.787092, step = 46101 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.427\n",
      "INFO:tensorflow:loss = 0.470732, step = 46201 (0.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.812\n",
      "INFO:tensorflow:loss = 0.681167, step = 46301 (0.385 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.485\n",
      "INFO:tensorflow:loss = 0.678334, step = 46401 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.626\n",
      "INFO:tensorflow:loss = 0.685659, step = 46501 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.358\n",
      "INFO:tensorflow:loss = 0.459774, step = 46601 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.544\n",
      "INFO:tensorflow:loss = 0.409014, step = 46701 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.099\n",
      "INFO:tensorflow:loss = 0.363749, step = 46801 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.871\n",
      "INFO:tensorflow:loss = 0.680546, step = 46901 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.161\n",
      "INFO:tensorflow:loss = 0.428493, step = 47001 (0.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.818\n",
      "INFO:tensorflow:loss = 0.790897, step = 47101 (0.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.224\n",
      "INFO:tensorflow:loss = 0.800793, step = 47201 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.546\n",
      "INFO:tensorflow:loss = 0.747838, step = 47301 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.353\n",
      "INFO:tensorflow:loss = 0.481833, step = 47401 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.747\n",
      "INFO:tensorflow:loss = 0.60344, step = 47501 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.832\n",
      "INFO:tensorflow:loss = 0.796544, step = 47601 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.946\n",
      "INFO:tensorflow:loss = 0.673408, step = 47701 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.868\n",
      "INFO:tensorflow:loss = 0.441913, step = 47801 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.351\n",
      "INFO:tensorflow:loss = 0.752666, step = 47901 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.173\n",
      "INFO:tensorflow:loss = 0.71874, step = 48001 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.592\n",
      "INFO:tensorflow:loss = 0.737265, step = 48101 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.227\n",
      "INFO:tensorflow:loss = 0.830739, step = 48201 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.054\n",
      "INFO:tensorflow:loss = 0.653728, step = 48301 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.482\n",
      "INFO:tensorflow:loss = 0.551108, step = 48401 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.022\n",
      "INFO:tensorflow:loss = 0.537408, step = 48501 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.294\n",
      "INFO:tensorflow:loss = 0.381344, step = 48601 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.598\n",
      "INFO:tensorflow:loss = 0.722487, step = 48701 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.942\n",
      "INFO:tensorflow:loss = 0.640329, step = 48801 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 274.531\n",
      "INFO:tensorflow:loss = 0.623994, step = 48901 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.949\n",
      "INFO:tensorflow:loss = 0.506109, step = 49001 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.215\n",
      "INFO:tensorflow:loss = 0.670168, step = 49101 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.824\n",
      "INFO:tensorflow:loss = 0.401809, step = 49201 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.427\n",
      "INFO:tensorflow:loss = 0.783012, step = 49301 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.724\n",
      "INFO:tensorflow:loss = 0.712566, step = 49401 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.261\n",
      "INFO:tensorflow:loss = 0.632006, step = 49501 (0.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.781\n",
      "INFO:tensorflow:loss = 0.878442, step = 49601 (0.365 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.889\n",
      "INFO:tensorflow:loss = 0.779797, step = 49701 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.844\n",
      "INFO:tensorflow:loss = 0.628557, step = 49801 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.251\n",
      "INFO:tensorflow:loss = 0.627618, step = 49901 (0.395 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 50000 into /var/folders/qm/ssr8wzks7cj5449yqx9mtkhr0000gn/T/tmpvcfqjzpq/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.430092.\n",
      "TEST:\n",
      "[[[  5.89078865e+01   2.58108406e+01  -2.31308174e+00 ...,   1.07547512e+01\n",
      "     4.20869350e+00   9.28883702e-02]\n",
      "  [  5.12183533e+01   2.19637356e+01  -1.07148228e+01 ...,   1.58565788e+01\n",
      "     3.65461802e+00   1.71633825e-01]\n",
      "  [  5.53623390e+01   2.02520847e+01  -9.60559940e+00 ...,   1.03077297e+01\n",
      "     4.96064520e+00   1.31771219e+00]\n",
      "  [  6.72863235e+01   3.36393967e+01  -2.16457272e+00 ...,   5.40230036e+00\n",
      "     1.49767029e+00   1.04749095e+00]\n",
      "  [  7.96231689e+01   5.13166466e+01   2.45462399e+01 ...,   9.99637902e-01\n",
      "     7.51560271e-01   3.93629551e+00]]\n",
      "\n",
      " [[  5.12183533e+01   2.19637356e+01  -1.07148228e+01 ...,   1.58565788e+01\n",
      "     3.65461802e+00   1.71633825e-01]\n",
      "  [  5.53623390e+01   2.02520847e+01  -9.60559940e+00 ...,   1.03077297e+01\n",
      "     4.96064520e+00   1.31771219e+00]\n",
      "  [  6.72863235e+01   3.36393967e+01  -2.16457272e+00 ...,   5.40230036e+00\n",
      "     1.49767029e+00   1.04749095e+00]\n",
      "  [  7.96231689e+01   5.13166466e+01   2.45462399e+01 ...,   9.99637902e-01\n",
      "     7.51560271e-01   3.93629551e+00]\n",
      "  [  8.83500519e+01   6.27725983e+01   3.63391037e+01 ...,   2.07769454e-01\n",
      "     1.46373224e+00   5.89974785e+00]]\n",
      "\n",
      " [[  5.53623390e+01   2.02520847e+01  -9.60559940e+00 ...,   1.03077297e+01\n",
      "     4.96064520e+00   1.31771219e+00]\n",
      "  [  6.72863235e+01   3.36393967e+01  -2.16457272e+00 ...,   5.40230036e+00\n",
      "     1.49767029e+00   1.04749095e+00]\n",
      "  [  7.96231689e+01   5.13166466e+01   2.45462399e+01 ...,   9.99637902e-01\n",
      "     7.51560271e-01   3.93629551e+00]\n",
      "  [  8.83500519e+01   6.27725983e+01   3.63391037e+01 ...,   2.07769454e-01\n",
      "     1.46373224e+00   5.89974785e+00]\n",
      "  [  8.91384659e+01   7.10007248e+01   4.91666336e+01 ...,   0.00000000e+00\n",
      "     4.30105114e+00   8.60881329e+00]]\n",
      "\n",
      " ..., \n",
      " [[  8.68227615e+01   6.13170166e+01   3.71236725e+01 ...,   0.00000000e+00\n",
      "     2.16234612e+00   6.82728100e+00]\n",
      "  [  9.34361572e+01   7.26695557e+01   4.97181129e+01 ...,   0.00000000e+00\n",
      "     1.86610115e+00   4.33830547e+00]\n",
      "  [  9.24334641e+01   7.41954956e+01   5.39682465e+01 ...,   0.00000000e+00\n",
      "     4.76597261e+00   9.19370461e+00]\n",
      "  [  8.97716141e+01   7.39761353e+01   5.31527863e+01 ...,   0.00000000e+00\n",
      "     3.44026613e+00   7.74664259e+00]\n",
      "  [  9.05397797e+01   6.85264206e+01   4.77807999e+01 ...,   0.00000000e+00\n",
      "     3.30768657e+00   4.11556864e+00]]\n",
      "\n",
      " [[  9.34361572e+01   7.26695557e+01   4.97181129e+01 ...,   0.00000000e+00\n",
      "     1.86610115e+00   4.33830547e+00]\n",
      "  [  9.24334641e+01   7.41954956e+01   5.39682465e+01 ...,   0.00000000e+00\n",
      "     4.76597261e+00   9.19370461e+00]\n",
      "  [  8.97716141e+01   7.39761353e+01   5.31527863e+01 ...,   0.00000000e+00\n",
      "     3.44026613e+00   7.74664259e+00]\n",
      "  [  9.05397797e+01   6.85264206e+01   4.77807999e+01 ...,   0.00000000e+00\n",
      "     3.30768657e+00   4.11556864e+00]\n",
      "  [  8.34015274e+01   5.80128479e+01   3.34319153e+01 ...,   2.22736485e-02\n",
      "     8.72271478e-01   3.45209908e+00]]\n",
      "\n",
      " [[  9.24334641e+01   7.41954956e+01   5.39682465e+01 ...,   0.00000000e+00\n",
      "     4.76597261e+00   9.19370461e+00]\n",
      "  [  8.97716141e+01   7.39761353e+01   5.31527863e+01 ...,   0.00000000e+00\n",
      "     3.44026613e+00   7.74664259e+00]\n",
      "  [  9.05397797e+01   6.85264206e+01   4.77807999e+01 ...,   0.00000000e+00\n",
      "     3.30768657e+00   4.11556864e+00]\n",
      "  [  8.34015274e+01   5.80128479e+01   3.34319153e+01 ...,   2.22736485e-02\n",
      "     8.72271478e-01   3.45209908e+00]\n",
      "  [  7.63235855e+01   4.45345573e+01   1.65089493e+01 ...,   1.08339453e+00\n",
      "     3.19852948e+00   1.92999196e+00]]]\n",
      "[[ 4.76881742]\n",
      " [ 4.67883968]\n",
      " [ 4.1189785 ]\n",
      " [ 3.7390728 ]\n",
      " [ 3.50912976]\n",
      " [ 3.57911229]\n",
      " [ 3.62910008]\n",
      " [ 3.80905533]\n",
      " [ 3.81905293]\n",
      " [ 3.77906275]\n",
      " [ 3.78906035]\n",
      " [ 3.70908022]\n",
      " [ 3.61910248]\n",
      " [ 3.64909506]\n",
      " [ 3.84904552]\n",
      " [ 3.62910008]\n",
      " [ 3.68908501]\n",
      " [ 3.69908261]\n",
      " [ 3.69908261]\n",
      " [ 3.78906035]\n",
      " [ 3.72907519]\n",
      " [ 3.62910008]\n",
      " [ 3.66909003]\n",
      " [ 3.66909003]\n",
      " [ 3.81905293]\n",
      " [ 3.91902804]\n",
      " [ 3.62910008]\n",
      " [ 3.16921401]\n",
      " [ 3.26918936]\n",
      " [ 3.2991817 ]\n",
      " [ 3.31917691]\n",
      " [ 3.46913958]]\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/models.py:107: mean_squared_error_regressor (from tensorflow.contrib.learn.python.learn.ops.losses_ops) is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Use `tf.contrib.losses.mean_squared_error` and explicit logits computation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/ops/losses_ops.py:39: mean_squared_error (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.mean_squared_error instead.\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:539: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From //anaconda/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/qm/ssr8wzks7cj5449yqx9mtkhr0000gn/T/tmpvcfqjzpq/model.ckpt-50000\n",
      "MSE: 1.261535\n"
     ]
    }
   ],
   "source": [
    "#regressor.fit(X['train'], y['train'], monitors=[validation_monitor])\n",
    "\n",
    "regressor.fit(X['train'], y['train'],\n",
    "              monitors=[validation_monitor],\n",
    "              batch_size=BATCH_SIZE,\n",
    "              steps=TRAINING_STEPS)\n",
    "\n",
    "print(\"TEST:\")\n",
    "print (X['test'])\n",
    "print (y['test'])\n",
    "\n",
    "\n",
    "predicted = regressor.predict(X['test'])\n",
    "\n",
    "\n",
    "rmse = np.sqrt(((predicted - y['test']) ** 2).mean(axis=0))\n",
    "score = mean_squared_error(predicted, y['test'])\n",
    "print (\"MSE: %f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBTuIiIhpHAXEQkhhbuISAj9PydTuEZXtY9zAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12018ba58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_predicted, = plt.plot(predicted, label='predicted')\n",
    "plot_test, = plt.plot(y['test'], label='test')\n",
    "plt.legend(handles=[plot_predicted, plot_test])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.76881742]\n",
      " [ 4.67883968]\n",
      " [ 4.1189785 ]\n",
      " [ 3.7390728 ]\n",
      " [ 3.50912976]\n",
      " [ 3.57911229]\n",
      " [ 3.62910008]\n",
      " [ 3.80905533]\n",
      " [ 3.81905293]\n",
      " [ 3.77906275]\n",
      " [ 3.78906035]\n",
      " [ 3.70908022]\n",
      " [ 3.61910248]\n",
      " [ 3.64909506]\n",
      " [ 3.84904552]\n",
      " [ 3.62910008]\n",
      " [ 3.68908501]\n",
      " [ 3.69908261]\n",
      " [ 3.69908261]\n",
      " [ 3.78906035]\n",
      " [ 3.72907519]\n",
      " [ 3.62910008]\n",
      " [ 3.66909003]\n",
      " [ 3.66909003]\n",
      " [ 3.81905293]\n",
      " [ 3.91902804]\n",
      " [ 3.62910008]\n",
      " [ 3.16921401]\n",
      " [ 3.26918936]\n",
      " [ 3.2991817 ]\n",
      " [ 3.31917691]\n",
      " [ 3.46913958]]\n"
     ]
    }
   ],
   "source": [
    "print(y['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.64688134  2.64688134  2.64688158  2.64688134  2.64688134  2.64688134\n",
      "  2.64688134  2.64688134  2.64688134  2.64688158  2.64688134  2.64688158\n",
      "  2.64688134  2.64688158  2.64688158  2.64688134  2.64688134  2.64688134\n",
      "  2.64688134  2.64688134  2.64688134  2.64688134  2.64688134  2.64688134\n",
      "  2.64688134  2.64688134  2.64688134  2.64688134  2.64688134  2.64688134\n",
      "  2.64688134  2.64688134]\n"
     ]
    }
   ],
   "source": [
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.223681361102944"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y['test'], predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
